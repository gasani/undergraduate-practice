{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read en0\n",
      "read es0\n",
      "read en1\n",
      "read es1\n",
      "read en2\n",
      "read es2\n"
     ]
    }
   ],
   "source": [
    "# BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "#dataRead\n",
    "\n",
    "\n",
    "en_sentences = []\n",
    "es_sentences = []\n",
    "\n",
    "for i in range(3):\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".en\", 'r') as en_file:\n",
    "        for en_line in en_file:\n",
    "            en_sentences.append(en_line)\n",
    "    print (\"read en\" + str(i))\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".es\", 'r') as es_file:\n",
    "        for es_line in es_file:\n",
    "            es_sentences.append(es_line)\n",
    "    print (\"read es\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I regret this, but the vote has already been taken and the decision is made so let us leave the matter there.\n",
      "\n",
      "Lo lamento, pero la votación se ha realizado, se ha adoptado la decisión y, por consiguiente, dejemos así las cosas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 123\n",
    "print (en_sentences[index])\n",
    "print (es_sentences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataPreparation\n",
    "def create_dataset(source_sentences,target_sentences):\n",
    "    source_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in source_sentences for word in sentence.split())\n",
    "    target_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in target_sentences for word in sentence.split())\n",
    "\n",
    "    source_vocab = list(map(lambda x: x[0], sorted(source_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    target_vocab = list(map(lambda x: x[0], sorted(target_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    \n",
    "    source_vocab = source_vocab[:20000]\n",
    "    target_vocab = target_vocab[:30000]\n",
    "    \n",
    "    start_idx = 2\n",
    "    source_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(source_vocab)])\n",
    "    source_word2idx['<ukn>'] = 0\n",
    "    source_word2idx['<pad>'] = 1\n",
    "    source_idx2word = dict([(idx, word) for word, idx in source_word2idx.items()])\n",
    "    \n",
    "    start_idx = 4\n",
    "    target_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(target_vocab)])\n",
    "    target_word2idx['<ukn>'] = 0\n",
    "    target_word2idx['<go>']  = 1\n",
    "    target_word2idx['<eos>'] = 2\n",
    "    target_word2idx['<pad>'] = 3\n",
    "    \n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    x = [[source_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in source_sentences]\n",
    "    y = [[target_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in target_sentences]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        if abs(n1 - n2) <= 0.3 * n:\n",
    "            if n1 <= 15 and n2 <= 15:\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "    return X, Y, source_word2idx, source_idx2word, source_vocab, target_word2idx, target_idx2word, target_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_sentences = ['hello my friend', 'we need to reboot the server']\n",
    "#es_sentences = ['hola mi amigo', 'necesitamos reiniciar el servidor']\n",
    "save_dataset('./data.pkl', create_dataset(en_sentences, es_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, es_word2idx, es_idx2word, es_vocab = read_dataset('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [9207, 3, 2, 1713]\n",
      "Sentence in Spanish - encoded: [11270, 13, 594, 4, 1490]\n",
      "Decoded:\n",
      "------------------------\n",
      "Resumption of the session \n",
      "\n",
      "Reanudación del período de sesiones "
     ]
    }
   ],
   "source": [
    "#CHECK THAT WORKs\n",
    "print ('Sentence in English - encoded:', X[0])\n",
    "print ('Sentence in Spanish - encoded:', Y[0])\n",
    "print ('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[0])):\n",
    "    print (en_idx2word[X[0][i]], end = ' ')\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[0])):\n",
    "    print (es_idx2word[Y[0][i]], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [es_word2idx['<go>']] + y[i] + [es_word2idx['<eos>']] + (length-len(y[i])) * [es_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "#print X\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 15\n",
    "output_seq_len = 17\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "es_vocab_size = len(es_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = es_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = es_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 8): #batch size was 64\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = es_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == es_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(es_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64 #was64\n",
    "steps = 50000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 8.92647933959961\n",
      "step: 4, loss: 9.084221839904785\n",
      "step: 9, loss: 9.157144546508789\n",
      "step: 14, loss: 8.999015808105469\n",
      "step: 19, loss: 9.148736953735352\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 9.023209571838379\n",
      "step: 29, loss: 8.990303993225098\n",
      "step: 34, loss: 8.869390487670898\n",
      "step: 39, loss: 9.280086517333984\n",
      "Checkpoint is saved\n",
      "step: 44, loss: 8.906085014343262\n",
      "step: 49, loss: 8.867454528808594\n",
      "step: 54, loss: 8.768623352050781\n",
      "step: 59, loss: 8.021533012390137\n",
      "Checkpoint is saved\n",
      "step: 64, loss: 6.755884170532227\n",
      "step: 69, loss: 6.796221733093262\n",
      "step: 74, loss: 6.685888767242432\n",
      "step: 79, loss: 6.378986835479736\n",
      "Checkpoint is saved\n",
      "step: 84, loss: 5.777687072753906\n",
      "step: 89, loss: 7.06730842590332\n",
      "step: 94, loss: 4.811654090881348\n",
      "step: 99, loss: 5.420162200927734\n",
      "Checkpoint is saved\n",
      "step: 104, loss: 6.4593987464904785\n",
      "step: 109, loss: 5.701964378356934\n",
      "step: 114, loss: 5.367950439453125\n",
      "step: 119, loss: 6.0027174949646\n",
      "Checkpoint is saved\n",
      "step: 124, loss: 5.214349269866943\n",
      "step: 129, loss: 5.496886730194092\n",
      "step: 134, loss: 5.15963077545166\n",
      "step: 139, loss: 5.438056468963623\n",
      "Checkpoint is saved\n",
      "step: 144, loss: 5.495721817016602\n",
      "step: 149, loss: 5.1202712059021\n",
      "step: 154, loss: 5.496108531951904\n",
      "step: 159, loss: 4.753691673278809\n",
      "Checkpoint is saved\n",
      "step: 164, loss: 4.941565990447998\n",
      "step: 169, loss: 5.350869178771973\n",
      "step: 174, loss: 4.145157814025879\n",
      "step: 179, loss: 4.8622918128967285\n",
      "Checkpoint is saved\n",
      "step: 184, loss: 4.880193710327148\n",
      "step: 189, loss: 4.484440803527832\n",
      "step: 194, loss: 5.506190299987793\n",
      "step: 199, loss: 4.061525821685791\n",
      "Checkpoint is saved\n",
      "step: 204, loss: 4.922412872314453\n",
      "step: 209, loss: 4.398212909698486\n",
      "step: 214, loss: 4.967017650604248\n",
      "step: 219, loss: 4.069465637207031\n",
      "Checkpoint is saved\n",
      "step: 224, loss: 4.187398910522461\n",
      "step: 229, loss: 4.955974578857422\n",
      "step: 234, loss: 4.244900703430176\n",
      "step: 239, loss: 4.003035545349121\n",
      "Checkpoint is saved\n",
      "step: 244, loss: 4.966973304748535\n",
      "step: 249, loss: 5.060318946838379\n",
      "step: 254, loss: 3.921849250793457\n",
      "step: 259, loss: 4.915754318237305\n",
      "Checkpoint is saved\n",
      "step: 264, loss: 4.4878339767456055\n",
      "step: 269, loss: 4.774295806884766\n",
      "step: 274, loss: 4.120884895324707\n",
      "step: 279, loss: 4.812160015106201\n",
      "Checkpoint is saved\n",
      "step: 284, loss: 3.7781472206115723\n",
      "step: 289, loss: 3.913689374923706\n",
      "step: 294, loss: 3.539215564727783\n",
      "step: 299, loss: 4.277431488037109\n",
      "Checkpoint is saved\n",
      "step: 304, loss: 4.396084308624268\n",
      "step: 309, loss: 3.93560528755188\n",
      "step: 314, loss: 4.226590156555176\n",
      "step: 319, loss: 4.149928092956543\n",
      "Checkpoint is saved\n",
      "step: 324, loss: 3.894728899002075\n",
      "step: 329, loss: 3.7595174312591553\n",
      "step: 334, loss: 4.736721038818359\n",
      "step: 339, loss: 4.240592956542969\n",
      "Checkpoint is saved\n",
      "step: 344, loss: 3.687685489654541\n",
      "step: 349, loss: 3.8694496154785156\n",
      "step: 354, loss: 3.847311019897461\n",
      "step: 359, loss: 3.8404712677001953\n",
      "Checkpoint is saved\n",
      "step: 364, loss: 3.8422749042510986\n",
      "step: 369, loss: 4.651447296142578\n",
      "step: 374, loss: 3.811500310897827\n",
      "step: 379, loss: 2.8324694633483887\n",
      "Checkpoint is saved\n",
      "step: 384, loss: 3.915620803833008\n",
      "step: 389, loss: 3.271965265274048\n",
      "step: 394, loss: 3.493527412414551\n",
      "step: 399, loss: 3.7069406509399414\n",
      "Checkpoint is saved\n",
      "step: 404, loss: 3.246399402618408\n",
      "step: 409, loss: 3.285219430923462\n",
      "step: 414, loss: 3.6394705772399902\n",
      "step: 419, loss: 3.6416492462158203\n",
      "Checkpoint is saved\n",
      "step: 424, loss: 4.390836715698242\n",
      "step: 429, loss: 3.44769287109375\n",
      "step: 434, loss: 5.301272869110107\n",
      "step: 439, loss: 3.097626209259033\n",
      "Checkpoint is saved\n",
      "step: 444, loss: 4.4143452644348145\n",
      "step: 449, loss: 3.233445167541504\n",
      "step: 454, loss: 3.5647292137145996\n",
      "step: 459, loss: 2.4717719554901123\n",
      "Checkpoint is saved\n",
      "step: 464, loss: 3.3040273189544678\n",
      "step: 469, loss: 3.592165470123291\n",
      "step: 474, loss: 3.688721179962158\n",
      "step: 479, loss: 3.2180333137512207\n",
      "Checkpoint is saved\n",
      "step: 484, loss: 3.1549911499023438\n",
      "step: 489, loss: 3.9987244606018066\n",
      "step: 494, loss: 3.3549838066101074\n",
      "step: 499, loss: 4.087368965148926\n",
      "Checkpoint is saved\n",
      "step: 504, loss: 4.8341522216796875\n",
      "step: 509, loss: 3.25657320022583\n",
      "step: 514, loss: 3.6580967903137207\n",
      "step: 519, loss: 2.4966769218444824\n",
      "Checkpoint is saved\n",
      "step: 524, loss: 3.596109628677368\n",
      "step: 529, loss: 3.76761531829834\n",
      "step: 534, loss: 3.1798696517944336\n",
      "step: 539, loss: 3.040363311767578\n",
      "Checkpoint is saved\n",
      "step: 544, loss: 3.2395119667053223\n",
      "step: 549, loss: 3.533755302429199\n",
      "step: 554, loss: 3.3853187561035156\n",
      "step: 559, loss: 2.584583044052124\n",
      "Checkpoint is saved\n",
      "step: 564, loss: 3.154785633087158\n",
      "step: 569, loss: 3.0903661251068115\n",
      "step: 574, loss: 3.865875720977783\n",
      "step: 579, loss: 3.7297520637512207\n",
      "Checkpoint is saved\n",
      "step: 584, loss: 3.2697277069091797\n",
      "step: 589, loss: 3.9749412536621094\n",
      "step: 594, loss: 3.021427631378174\n",
      "step: 599, loss: 3.2942166328430176\n",
      "Checkpoint is saved\n",
      "step: 604, loss: 3.218132972717285\n",
      "step: 609, loss: 2.709196090698242\n",
      "step: 614, loss: 2.9402618408203125\n",
      "step: 619, loss: 3.062213897705078\n",
      "Checkpoint is saved\n",
      "step: 624, loss: 2.5809731483459473\n",
      "step: 629, loss: 3.7604098320007324\n",
      "step: 634, loss: 3.0111196041107178\n",
      "step: 639, loss: 3.0101239681243896\n",
      "Checkpoint is saved\n",
      "step: 644, loss: 3.362089157104492\n",
      "step: 649, loss: 3.1988072395324707\n",
      "step: 654, loss: 3.0061604976654053\n",
      "step: 659, loss: 3.074843406677246\n",
      "Checkpoint is saved\n",
      "step: 664, loss: 3.1377909183502197\n",
      "step: 669, loss: 2.7420644760131836\n",
      "step: 674, loss: 3.1659762859344482\n",
      "step: 679, loss: 3.327847480773926\n",
      "Checkpoint is saved\n",
      "step: 684, loss: 3.3757519721984863\n",
      "step: 689, loss: 2.886815071105957\n",
      "step: 694, loss: 2.8878977298736572\n",
      "step: 699, loss: 2.6698687076568604\n",
      "Checkpoint is saved\n",
      "step: 704, loss: 1.762585997581482\n",
      "step: 709, loss: 2.171098470687866\n",
      "step: 714, loss: 2.8706064224243164\n",
      "step: 719, loss: 3.317152976989746\n",
      "Checkpoint is saved\n",
      "step: 724, loss: 3.105584144592285\n",
      "step: 729, loss: 3.345691442489624\n",
      "step: 734, loss: 3.6514134407043457\n",
      "step: 739, loss: 2.933326244354248\n",
      "Checkpoint is saved\n",
      "step: 744, loss: 2.5178322792053223\n",
      "step: 749, loss: 3.250565528869629\n",
      "step: 754, loss: 3.491520404815674\n",
      "step: 759, loss: 3.20739483833313\n",
      "Checkpoint is saved\n",
      "step: 764, loss: 3.1854405403137207\n",
      "step: 769, loss: 3.5058557987213135\n",
      "step: 774, loss: 2.3554837703704834\n",
      "step: 779, loss: 2.560153007507324\n",
      "Checkpoint is saved\n",
      "step: 784, loss: 2.5076560974121094\n",
      "step: 789, loss: 2.5487327575683594\n",
      "step: 794, loss: 2.6351356506347656\n",
      "step: 799, loss: 2.4592630863189697\n",
      "Checkpoint is saved\n",
      "step: 804, loss: 2.401951313018799\n",
      "step: 809, loss: 2.3702542781829834\n",
      "step: 814, loss: 3.025348663330078\n",
      "step: 819, loss: 2.92997407913208\n",
      "Checkpoint is saved\n",
      "step: 824, loss: 2.900381565093994\n",
      "step: 829, loss: 1.999489188194275\n",
      "step: 834, loss: 3.146879196166992\n",
      "step: 839, loss: 3.256680727005005\n",
      "Checkpoint is saved\n",
      "step: 844, loss: 2.1201071739196777\n",
      "step: 849, loss: 3.256627082824707\n",
      "step: 854, loss: 3.6945879459381104\n",
      "step: 859, loss: 2.782442331314087\n",
      "Checkpoint is saved\n",
      "step: 864, loss: 2.369199275970459\n",
      "step: 869, loss: 3.0030951499938965\n",
      "step: 874, loss: 2.981297016143799\n",
      "step: 879, loss: 2.603275775909424\n",
      "Checkpoint is saved\n",
      "step: 884, loss: 2.2660462856292725\n",
      "step: 889, loss: 2.6284995079040527\n",
      "step: 894, loss: 3.347269296646118\n",
      "step: 899, loss: 2.6473512649536133\n",
      "Checkpoint is saved\n",
      "step: 904, loss: 2.785601854324341\n",
      "step: 909, loss: 3.374116897583008\n",
      "step: 914, loss: 2.8233144283294678\n",
      "step: 919, loss: 2.3658885955810547\n",
      "Checkpoint is saved\n",
      "step: 924, loss: 2.3034563064575195\n",
      "step: 929, loss: 2.6543221473693848\n",
      "step: 934, loss: 2.779761791229248\n",
      "step: 939, loss: 2.532142162322998\n",
      "Checkpoint is saved\n",
      "step: 944, loss: 1.964033842086792\n",
      "step: 949, loss: 2.0735442638397217\n",
      "step: 954, loss: 2.866506576538086\n",
      "step: 959, loss: 2.0586490631103516\n",
      "Checkpoint is saved\n",
      "step: 964, loss: 2.87998366355896\n",
      "step: 969, loss: 2.3343210220336914\n",
      "step: 974, loss: 2.544833183288574\n",
      "step: 979, loss: 2.659248113632202\n",
      "Checkpoint is saved\n",
      "step: 984, loss: 2.7826919555664062\n",
      "step: 989, loss: 2.6296398639678955\n",
      "step: 994, loss: 3.264941692352295\n",
      "step: 999, loss: 2.6807918548583984\n",
      "Checkpoint is saved\n",
      "step: 1004, loss: 2.3366217613220215\n",
      "step: 1009, loss: 2.7904458045959473\n",
      "step: 1014, loss: 3.2279114723205566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1019, loss: 3.078920841217041\n",
      "Checkpoint is saved\n",
      "step: 1024, loss: 2.0943515300750732\n",
      "step: 1029, loss: 2.6538872718811035\n",
      "step: 1034, loss: 2.238340139389038\n",
      "step: 1039, loss: 2.3308749198913574\n",
      "Checkpoint is saved\n",
      "step: 1044, loss: 2.9675960540771484\n",
      "step: 1049, loss: 2.404086112976074\n",
      "step: 1054, loss: 3.2782068252563477\n",
      "step: 1059, loss: 2.7853057384490967\n",
      "Checkpoint is saved\n",
      "step: 1064, loss: 1.637933611869812\n",
      "step: 1069, loss: 1.8432793617248535\n",
      "step: 1074, loss: 1.778212070465088\n",
      "step: 1079, loss: 2.5289201736450195\n",
      "Checkpoint is saved\n",
      "step: 1084, loss: 2.438446521759033\n",
      "step: 1089, loss: 2.2360332012176514\n",
      "step: 1094, loss: 2.2940711975097656\n",
      "step: 1099, loss: 3.009209156036377\n",
      "Checkpoint is saved\n",
      "step: 1104, loss: 3.634120225906372\n",
      "step: 1109, loss: 1.8969950675964355\n",
      "step: 1114, loss: 2.229051113128662\n",
      "step: 1119, loss: 2.1644132137298584\n",
      "Checkpoint is saved\n",
      "step: 1124, loss: 2.890711545944214\n",
      "step: 1129, loss: 2.19925594329834\n",
      "step: 1134, loss: 2.0452234745025635\n",
      "step: 1139, loss: 2.0341579914093018\n",
      "Checkpoint is saved\n",
      "step: 1144, loss: 2.471503257751465\n",
      "step: 1149, loss: 2.28820538520813\n",
      "step: 1154, loss: 2.951694965362549\n",
      "step: 1159, loss: 2.818784713745117\n",
      "Checkpoint is saved\n",
      "step: 1164, loss: 2.6171836853027344\n",
      "step: 1169, loss: 2.4905147552490234\n",
      "step: 1174, loss: 3.0997653007507324\n",
      "step: 1179, loss: 2.2220027446746826\n",
      "Checkpoint is saved\n",
      "step: 1184, loss: 2.9700961112976074\n",
      "step: 1189, loss: 2.484471321105957\n",
      "step: 1194, loss: 2.632035732269287\n",
      "step: 1199, loss: 2.66920804977417\n",
      "Checkpoint is saved\n",
      "step: 1204, loss: 2.885313034057617\n",
      "step: 1209, loss: 1.937225580215454\n",
      "step: 1214, loss: 2.2621772289276123\n",
      "step: 1219, loss: 2.612699270248413\n",
      "Checkpoint is saved\n",
      "step: 1224, loss: 2.7088592052459717\n",
      "step: 1229, loss: 1.970590353012085\n",
      "step: 1234, loss: 1.874056339263916\n",
      "step: 1239, loss: 1.7236754894256592\n",
      "Checkpoint is saved\n",
      "step: 1244, loss: 2.0186498165130615\n",
      "step: 1249, loss: 2.1361210346221924\n",
      "step: 1254, loss: 1.4546326398849487\n",
      "step: 1259, loss: 2.5026636123657227\n",
      "Checkpoint is saved\n",
      "step: 1264, loss: 3.1236419677734375\n",
      "step: 1269, loss: 2.137265205383301\n",
      "step: 1274, loss: 2.4989664554595947\n",
      "step: 1279, loss: 1.7434476613998413\n",
      "Checkpoint is saved\n",
      "step: 1284, loss: 2.5184569358825684\n",
      "step: 1289, loss: 2.950010299682617\n",
      "step: 1294, loss: 2.4929068088531494\n",
      "step: 1299, loss: 2.500880241394043\n",
      "Checkpoint is saved\n",
      "step: 1304, loss: 2.265235424041748\n",
      "step: 1309, loss: 1.9086827039718628\n",
      "step: 1314, loss: 2.1332530975341797\n",
      "step: 1319, loss: 2.090480327606201\n",
      "Checkpoint is saved\n",
      "step: 1324, loss: 2.6582813262939453\n",
      "step: 1329, loss: 1.9020018577575684\n",
      "step: 1334, loss: 1.9423695802688599\n",
      "step: 1339, loss: 2.5507073402404785\n",
      "Checkpoint is saved\n",
      "step: 1344, loss: 3.050117254257202\n",
      "step: 1349, loss: 1.596299409866333\n",
      "step: 1354, loss: 2.3606061935424805\n",
      "step: 1359, loss: 2.0980303287506104\n",
      "Checkpoint is saved\n",
      "step: 1364, loss: 2.0483784675598145\n",
      "step: 1369, loss: 2.5112431049346924\n",
      "step: 1374, loss: 2.23038387298584\n",
      "step: 1379, loss: 2.2887375354766846\n",
      "Checkpoint is saved\n",
      "step: 1384, loss: 2.4456112384796143\n",
      "step: 1389, loss: 1.5172772407531738\n",
      "step: 1394, loss: 2.471806526184082\n",
      "step: 1399, loss: 2.216968059539795\n",
      "Checkpoint is saved\n",
      "step: 1404, loss: 2.5097107887268066\n",
      "step: 1409, loss: 2.5419697761535645\n",
      "step: 1414, loss: 1.4054925441741943\n",
      "step: 1419, loss: 2.443960189819336\n",
      "Checkpoint is saved\n",
      "step: 1424, loss: 2.2001266479492188\n",
      "step: 1429, loss: 2.24556040763855\n",
      "step: 1434, loss: 1.738645076751709\n",
      "step: 1439, loss: 2.3939285278320312\n",
      "Checkpoint is saved\n",
      "step: 1444, loss: 2.6073238849639893\n",
      "step: 1449, loss: 1.8613076210021973\n",
      "step: 1454, loss: 2.24601149559021\n",
      "step: 1459, loss: 2.2749485969543457\n",
      "Checkpoint is saved\n",
      "step: 1464, loss: 2.274261474609375\n",
      "step: 1469, loss: 2.2127525806427\n",
      "step: 1474, loss: 1.1857211589813232\n",
      "step: 1479, loss: 1.4430423974990845\n",
      "Checkpoint is saved\n",
      "step: 1484, loss: 2.383246898651123\n",
      "step: 1489, loss: 2.1369082927703857\n",
      "step: 1494, loss: 2.1914544105529785\n",
      "step: 1499, loss: 2.222822666168213\n",
      "Checkpoint is saved\n",
      "step: 1504, loss: 2.3499178886413574\n",
      "step: 1509, loss: 1.8024905920028687\n",
      "step: 1514, loss: 2.059993267059326\n",
      "step: 1519, loss: 1.7333844900131226\n",
      "Checkpoint is saved\n",
      "step: 1524, loss: 2.156081199645996\n",
      "step: 1529, loss: 1.4506072998046875\n",
      "step: 1534, loss: 1.4745848178863525\n",
      "step: 1539, loss: 2.252140998840332\n",
      "Checkpoint is saved\n",
      "step: 1544, loss: 1.8522896766662598\n",
      "step: 1549, loss: 2.4636082649230957\n",
      "step: 1554, loss: 2.1552608013153076\n",
      "step: 1559, loss: 1.3079650402069092\n",
      "Checkpoint is saved\n",
      "step: 1564, loss: 2.3236470222473145\n",
      "step: 1569, loss: 1.8949098587036133\n",
      "step: 1574, loss: 1.1530061960220337\n",
      "step: 1579, loss: 2.075495719909668\n",
      "Checkpoint is saved\n",
      "step: 1584, loss: 1.68031644821167\n",
      "step: 1589, loss: 1.640876054763794\n",
      "step: 1594, loss: 2.0780200958251953\n",
      "step: 1599, loss: 1.6357421875\n",
      "Checkpoint is saved\n",
      "step: 1604, loss: 2.1380531787872314\n",
      "step: 1609, loss: 2.191610097885132\n",
      "step: 1614, loss: 2.505814552307129\n",
      "step: 1619, loss: 2.1140434741973877\n",
      "Checkpoint is saved\n",
      "step: 1624, loss: 2.2786483764648438\n",
      "step: 1629, loss: 2.0173490047454834\n",
      "step: 1634, loss: 1.3159198760986328\n",
      "step: 1639, loss: 2.187201499938965\n",
      "Checkpoint is saved\n",
      "step: 1644, loss: 2.011439561843872\n",
      "step: 1649, loss: 2.695220470428467\n",
      "step: 1654, loss: 1.949556589126587\n",
      "step: 1659, loss: 2.05135440826416\n",
      "Checkpoint is saved\n",
      "step: 1664, loss: 2.281726360321045\n",
      "step: 1669, loss: 1.5835089683532715\n",
      "step: 1674, loss: 2.21388578414917\n",
      "step: 1679, loss: 2.451627492904663\n",
      "Checkpoint is saved\n",
      "step: 1684, loss: 2.2550301551818848\n",
      "step: 1689, loss: 1.7245392799377441\n",
      "step: 1694, loss: 1.7451227903366089\n",
      "step: 1699, loss: 1.9234607219696045\n",
      "Checkpoint is saved\n",
      "step: 1704, loss: 2.645125150680542\n",
      "step: 1709, loss: 1.7607293128967285\n",
      "step: 1714, loss: 1.6839616298675537\n",
      "step: 1719, loss: 2.507092237472534\n",
      "Checkpoint is saved\n",
      "step: 1724, loss: 1.08570396900177\n",
      "step: 1729, loss: 2.28147554397583\n",
      "step: 1734, loss: 1.5305360555648804\n",
      "step: 1739, loss: 1.8744655847549438\n",
      "Checkpoint is saved\n",
      "step: 1744, loss: 1.3165326118469238\n",
      "step: 1749, loss: 2.048379421234131\n",
      "step: 1754, loss: 1.9755189418792725\n",
      "step: 1759, loss: 2.270524024963379\n",
      "Checkpoint is saved\n",
      "step: 1764, loss: 1.6879640817642212\n",
      "step: 1769, loss: 1.8562639951705933\n",
      "step: 1774, loss: 1.8409844636917114\n",
      "step: 1779, loss: 1.8718749284744263\n",
      "Checkpoint is saved\n",
      "step: 1784, loss: 2.238492250442505\n",
      "step: 1789, loss: 1.5594747066497803\n",
      "step: 1794, loss: 1.7740199565887451\n",
      "step: 1799, loss: 1.5471327304840088\n",
      "Checkpoint is saved\n",
      "step: 1804, loss: 1.6796369552612305\n",
      "step: 1809, loss: 1.5672342777252197\n",
      "step: 1814, loss: 1.9608941078186035\n",
      "step: 1819, loss: 1.563502311706543\n",
      "Checkpoint is saved\n",
      "step: 1824, loss: 1.6756961345672607\n",
      "step: 1829, loss: 1.8381218910217285\n",
      "step: 1834, loss: 2.300356864929199\n",
      "step: 1839, loss: 1.4134997129440308\n",
      "Checkpoint is saved\n",
      "step: 1844, loss: 1.7582135200500488\n",
      "step: 1849, loss: 1.6364778280258179\n",
      "step: 1854, loss: 1.7778013944625854\n",
      "step: 1859, loss: 2.247684955596924\n",
      "Checkpoint is saved\n",
      "step: 1864, loss: 1.5615015029907227\n",
      "step: 1869, loss: 1.6561839580535889\n",
      "step: 1874, loss: 2.1154351234436035\n",
      "step: 1879, loss: 1.5850613117218018\n",
      "Checkpoint is saved\n",
      "step: 1884, loss: 1.983481526374817\n",
      "step: 1889, loss: 1.372104287147522\n",
      "step: 1894, loss: 2.018281936645508\n",
      "step: 1899, loss: 1.465456247329712\n",
      "Checkpoint is saved\n",
      "step: 1904, loss: 1.3147637844085693\n",
      "step: 1909, loss: 1.7427423000335693\n",
      "step: 1914, loss: 1.8192014694213867\n",
      "step: 1919, loss: 1.7928526401519775\n",
      "Checkpoint is saved\n",
      "step: 1924, loss: 2.181978702545166\n",
      "step: 1929, loss: 2.085132122039795\n",
      "step: 1934, loss: 2.3035268783569336\n",
      "step: 1939, loss: 1.5586758852005005\n",
      "Checkpoint is saved\n",
      "step: 1944, loss: 1.4250354766845703\n",
      "step: 1949, loss: 1.246032953262329\n",
      "step: 1954, loss: 2.0438895225524902\n",
      "step: 1959, loss: 1.858764886856079\n",
      "Checkpoint is saved\n",
      "step: 1964, loss: 1.6576180458068848\n",
      "step: 1969, loss: 1.8611412048339844\n",
      "step: 1974, loss: 1.4433798789978027\n",
      "step: 1979, loss: 1.9712989330291748\n",
      "Checkpoint is saved\n",
      "step: 1984, loss: 2.1908531188964844\n",
      "step: 1989, loss: 2.092902660369873\n",
      "step: 1994, loss: 0.9586092233657837\n",
      "step: 1999, loss: 2.192117214202881\n",
      "Checkpoint is saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2004, loss: 1.6461824178695679\n",
      "step: 2009, loss: 2.021388530731201\n",
      "step: 2014, loss: 1.370367407798767\n",
      "step: 2019, loss: 1.2743171453475952\n",
      "Checkpoint is saved\n",
      "step: 2024, loss: 2.2624542713165283\n",
      "step: 2029, loss: 1.754014253616333\n",
      "step: 2034, loss: 1.1479837894439697\n",
      "step: 2039, loss: 2.237621545791626\n",
      "Checkpoint is saved\n",
      "step: 2044, loss: 1.064126968383789\n",
      "step: 2049, loss: 1.8459323644638062\n",
      "step: 2054, loss: 1.465907335281372\n",
      "step: 2059, loss: 2.145446300506592\n",
      "Checkpoint is saved\n",
      "step: 2064, loss: 0.9983392953872681\n",
      "step: 2069, loss: 1.7570672035217285\n",
      "step: 2074, loss: 1.4856176376342773\n",
      "step: 2079, loss: 1.5306448936462402\n",
      "Checkpoint is saved\n",
      "step: 2084, loss: 1.516265869140625\n",
      "step: 2089, loss: 1.2653653621673584\n",
      "step: 2094, loss: 1.8608033657073975\n",
      "step: 2099, loss: 1.8285846710205078\n",
      "Checkpoint is saved\n",
      "step: 2104, loss: 1.932084083557129\n",
      "step: 2109, loss: 1.2875418663024902\n",
      "step: 2114, loss: 1.6023967266082764\n",
      "step: 2119, loss: 1.715505838394165\n",
      "Checkpoint is saved\n",
      "step: 2124, loss: 1.1397242546081543\n",
      "step: 2129, loss: 1.7072153091430664\n",
      "step: 2134, loss: 1.9189823865890503\n",
      "step: 2139, loss: 2.109065294265747\n",
      "Checkpoint is saved\n",
      "step: 2144, loss: 1.253854513168335\n",
      "step: 2149, loss: 1.4974133968353271\n",
      "step: 2154, loss: 1.3382898569107056\n",
      "step: 2159, loss: 1.249799370765686\n",
      "Checkpoint is saved\n",
      "step: 2164, loss: 1.8828790187835693\n",
      "step: 2169, loss: 1.574990153312683\n",
      "step: 2174, loss: 1.5845619440078735\n",
      "step: 2179, loss: 1.5814934968948364\n",
      "Checkpoint is saved\n",
      "step: 2184, loss: 2.0708701610565186\n",
      "step: 2189, loss: 1.610873818397522\n",
      "step: 2194, loss: 1.3101520538330078\n",
      "step: 2199, loss: 1.7207105159759521\n",
      "Checkpoint is saved\n",
      "step: 2204, loss: 1.969862937927246\n",
      "step: 2209, loss: 1.470346450805664\n",
      "step: 2214, loss: 1.5806611776351929\n",
      "step: 2219, loss: 1.6113967895507812\n",
      "Checkpoint is saved\n",
      "step: 2224, loss: 1.7456384897232056\n",
      "step: 2229, loss: 1.6325303316116333\n",
      "step: 2234, loss: 2.2501277923583984\n",
      "step: 2239, loss: 2.2237343788146973\n",
      "Checkpoint is saved\n",
      "step: 2244, loss: 1.5317411422729492\n",
      "step: 2249, loss: 1.8507097959518433\n",
      "step: 2254, loss: 1.4314053058624268\n",
      "step: 2259, loss: 1.4697692394256592\n",
      "Checkpoint is saved\n",
      "step: 2264, loss: 1.769538164138794\n",
      "step: 2269, loss: 1.662203073501587\n",
      "step: 2274, loss: 1.5970778465270996\n",
      "step: 2279, loss: 1.6069962978363037\n",
      "Checkpoint is saved\n",
      "step: 2284, loss: 1.3954741954803467\n",
      "step: 2289, loss: 1.1962785720825195\n",
      "step: 2294, loss: 1.594218134880066\n",
      "step: 2299, loss: 1.63352370262146\n",
      "Checkpoint is saved\n",
      "step: 2304, loss: 2.5738017559051514\n",
      "step: 2309, loss: 1.7167736291885376\n",
      "step: 2314, loss: 1.9912126064300537\n",
      "step: 2319, loss: 1.5229153633117676\n",
      "Checkpoint is saved\n",
      "step: 2324, loss: 1.6447309255599976\n",
      "step: 2329, loss: 1.4225330352783203\n",
      "step: 2334, loss: 1.3905887603759766\n",
      "step: 2339, loss: 1.241581916809082\n",
      "Checkpoint is saved\n",
      "step: 2344, loss: 1.6388094425201416\n",
      "step: 2349, loss: 1.4595856666564941\n",
      "step: 2354, loss: 1.4529556035995483\n",
      "step: 2359, loss: 1.4637374877929688\n",
      "Checkpoint is saved\n",
      "step: 2364, loss: 2.0615410804748535\n",
      "step: 2369, loss: 1.4765920639038086\n",
      "step: 2374, loss: 1.7349021434783936\n",
      "step: 2379, loss: 1.374140977859497\n",
      "Checkpoint is saved\n",
      "step: 2384, loss: 1.0128846168518066\n",
      "step: 2389, loss: 0.9845059514045715\n",
      "step: 2394, loss: 1.5015472173690796\n",
      "step: 2399, loss: 1.0601539611816406\n",
      "Checkpoint is saved\n",
      "step: 2404, loss: 0.9564329385757446\n",
      "step: 2409, loss: 1.6425668001174927\n",
      "step: 2414, loss: 2.113129138946533\n",
      "step: 2419, loss: 1.8965363502502441\n",
      "Checkpoint is saved\n",
      "step: 2424, loss: 1.2475218772888184\n",
      "step: 2429, loss: 1.500723123550415\n",
      "step: 2434, loss: 1.401888370513916\n",
      "step: 2439, loss: 1.3659858703613281\n",
      "Checkpoint is saved\n",
      "step: 2444, loss: 1.3689343929290771\n",
      "step: 2449, loss: 1.1345051527023315\n",
      "step: 2454, loss: 1.2299363613128662\n",
      "step: 2459, loss: 1.5405570268630981\n",
      "Checkpoint is saved\n",
      "step: 2464, loss: 1.2000036239624023\n",
      "step: 2469, loss: 1.4155833721160889\n",
      "step: 2474, loss: 1.7867798805236816\n",
      "step: 2479, loss: 1.2811304330825806\n",
      "Checkpoint is saved\n",
      "step: 2484, loss: 2.2177178859710693\n",
      "step: 2489, loss: 1.4411460161209106\n",
      "step: 2494, loss: 1.9998105764389038\n",
      "step: 2499, loss: 1.0345596075057983\n",
      "Checkpoint is saved\n",
      "step: 2504, loss: 1.8264621496200562\n",
      "step: 2509, loss: 2.010188341140747\n",
      "step: 2514, loss: 1.4499030113220215\n",
      "step: 2519, loss: 1.3650181293487549\n",
      "Checkpoint is saved\n",
      "step: 2524, loss: 1.1164785623550415\n",
      "step: 2529, loss: 1.8648236989974976\n",
      "step: 2534, loss: 1.7981222867965698\n",
      "step: 2539, loss: 1.458263874053955\n",
      "Checkpoint is saved\n",
      "step: 2544, loss: 1.285711646080017\n",
      "step: 2549, loss: 1.1774165630340576\n",
      "step: 2554, loss: 1.411966323852539\n",
      "step: 2559, loss: 1.4832227230072021\n",
      "Checkpoint is saved\n",
      "step: 2564, loss: 1.8378138542175293\n",
      "step: 2569, loss: 0.958193838596344\n",
      "step: 2574, loss: 1.7327698469161987\n",
      "step: 2579, loss: 1.3477866649627686\n",
      "Checkpoint is saved\n",
      "step: 2584, loss: 1.285989761352539\n",
      "step: 2589, loss: 1.7618458271026611\n",
      "step: 2594, loss: 1.563788890838623\n",
      "step: 2599, loss: 1.333317756652832\n",
      "Checkpoint is saved\n",
      "step: 2604, loss: 1.4256024360656738\n",
      "step: 2609, loss: 1.71718168258667\n",
      "step: 2614, loss: 1.7616922855377197\n",
      "step: 2619, loss: 1.9710092544555664\n",
      "Checkpoint is saved\n",
      "step: 2624, loss: 1.7929447889328003\n",
      "step: 2629, loss: 1.810049057006836\n",
      "step: 2634, loss: 1.5233433246612549\n",
      "step: 2639, loss: 1.5905003547668457\n",
      "Checkpoint is saved\n",
      "step: 2644, loss: 1.8489725589752197\n",
      "step: 2649, loss: 1.5899354219436646\n",
      "step: 2654, loss: 1.5078887939453125\n",
      "step: 2659, loss: 1.4480857849121094\n",
      "Checkpoint is saved\n",
      "step: 2664, loss: 1.57150399684906\n",
      "step: 2669, loss: 1.6087818145751953\n",
      "step: 2674, loss: 1.204790472984314\n",
      "step: 2679, loss: 1.3316237926483154\n",
      "Checkpoint is saved\n",
      "step: 2684, loss: 1.780587911605835\n",
      "step: 2689, loss: 0.7747387886047363\n",
      "step: 2694, loss: 1.122480034828186\n",
      "step: 2699, loss: 1.6252710819244385\n",
      "Checkpoint is saved\n",
      "step: 2704, loss: 1.7418252229690552\n",
      "step: 2709, loss: 1.4535126686096191\n",
      "step: 2714, loss: 1.0758249759674072\n",
      "step: 2719, loss: 1.9693723917007446\n",
      "Checkpoint is saved\n",
      "step: 2724, loss: 2.2051734924316406\n",
      "step: 2729, loss: 1.182259440422058\n",
      "step: 2734, loss: 1.7430503368377686\n",
      "step: 2739, loss: 1.041029930114746\n",
      "Checkpoint is saved\n",
      "step: 2744, loss: 1.376378059387207\n",
      "step: 2749, loss: 1.4740996360778809\n",
      "step: 2754, loss: 1.523094654083252\n",
      "step: 2759, loss: 1.0149905681610107\n",
      "Checkpoint is saved\n",
      "step: 2764, loss: 1.7983274459838867\n",
      "step: 2769, loss: 1.6733801364898682\n",
      "step: 2774, loss: 1.232265830039978\n",
      "step: 2779, loss: 1.510191559791565\n",
      "Checkpoint is saved\n",
      "step: 2784, loss: 1.2492125034332275\n",
      "step: 2789, loss: 1.755687952041626\n",
      "step: 2794, loss: 1.4778904914855957\n",
      "step: 2799, loss: 1.5709128379821777\n",
      "Checkpoint is saved\n",
      "step: 2804, loss: 1.7303682565689087\n",
      "step: 2809, loss: 1.1495026350021362\n",
      "step: 2814, loss: 1.650879979133606\n",
      "step: 2819, loss: 1.4660369157791138\n",
      "Checkpoint is saved\n",
      "step: 2824, loss: 1.0008594989776611\n",
      "step: 2829, loss: 1.1861497163772583\n",
      "step: 2834, loss: 1.8838820457458496\n",
      "step: 2839, loss: 1.410517692565918\n",
      "Checkpoint is saved\n",
      "step: 2844, loss: 1.0394350290298462\n",
      "step: 2849, loss: 1.5558655261993408\n",
      "step: 2854, loss: 1.735022783279419\n",
      "step: 2859, loss: 1.394599437713623\n",
      "Checkpoint is saved\n",
      "step: 2864, loss: 0.9717741012573242\n",
      "step: 2869, loss: 1.679322600364685\n",
      "step: 2874, loss: 1.0586036443710327\n",
      "step: 2879, loss: 1.2080515623092651\n",
      "Checkpoint is saved\n",
      "step: 2884, loss: 1.3347268104553223\n",
      "step: 2889, loss: 1.7122602462768555\n",
      "step: 2894, loss: 1.4906585216522217\n",
      "step: 2899, loss: 1.9441049098968506\n",
      "Checkpoint is saved\n",
      "step: 2904, loss: 1.4526519775390625\n",
      "step: 2909, loss: 1.3603137731552124\n",
      "step: 2914, loss: 1.451179027557373\n",
      "step: 2919, loss: 1.6442749500274658\n",
      "Checkpoint is saved\n",
      "step: 2924, loss: 1.4375405311584473\n",
      "step: 2929, loss: 1.404529094696045\n",
      "step: 2934, loss: 1.0810294151306152\n",
      "step: 2939, loss: 1.3944807052612305\n",
      "Checkpoint is saved\n",
      "step: 2944, loss: 1.6739375591278076\n",
      "step: 2949, loss: 1.2784775495529175\n",
      "step: 2954, loss: 1.5204761028289795\n",
      "step: 2959, loss: 1.4056165218353271\n",
      "Checkpoint is saved\n",
      "step: 2964, loss: 1.261122703552246\n",
      "step: 2969, loss: 1.5938725471496582\n",
      "step: 2974, loss: 1.5207343101501465\n",
      "step: 2979, loss: 1.683077096939087\n",
      "Checkpoint is saved\n",
      "step: 2984, loss: 1.453943133354187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2989, loss: 1.0010937452316284\n",
      "step: 2994, loss: 1.1660596132278442\n",
      "step: 2999, loss: 1.6562204360961914\n",
      "Checkpoint is saved\n",
      "step: 3004, loss: 1.674649715423584\n",
      "step: 3009, loss: 1.7740535736083984\n",
      "step: 3014, loss: 1.3159840106964111\n",
      "step: 3019, loss: 1.131794810295105\n",
      "Checkpoint is saved\n",
      "step: 3024, loss: 1.1047931909561157\n",
      "step: 3029, loss: 1.7677323818206787\n",
      "step: 3034, loss: 1.2250826358795166\n",
      "step: 3039, loss: 1.30104398727417\n",
      "Checkpoint is saved\n",
      "step: 3044, loss: 1.4526780843734741\n",
      "step: 3049, loss: 0.7487020492553711\n",
      "step: 3054, loss: 1.2841575145721436\n",
      "step: 3059, loss: 1.588810682296753\n",
      "Checkpoint is saved\n",
      "step: 3064, loss: 1.4696252346038818\n",
      "step: 3069, loss: 1.7522398233413696\n",
      "step: 3074, loss: 1.9325828552246094\n",
      "step: 3079, loss: 1.4932396411895752\n",
      "Checkpoint is saved\n",
      "step: 3084, loss: 1.6726195812225342\n",
      "step: 3089, loss: 0.7608859539031982\n",
      "step: 3094, loss: 0.9139081835746765\n",
      "step: 3099, loss: 1.2382469177246094\n",
      "Checkpoint is saved\n",
      "step: 3104, loss: 0.9515008926391602\n",
      "step: 3109, loss: 1.5939745903015137\n",
      "step: 3114, loss: 1.145735263824463\n",
      "step: 3119, loss: 0.9341235160827637\n",
      "Checkpoint is saved\n",
      "step: 3124, loss: 1.5873973369598389\n",
      "step: 3129, loss: 1.7740596532821655\n",
      "step: 3134, loss: 0.8614509105682373\n",
      "step: 3139, loss: 1.0078274011611938\n",
      "Checkpoint is saved\n",
      "step: 3144, loss: 1.475950002670288\n",
      "step: 3149, loss: 0.7638043165206909\n",
      "step: 3154, loss: 1.1302629709243774\n",
      "step: 3159, loss: 1.4212150573730469\n",
      "Checkpoint is saved\n",
      "step: 3164, loss: 1.0802714824676514\n",
      "step: 3169, loss: 1.262444019317627\n",
      "step: 3174, loss: 1.6336153745651245\n",
      "step: 3179, loss: 1.4074772596359253\n",
      "Checkpoint is saved\n",
      "step: 3184, loss: 0.9956075549125671\n",
      "step: 3189, loss: 1.0576099157333374\n",
      "step: 3194, loss: 1.2901248931884766\n",
      "step: 3199, loss: 1.0568177700042725\n",
      "Checkpoint is saved\n",
      "step: 3204, loss: 1.1833606958389282\n",
      "step: 3209, loss: 1.6299313306808472\n",
      "step: 3214, loss: 1.7413828372955322\n",
      "step: 3219, loss: 1.3043911457061768\n",
      "Checkpoint is saved\n",
      "step: 3224, loss: 1.7863777875900269\n",
      "step: 3229, loss: 1.2222192287445068\n",
      "step: 3234, loss: 1.5907469987869263\n",
      "step: 3239, loss: 1.1773803234100342\n",
      "Checkpoint is saved\n",
      "step: 3244, loss: 1.1283799409866333\n",
      "step: 3249, loss: 1.6343867778778076\n",
      "step: 3254, loss: 1.5112299919128418\n",
      "step: 3259, loss: 1.6465015411376953\n",
      "Checkpoint is saved\n",
      "step: 3264, loss: 1.116506814956665\n",
      "step: 3269, loss: 1.2061840295791626\n",
      "step: 3274, loss: 1.0975244045257568\n",
      "step: 3279, loss: 1.0970574617385864\n",
      "Checkpoint is saved\n",
      "step: 3284, loss: 0.8530749678611755\n",
      "step: 3289, loss: 1.501227617263794\n",
      "step: 3294, loss: 1.434836983680725\n",
      "step: 3299, loss: 0.7875419855117798\n",
      "Checkpoint is saved\n",
      "step: 3304, loss: 1.280224323272705\n",
      "step: 3309, loss: 1.6790276765823364\n",
      "step: 3314, loss: 1.219658613204956\n",
      "step: 3319, loss: 1.368927240371704\n",
      "Checkpoint is saved\n",
      "step: 3324, loss: 1.1610524654388428\n",
      "step: 3329, loss: 1.1703952550888062\n",
      "step: 3334, loss: 1.3826303482055664\n",
      "step: 3339, loss: 1.51736319065094\n",
      "Checkpoint is saved\n",
      "step: 3344, loss: 0.7089521884918213\n",
      "step: 3349, loss: 1.3238489627838135\n",
      "step: 3354, loss: 1.5433614253997803\n",
      "step: 3359, loss: 1.1470203399658203\n",
      "Checkpoint is saved\n",
      "step: 3364, loss: 0.8741346597671509\n",
      "step: 3369, loss: 1.5809626579284668\n",
      "step: 3374, loss: 1.6677792072296143\n",
      "step: 3379, loss: 1.30613112449646\n",
      "Checkpoint is saved\n",
      "step: 3384, loss: 1.6807661056518555\n",
      "step: 3389, loss: 1.8309664726257324\n",
      "step: 3394, loss: 1.2303733825683594\n",
      "step: 3399, loss: 0.4849325120449066\n",
      "Checkpoint is saved\n",
      "step: 3404, loss: 0.8360607624053955\n",
      "step: 3409, loss: 1.2107020616531372\n",
      "step: 3414, loss: 1.2863160371780396\n",
      "step: 3419, loss: 1.3515945672988892\n",
      "Checkpoint is saved\n",
      "step: 3424, loss: 1.3803794384002686\n",
      "step: 3429, loss: 1.5785343647003174\n",
      "step: 3434, loss: 1.6445140838623047\n",
      "step: 3439, loss: 1.6053423881530762\n",
      "Checkpoint is saved\n",
      "step: 3444, loss: 1.6110810041427612\n",
      "step: 3449, loss: 1.6241843700408936\n",
      "step: 3454, loss: 1.3987631797790527\n",
      "step: 3459, loss: 1.083158254623413\n",
      "Checkpoint is saved\n",
      "step: 3464, loss: 1.487396001815796\n",
      "step: 3469, loss: 1.4328300952911377\n",
      "step: 3474, loss: 1.4702141284942627\n",
      "step: 3479, loss: 2.1788675785064697\n",
      "Checkpoint is saved\n",
      "step: 3484, loss: 1.4181932210922241\n",
      "step: 3489, loss: 0.9491223096847534\n",
      "step: 3494, loss: 0.9263653755187988\n",
      "step: 3499, loss: 0.5984705686569214\n",
      "Checkpoint is saved\n",
      "step: 3504, loss: 1.0925081968307495\n",
      "step: 3509, loss: 1.1463834047317505\n",
      "step: 3514, loss: 1.2365434169769287\n",
      "step: 3519, loss: 1.2834628820419312\n",
      "Checkpoint is saved\n",
      "step: 3524, loss: 1.187453269958496\n",
      "step: 3529, loss: 1.6956512928009033\n",
      "step: 3534, loss: 1.1414539813995361\n",
      "step: 3539, loss: 1.4085942506790161\n",
      "Checkpoint is saved\n",
      "step: 3544, loss: 1.1986485719680786\n",
      "step: 3549, loss: 1.022273302078247\n",
      "step: 3554, loss: 0.8784893751144409\n",
      "step: 3559, loss: 1.350121021270752\n",
      "Checkpoint is saved\n",
      "step: 3564, loss: 1.4526588916778564\n",
      "step: 3569, loss: 1.4723421335220337\n",
      "step: 3574, loss: 1.1913268566131592\n",
      "step: 3579, loss: 1.2757959365844727\n",
      "Checkpoint is saved\n",
      "step: 3584, loss: 1.1062309741973877\n",
      "step: 3589, loss: 1.1811566352844238\n",
      "step: 3594, loss: 1.3693172931671143\n",
      "step: 3599, loss: 1.3506971597671509\n",
      "Checkpoint is saved\n",
      "step: 3604, loss: 1.1029325723648071\n",
      "step: 3609, loss: 1.152882695198059\n",
      "step: 3614, loss: 0.8651676177978516\n",
      "step: 3619, loss: 0.6492440104484558\n",
      "Checkpoint is saved\n",
      "step: 3624, loss: 1.0764250755310059\n",
      "step: 3629, loss: 1.2682905197143555\n",
      "step: 3634, loss: 1.1590595245361328\n",
      "step: 3639, loss: 1.1252672672271729\n",
      "Checkpoint is saved\n",
      "step: 3644, loss: 1.6962125301361084\n",
      "step: 3649, loss: 0.5191525816917419\n",
      "step: 3654, loss: 1.4744133949279785\n",
      "step: 3659, loss: 1.3685691356658936\n",
      "Checkpoint is saved\n",
      "step: 3664, loss: 0.926371693611145\n",
      "step: 3669, loss: 1.283106803894043\n",
      "step: 3674, loss: 1.4231421947479248\n",
      "step: 3679, loss: 1.2379162311553955\n",
      "Checkpoint is saved\n",
      "step: 3684, loss: 1.7521419525146484\n",
      "step: 3689, loss: 0.8540318608283997\n",
      "step: 3694, loss: 1.428733229637146\n",
      "step: 3699, loss: 1.4632364511489868\n",
      "Checkpoint is saved\n",
      "step: 3704, loss: 1.0820947885513306\n",
      "step: 3709, loss: 1.2770931720733643\n",
      "step: 3714, loss: 1.0537203550338745\n",
      "step: 3719, loss: 1.387559413909912\n",
      "Checkpoint is saved\n",
      "step: 3724, loss: 1.0371484756469727\n",
      "step: 3729, loss: 1.5866581201553345\n",
      "step: 3734, loss: 1.2875392436981201\n",
      "step: 3739, loss: 1.693329095840454\n",
      "Checkpoint is saved\n",
      "step: 3744, loss: 1.209269642829895\n",
      "step: 3749, loss: 1.5317490100860596\n",
      "step: 3754, loss: 1.118753433227539\n",
      "step: 3759, loss: 1.4422003030776978\n",
      "Checkpoint is saved\n",
      "step: 3764, loss: 1.5474443435668945\n",
      "step: 3769, loss: 0.8959479331970215\n",
      "step: 3774, loss: 1.3621304035186768\n",
      "step: 3779, loss: 1.8747079372406006\n",
      "Checkpoint is saved\n",
      "step: 3784, loss: 0.7223100066184998\n",
      "step: 3789, loss: 1.3604226112365723\n",
      "step: 3794, loss: 2.7932653427124023\n",
      "step: 3799, loss: 1.6959741115570068\n",
      "Checkpoint is saved\n",
      "step: 3804, loss: 1.1116588115692139\n",
      "step: 3809, loss: 1.5358777046203613\n",
      "step: 3814, loss: 2.1182451248168945\n",
      "step: 3819, loss: 1.0617763996124268\n",
      "Checkpoint is saved\n",
      "step: 3824, loss: 1.1351048946380615\n",
      "step: 3829, loss: 1.7736302614212036\n",
      "step: 3834, loss: 1.30558443069458\n",
      "step: 3839, loss: 1.2361178398132324\n",
      "Checkpoint is saved\n",
      "step: 3844, loss: 1.8325953483581543\n",
      "step: 3849, loss: 1.6540229320526123\n",
      "step: 3854, loss: 1.3666298389434814\n",
      "step: 3859, loss: 0.9882287383079529\n",
      "Checkpoint is saved\n",
      "step: 3864, loss: 1.1096729040145874\n",
      "step: 3869, loss: 1.313823938369751\n",
      "step: 3874, loss: 0.9870821237564087\n",
      "step: 3879, loss: 1.6885546445846558\n",
      "Checkpoint is saved\n",
      "step: 3884, loss: 1.4235165119171143\n",
      "step: 3889, loss: 1.422039270401001\n",
      "step: 3894, loss: 0.9222705364227295\n",
      "step: 3899, loss: 2.1441633701324463\n",
      "Checkpoint is saved\n",
      "step: 3904, loss: 0.847313404083252\n",
      "step: 3909, loss: 1.0821020603179932\n",
      "step: 3914, loss: 0.8438578248023987\n",
      "step: 3919, loss: 0.9316270351409912\n",
      "Checkpoint is saved\n",
      "step: 3924, loss: 1.3585103750228882\n",
      "step: 3929, loss: 1.0745314359664917\n",
      "step: 3934, loss: 1.263314127922058\n",
      "step: 3939, loss: 1.1715185642242432\n",
      "Checkpoint is saved\n",
      "step: 3944, loss: 1.4263222217559814\n",
      "step: 3949, loss: 0.8653571605682373\n",
      "step: 3954, loss: 1.0932037830352783\n",
      "step: 3959, loss: 0.9786184430122375\n",
      "Checkpoint is saved\n",
      "step: 3964, loss: 1.419837236404419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3969, loss: 1.1342170238494873\n",
      "step: 3974, loss: 1.0199674367904663\n",
      "step: 3979, loss: 2.77104115486145\n",
      "Checkpoint is saved\n",
      "step: 3984, loss: 1.7920265197753906\n",
      "step: 3989, loss: 1.841027021408081\n",
      "step: 3994, loss: 1.0062509775161743\n",
      "step: 3999, loss: 1.949478030204773\n",
      "Checkpoint is saved\n",
      "step: 4004, loss: 1.1063730716705322\n",
      "step: 4009, loss: 1.30863618850708\n",
      "step: 4014, loss: 1.5402660369873047\n",
      "step: 4019, loss: 0.9316458702087402\n",
      "Checkpoint is saved\n",
      "step: 4024, loss: 1.4129674434661865\n",
      "step: 4029, loss: 0.9410624504089355\n",
      "step: 4034, loss: 1.4625176191329956\n",
      "step: 4039, loss: 0.5433444976806641\n",
      "Checkpoint is saved\n",
      "step: 4044, loss: 1.4932557344436646\n",
      "step: 4049, loss: 1.360116720199585\n",
      "step: 4054, loss: 1.318915605545044\n",
      "step: 4059, loss: 1.540112018585205\n",
      "Checkpoint is saved\n",
      "step: 4064, loss: 1.4735851287841797\n",
      "step: 4069, loss: 0.6874871850013733\n",
      "step: 4074, loss: 1.2087310552597046\n",
      "step: 4079, loss: 1.3161954879760742\n",
      "Checkpoint is saved\n",
      "step: 4084, loss: 1.802168846130371\n",
      "step: 4089, loss: 1.1245601177215576\n",
      "step: 4094, loss: 0.7959989309310913\n",
      "step: 4099, loss: 0.5543109178543091\n",
      "Checkpoint is saved\n",
      "step: 4104, loss: 1.124433159828186\n",
      "step: 4109, loss: 1.1273491382598877\n",
      "step: 4114, loss: 1.2265453338623047\n",
      "step: 4119, loss: 1.2738959789276123\n",
      "Checkpoint is saved\n",
      "step: 4124, loss: 1.111138105392456\n",
      "step: 4129, loss: 1.134764552116394\n",
      "step: 4134, loss: 1.4382034540176392\n",
      "step: 4139, loss: 1.1518967151641846\n",
      "Checkpoint is saved\n",
      "step: 4144, loss: 1.126389741897583\n",
      "step: 4149, loss: 0.9781438708305359\n",
      "step: 4154, loss: 1.0134845972061157\n",
      "step: 4159, loss: 1.3069806098937988\n",
      "Checkpoint is saved\n",
      "step: 4164, loss: 1.1452968120574951\n",
      "step: 4169, loss: 1.558787226676941\n",
      "step: 4174, loss: 0.8908016681671143\n",
      "step: 4179, loss: 1.8629765510559082\n",
      "Checkpoint is saved\n",
      "step: 4184, loss: 1.2464772462844849\n",
      "step: 4189, loss: 0.8895825147628784\n",
      "step: 4194, loss: 1.1064409017562866\n",
      "step: 4199, loss: 1.73460853099823\n",
      "Checkpoint is saved\n",
      "step: 4204, loss: 1.0850565433502197\n",
      "step: 4209, loss: 2.0538265705108643\n",
      "step: 4214, loss: 1.534053921699524\n",
      "step: 4219, loss: 0.6401509046554565\n",
      "Checkpoint is saved\n",
      "step: 4224, loss: 0.6983822584152222\n",
      "step: 4229, loss: 1.0600569248199463\n",
      "step: 4234, loss: 1.0446250438690186\n",
      "step: 4239, loss: 1.170019507408142\n",
      "Checkpoint is saved\n",
      "step: 4244, loss: 1.4884028434753418\n",
      "step: 4249, loss: 1.274531602859497\n",
      "step: 4254, loss: 0.9983896613121033\n",
      "step: 4259, loss: 1.9029078483581543\n",
      "Checkpoint is saved\n",
      "step: 4264, loss: 0.7253774404525757\n",
      "step: 4269, loss: 0.8976250886917114\n",
      "step: 4274, loss: 2.0056300163269043\n",
      "step: 4279, loss: 1.446952223777771\n",
      "Checkpoint is saved\n",
      "step: 4284, loss: 1.08219313621521\n",
      "step: 4289, loss: 1.103332281112671\n",
      "step: 4294, loss: 1.2976436614990234\n",
      "step: 4299, loss: 1.2874447107315063\n",
      "Checkpoint is saved\n",
      "step: 4304, loss: 0.9614930748939514\n",
      "step: 4309, loss: 1.3526313304901123\n",
      "step: 4314, loss: 0.5257315635681152\n",
      "step: 4319, loss: 1.3257067203521729\n",
      "Checkpoint is saved\n",
      "step: 4324, loss: 1.4713900089263916\n",
      "step: 4329, loss: 1.3515756130218506\n",
      "step: 4334, loss: 0.8802027702331543\n",
      "step: 4339, loss: 1.2052876949310303\n",
      "Checkpoint is saved\n",
      "step: 4344, loss: 1.1566083431243896\n",
      "step: 4349, loss: 1.1007821559906006\n",
      "step: 4354, loss: 1.319334864616394\n",
      "step: 4359, loss: 1.6515120267868042\n",
      "Checkpoint is saved\n",
      "step: 4364, loss: 1.621840000152588\n",
      "step: 4369, loss: 1.542323350906372\n",
      "step: 4374, loss: 0.9418538808822632\n",
      "step: 4379, loss: 1.0760773420333862\n",
      "Checkpoint is saved\n",
      "step: 4384, loss: 1.1821870803833008\n",
      "step: 4389, loss: 0.9837690591812134\n",
      "step: 4394, loss: 0.8870837092399597\n",
      "step: 4399, loss: 1.2037782669067383\n",
      "Checkpoint is saved\n",
      "step: 4404, loss: 1.377774953842163\n",
      "step: 4409, loss: 1.6998957395553589\n",
      "step: 4414, loss: 1.4778478145599365\n",
      "step: 4419, loss: 1.211682677268982\n",
      "Checkpoint is saved\n",
      "step: 4424, loss: 0.9258027076721191\n",
      "step: 4429, loss: 1.1413397789001465\n",
      "step: 4434, loss: 1.4500404596328735\n",
      "step: 4439, loss: 0.8592725992202759\n",
      "Checkpoint is saved\n",
      "step: 4444, loss: 0.8446739315986633\n",
      "step: 4449, loss: 1.173274040222168\n",
      "step: 4454, loss: 0.6226955056190491\n",
      "step: 4459, loss: 1.3207671642303467\n",
      "Checkpoint is saved\n",
      "step: 4464, loss: 0.7409471869468689\n",
      "step: 4469, loss: 0.9182021617889404\n",
      "step: 4474, loss: 1.1664568185806274\n",
      "step: 4479, loss: 1.327425479888916\n",
      "Checkpoint is saved\n",
      "step: 4484, loss: 1.3074429035186768\n",
      "step: 4489, loss: 1.6303331851959229\n",
      "step: 4494, loss: 1.3563698530197144\n",
      "step: 4499, loss: 1.2449061870574951\n",
      "Checkpoint is saved\n",
      "step: 4504, loss: 1.0579402446746826\n",
      "step: 4509, loss: 1.1264920234680176\n",
      "step: 4514, loss: 0.9928176999092102\n",
      "step: 4519, loss: 1.1623408794403076\n",
      "Checkpoint is saved\n",
      "step: 4524, loss: 1.4580475091934204\n",
      "step: 4529, loss: 1.0834465026855469\n",
      "step: 4534, loss: 1.1243823766708374\n",
      "step: 4539, loss: 0.9703413844108582\n",
      "Checkpoint is saved\n",
      "step: 4544, loss: 1.1395392417907715\n",
      "step: 4549, loss: 1.5280048847198486\n",
      "step: 4554, loss: 1.6023197174072266\n",
      "step: 4559, loss: 1.0621519088745117\n",
      "Checkpoint is saved\n",
      "step: 4564, loss: 1.4794954061508179\n",
      "step: 4569, loss: 1.5111284255981445\n",
      "step: 4574, loss: 0.8779459595680237\n",
      "step: 4579, loss: 1.1426416635513306\n",
      "Checkpoint is saved\n",
      "step: 4584, loss: 1.559767484664917\n",
      "step: 4589, loss: 1.1337625980377197\n",
      "step: 4594, loss: 1.279228687286377\n",
      "step: 4599, loss: 1.0260980129241943\n",
      "Checkpoint is saved\n",
      "step: 4604, loss: 0.7370150685310364\n",
      "step: 4609, loss: 1.4041831493377686\n",
      "step: 4614, loss: 1.2665150165557861\n",
      "step: 4619, loss: 0.7910175323486328\n",
      "Checkpoint is saved\n",
      "step: 4624, loss: 1.0368568897247314\n",
      "step: 4629, loss: 1.185042381286621\n",
      "step: 4634, loss: 1.4826622009277344\n",
      "step: 4639, loss: 0.8201302289962769\n",
      "Checkpoint is saved\n",
      "step: 4644, loss: 1.2473742961883545\n",
      "step: 4649, loss: 0.9388701319694519\n",
      "step: 4654, loss: 1.1699497699737549\n",
      "step: 4659, loss: 1.3454090356826782\n",
      "Checkpoint is saved\n",
      "step: 4664, loss: 1.247218132019043\n",
      "step: 4669, loss: 1.3321774005889893\n",
      "step: 4674, loss: 0.4847686290740967\n",
      "step: 4679, loss: 1.436802864074707\n",
      "Checkpoint is saved\n",
      "step: 4684, loss: 1.0126044750213623\n",
      "step: 4689, loss: 1.096667766571045\n",
      "step: 4694, loss: 1.4614323377609253\n",
      "step: 4699, loss: 0.8580372333526611\n",
      "Checkpoint is saved\n",
      "step: 4704, loss: 1.0996429920196533\n",
      "step: 4709, loss: 1.5471985340118408\n",
      "step: 4714, loss: 0.991949200630188\n",
      "step: 4719, loss: 1.5795516967773438\n",
      "Checkpoint is saved\n",
      "step: 4724, loss: 0.7462148666381836\n",
      "step: 4729, loss: 1.4383585453033447\n",
      "step: 4734, loss: 0.8145509958267212\n",
      "step: 4739, loss: 1.291527509689331\n",
      "Checkpoint is saved\n",
      "step: 4744, loss: 0.7862651348114014\n",
      "step: 4749, loss: 1.033875823020935\n",
      "step: 4754, loss: 1.1534013748168945\n",
      "step: 4759, loss: 1.4431359767913818\n",
      "Checkpoint is saved\n",
      "step: 4764, loss: 1.0276072025299072\n",
      "step: 4769, loss: 1.2195122241973877\n",
      "step: 4774, loss: 1.4540174007415771\n",
      "step: 4779, loss: 1.094926118850708\n",
      "Checkpoint is saved\n",
      "step: 4784, loss: 1.3404473066329956\n",
      "step: 4789, loss: 0.9943559169769287\n",
      "step: 4794, loss: 0.9761868715286255\n",
      "step: 4799, loss: 1.5896804332733154\n",
      "Checkpoint is saved\n",
      "step: 4804, loss: 1.5530701875686646\n",
      "step: 4809, loss: 1.4360295534133911\n",
      "step: 4814, loss: 1.1477224826812744\n",
      "step: 4819, loss: 1.1618160009384155\n",
      "Checkpoint is saved\n",
      "step: 4824, loss: 0.8613066077232361\n",
      "step: 4829, loss: 1.3370283842086792\n",
      "step: 4834, loss: 0.8282276391983032\n",
      "step: 4839, loss: 1.1217224597930908\n",
      "Checkpoint is saved\n",
      "step: 4844, loss: 1.4967178106307983\n",
      "step: 4849, loss: 0.7792593240737915\n",
      "step: 4854, loss: 1.3739184141159058\n",
      "step: 4859, loss: 1.1226065158843994\n",
      "Checkpoint is saved\n",
      "step: 4864, loss: 0.9760032296180725\n",
      "step: 4869, loss: 1.3611488342285156\n",
      "step: 4874, loss: 1.1990935802459717\n",
      "step: 4879, loss: 0.9212250113487244\n",
      "Checkpoint is saved\n",
      "step: 4884, loss: 1.075251817703247\n",
      "step: 4889, loss: 1.3495469093322754\n",
      "step: 4894, loss: 0.9418030381202698\n",
      "step: 4899, loss: 1.324314832687378\n",
      "Checkpoint is saved\n",
      "step: 4904, loss: 1.0091915130615234\n",
      "step: 4909, loss: 1.3852801322937012\n",
      "step: 4914, loss: 0.7874308228492737\n",
      "step: 4919, loss: 1.589275598526001\n",
      "Checkpoint is saved\n",
      "step: 4924, loss: 1.1774595975875854\n",
      "step: 4929, loss: 1.1988048553466797\n",
      "step: 4934, loss: 1.7141499519348145\n",
      "step: 4939, loss: 1.2049753665924072\n",
      "Checkpoint is saved\n",
      "step: 4944, loss: 0.9010970592498779\n",
      "step: 4949, loss: 0.9009624123573303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4954, loss: 1.3484618663787842\n",
      "step: 4959, loss: 1.1631529331207275\n",
      "Checkpoint is saved\n",
      "step: 4964, loss: 1.0160874128341675\n",
      "step: 4969, loss: 1.1592642068862915\n",
      "step: 4974, loss: 1.392019271850586\n",
      "step: 4979, loss: 0.9706223011016846\n",
      "Checkpoint is saved\n",
      "step: 4984, loss: 1.0907431840896606\n",
      "step: 4989, loss: 0.8838772177696228\n",
      "step: 4994, loss: 1.0445632934570312\n",
      "step: 4999, loss: 0.9037281274795532\n",
      "Checkpoint is saved\n",
      "step: 5004, loss: 1.004425048828125\n",
      "step: 5009, loss: 0.9750213623046875\n",
      "step: 5014, loss: 1.3009713888168335\n",
      "step: 5019, loss: 0.5650699734687805\n",
      "Checkpoint is saved\n",
      "step: 5024, loss: 1.1145915985107422\n",
      "step: 5029, loss: 1.2258260250091553\n",
      "step: 5034, loss: 1.7749015092849731\n",
      "step: 5039, loss: 1.2537879943847656\n",
      "Checkpoint is saved\n",
      "step: 5044, loss: 1.0474846363067627\n",
      "step: 5049, loss: 1.5648452043533325\n",
      "step: 5054, loss: 1.0003678798675537\n",
      "step: 5059, loss: 1.4636945724487305\n",
      "Checkpoint is saved\n",
      "step: 5064, loss: 1.0017430782318115\n",
      "step: 5069, loss: 1.7372335195541382\n",
      "step: 5074, loss: 1.1830109357833862\n",
      "step: 5079, loss: 0.7271790504455566\n",
      "Checkpoint is saved\n",
      "step: 5084, loss: 1.3939825296401978\n",
      "step: 5089, loss: 1.4064459800720215\n",
      "step: 5094, loss: 1.0088790655136108\n",
      "step: 5099, loss: 1.3129843473434448\n",
      "Checkpoint is saved\n",
      "step: 5104, loss: 1.7633755207061768\n",
      "step: 5109, loss: 1.0636401176452637\n",
      "step: 5114, loss: 1.5636088848114014\n",
      "step: 5119, loss: 0.555263340473175\n",
      "Checkpoint is saved\n",
      "step: 5124, loss: 0.8997812271118164\n",
      "step: 5129, loss: 1.333890438079834\n",
      "step: 5134, loss: 1.2148363590240479\n",
      "step: 5139, loss: 1.158153772354126\n",
      "Checkpoint is saved\n",
      "step: 5144, loss: 0.9688553810119629\n",
      "step: 5149, loss: 1.3463696241378784\n",
      "step: 5154, loss: 1.4872204065322876\n",
      "step: 5159, loss: 0.8139557242393494\n",
      "Checkpoint is saved\n",
      "step: 5164, loss: 1.3335957527160645\n",
      "step: 5169, loss: 0.9792838096618652\n",
      "step: 5174, loss: 0.8754345178604126\n",
      "step: 5179, loss: 1.1023049354553223\n",
      "Checkpoint is saved\n",
      "step: 5184, loss: 1.0657154321670532\n",
      "step: 5189, loss: 1.0651867389678955\n",
      "step: 5194, loss: 1.1205401420593262\n",
      "step: 5199, loss: 0.8108425140380859\n",
      "Checkpoint is saved\n",
      "step: 5204, loss: 1.3251640796661377\n",
      "step: 5209, loss: 1.426034688949585\n",
      "step: 5214, loss: 0.5738128423690796\n",
      "step: 5219, loss: 1.5614752769470215\n",
      "Checkpoint is saved\n",
      "step: 5224, loss: 0.8997242450714111\n",
      "step: 5229, loss: 0.5999306440353394\n",
      "step: 5234, loss: 1.2514116764068604\n",
      "step: 5239, loss: 0.8496160507202148\n",
      "Checkpoint is saved\n",
      "step: 5244, loss: 1.4586999416351318\n",
      "step: 5249, loss: 1.2922641038894653\n",
      "step: 5254, loss: 1.2719717025756836\n",
      "step: 5259, loss: 1.3973615169525146\n",
      "Checkpoint is saved\n",
      "step: 5264, loss: 1.3540186882019043\n",
      "step: 5269, loss: 0.7061423063278198\n",
      "step: 5274, loss: 1.1519975662231445\n",
      "step: 5279, loss: 0.9122329950332642\n",
      "Checkpoint is saved\n",
      "step: 5284, loss: 0.8438687324523926\n",
      "step: 5289, loss: 1.726830244064331\n",
      "step: 5294, loss: 1.7313110828399658\n",
      "step: 5299, loss: 0.7943324446678162\n",
      "Checkpoint is saved\n",
      "step: 5304, loss: 1.3803107738494873\n",
      "step: 5309, loss: 1.168149471282959\n",
      "step: 5314, loss: 1.3844525814056396\n",
      "step: 5319, loss: 0.9175190925598145\n",
      "Checkpoint is saved\n",
      "step: 5324, loss: 1.6208124160766602\n",
      "step: 5329, loss: 0.6945310235023499\n",
      "step: 5334, loss: 0.7361898422241211\n",
      "step: 5339, loss: 1.0778815746307373\n",
      "Checkpoint is saved\n",
      "step: 5344, loss: 0.7660540342330933\n",
      "step: 5349, loss: 1.4949538707733154\n",
      "step: 5354, loss: 0.8704319000244141\n",
      "step: 5359, loss: 1.2116656303405762\n",
      "Checkpoint is saved\n",
      "step: 5364, loss: 0.567110538482666\n",
      "step: 5369, loss: 0.8235755562782288\n",
      "step: 5374, loss: 0.9880398511886597\n",
      "step: 5379, loss: 1.3271205425262451\n",
      "Checkpoint is saved\n",
      "step: 5384, loss: 1.5118517875671387\n",
      "step: 5389, loss: 1.0391802787780762\n",
      "step: 5394, loss: 1.2614634037017822\n",
      "step: 5399, loss: 1.488023042678833\n",
      "Checkpoint is saved\n",
      "step: 5404, loss: 0.8634727001190186\n",
      "step: 5409, loss: 1.098799705505371\n",
      "step: 5414, loss: 0.7632805109024048\n",
      "step: 5419, loss: 0.9654223918914795\n",
      "Checkpoint is saved\n",
      "step: 5424, loss: 1.5831133127212524\n",
      "step: 5429, loss: 0.9100449085235596\n",
      "step: 5434, loss: 0.6350435018539429\n",
      "step: 5439, loss: 1.3053038120269775\n",
      "Checkpoint is saved\n",
      "step: 5444, loss: 1.116373062133789\n",
      "step: 5449, loss: 1.1328834295272827\n",
      "step: 5454, loss: 1.231614112854004\n",
      "step: 5459, loss: 0.9843612909317017\n",
      "Checkpoint is saved\n",
      "step: 5464, loss: 1.5490238666534424\n",
      "step: 5469, loss: 1.2864480018615723\n",
      "step: 5474, loss: 0.7177453637123108\n",
      "step: 5479, loss: 0.9716610908508301\n",
      "Checkpoint is saved\n",
      "step: 5484, loss: 1.0305869579315186\n",
      "step: 5489, loss: 0.9311562180519104\n",
      "step: 5494, loss: 0.7519388794898987\n",
      "step: 5499, loss: 0.9099487066268921\n",
      "Checkpoint is saved\n",
      "step: 5504, loss: 1.0159215927124023\n",
      "step: 5509, loss: 0.7330185174942017\n",
      "step: 5514, loss: 1.0023376941680908\n",
      "step: 5519, loss: 0.9865872263908386\n",
      "Checkpoint is saved\n",
      "step: 5524, loss: 1.4956731796264648\n",
      "step: 5529, loss: 0.8101893067359924\n",
      "step: 5534, loss: 1.2899237871170044\n",
      "step: 5539, loss: 0.7263665199279785\n",
      "Checkpoint is saved\n",
      "step: 5544, loss: 1.2680816650390625\n",
      "step: 5549, loss: 0.887470543384552\n",
      "step: 5554, loss: 1.6597645282745361\n",
      "step: 5559, loss: 1.1370444297790527\n",
      "Checkpoint is saved\n",
      "step: 5564, loss: 1.133697509765625\n",
      "step: 5569, loss: 1.4332849979400635\n",
      "step: 5574, loss: 1.4394348859786987\n",
      "step: 5579, loss: 1.278891682624817\n",
      "Checkpoint is saved\n",
      "step: 5584, loss: 0.8584168553352356\n",
      "step: 5589, loss: 1.2006410360336304\n",
      "step: 5594, loss: 0.6670627593994141\n",
      "step: 5599, loss: 1.0212831497192383\n",
      "Checkpoint is saved\n",
      "step: 5604, loss: 1.705856204032898\n",
      "step: 5609, loss: 0.6679550409317017\n",
      "step: 5614, loss: 1.6525505781173706\n",
      "step: 5619, loss: 1.0133628845214844\n",
      "Checkpoint is saved\n",
      "step: 5624, loss: 1.0586137771606445\n",
      "step: 5629, loss: 0.7873265743255615\n",
      "step: 5634, loss: 1.241798758506775\n",
      "step: 5639, loss: 1.377943515777588\n",
      "Checkpoint is saved\n",
      "step: 5644, loss: 1.658583402633667\n",
      "step: 5649, loss: 1.038395643234253\n",
      "step: 5654, loss: 1.3556305170059204\n",
      "step: 5659, loss: 1.0399675369262695\n",
      "Checkpoint is saved\n",
      "step: 5664, loss: 0.9441372156143188\n",
      "step: 5669, loss: 0.809951901435852\n",
      "step: 5674, loss: 1.1843276023864746\n",
      "step: 5679, loss: 0.9860893487930298\n",
      "Checkpoint is saved\n",
      "step: 5684, loss: 1.247084379196167\n",
      "step: 5689, loss: 1.1521674394607544\n",
      "step: 5694, loss: 1.1025660037994385\n",
      "step: 5699, loss: 1.4903686046600342\n",
      "Checkpoint is saved\n",
      "step: 5704, loss: 1.4505727291107178\n",
      "step: 5709, loss: 1.080704927444458\n",
      "step: 5714, loss: 1.3423352241516113\n",
      "step: 5719, loss: 0.9142427444458008\n",
      "Checkpoint is saved\n",
      "step: 5724, loss: 0.9316614866256714\n",
      "step: 5729, loss: 0.9981829524040222\n",
      "step: 5734, loss: 1.3238580226898193\n",
      "step: 5739, loss: 1.317091941833496\n",
      "Checkpoint is saved\n",
      "step: 5744, loss: 1.1587514877319336\n",
      "step: 5749, loss: 1.1731586456298828\n",
      "step: 5754, loss: 1.558565378189087\n",
      "step: 5759, loss: 1.5968654155731201\n",
      "Checkpoint is saved\n",
      "step: 5764, loss: 1.492260456085205\n",
      "step: 5769, loss: 1.3159723281860352\n",
      "step: 5774, loss: 1.1409621238708496\n",
      "step: 5779, loss: 1.1230549812316895\n",
      "Checkpoint is saved\n",
      "step: 5784, loss: 0.7606658935546875\n",
      "step: 5789, loss: 1.429631233215332\n",
      "step: 5794, loss: 0.9539849758148193\n",
      "step: 5799, loss: 1.1043131351470947\n",
      "Checkpoint is saved\n",
      "step: 5804, loss: 0.8472141027450562\n",
      "step: 5809, loss: 1.4036743640899658\n",
      "step: 5814, loss: 0.9836828112602234\n",
      "step: 5819, loss: 1.363509178161621\n",
      "Checkpoint is saved\n",
      "step: 5824, loss: 1.0763781070709229\n",
      "step: 5829, loss: 1.1843745708465576\n",
      "step: 5834, loss: 1.107351541519165\n",
      "step: 5839, loss: 0.8598914742469788\n",
      "Checkpoint is saved\n",
      "step: 5844, loss: 0.6094441413879395\n",
      "step: 5849, loss: 1.2703806161880493\n",
      "step: 5854, loss: 1.1851310729980469\n",
      "step: 5859, loss: 1.1473783254623413\n",
      "Checkpoint is saved\n",
      "step: 5864, loss: 1.4082947969436646\n",
      "step: 5869, loss: 0.9791052937507629\n",
      "step: 5874, loss: 1.0918105840682983\n",
      "step: 5879, loss: 0.5730819702148438\n",
      "Checkpoint is saved\n",
      "step: 5884, loss: 1.291236162185669\n",
      "step: 5889, loss: 1.5956600904464722\n",
      "step: 5894, loss: 0.8451242446899414\n",
      "step: 5899, loss: 1.1073814630508423\n",
      "Checkpoint is saved\n",
      "step: 5904, loss: 1.1676714420318604\n",
      "step: 5909, loss: 1.2684211730957031\n",
      "step: 5914, loss: 0.9189115762710571\n",
      "step: 5919, loss: 1.045419692993164\n",
      "Checkpoint is saved\n",
      "step: 5924, loss: 1.1398217678070068\n",
      "step: 5929, loss: 0.8661271333694458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5934, loss: 1.7030452489852905\n",
      "step: 5939, loss: 1.5246063470840454\n",
      "Checkpoint is saved\n",
      "step: 5944, loss: 1.0500915050506592\n",
      "step: 5949, loss: 0.6712859272956848\n",
      "step: 5954, loss: 1.0315951108932495\n",
      "step: 5959, loss: 1.1605689525604248\n",
      "Checkpoint is saved\n",
      "step: 5964, loss: 1.35678231716156\n",
      "step: 5969, loss: 1.7244024276733398\n",
      "step: 5974, loss: 0.6913909912109375\n",
      "step: 5979, loss: 1.2502524852752686\n",
      "Checkpoint is saved\n",
      "step: 5984, loss: 1.2206672430038452\n",
      "step: 5989, loss: 0.866767406463623\n",
      "step: 5994, loss: 0.9701200723648071\n",
      "step: 5999, loss: 1.0031086206436157\n",
      "Checkpoint is saved\n",
      "step: 6004, loss: 1.0545940399169922\n",
      "step: 6009, loss: 1.010063648223877\n",
      "step: 6014, loss: 0.8707854747772217\n",
      "step: 6019, loss: 1.1343014240264893\n",
      "Checkpoint is saved\n",
      "step: 6024, loss: 1.308975100517273\n",
      "step: 6029, loss: 1.0067222118377686\n",
      "step: 6034, loss: 1.125925064086914\n",
      "step: 6039, loss: 0.9315529465675354\n",
      "Checkpoint is saved\n",
      "step: 6044, loss: 0.9052364826202393\n",
      "step: 6049, loss: 1.4948804378509521\n",
      "step: 6054, loss: 1.5365506410598755\n",
      "step: 6059, loss: 1.5872986316680908\n",
      "Checkpoint is saved\n",
      "step: 6064, loss: 0.7905839681625366\n",
      "step: 6069, loss: 1.0963259935379028\n",
      "step: 6074, loss: 0.7049131393432617\n",
      "step: 6079, loss: 0.8444651961326599\n",
      "Checkpoint is saved\n",
      "step: 6084, loss: 0.3844641447067261\n",
      "step: 6089, loss: 0.7139260172843933\n",
      "step: 6094, loss: 1.4428703784942627\n",
      "step: 6099, loss: 1.3371363878250122\n",
      "Checkpoint is saved\n",
      "step: 6104, loss: 1.2208616733551025\n",
      "step: 6109, loss: 0.7994984984397888\n",
      "step: 6114, loss: 1.0867836475372314\n",
      "step: 6119, loss: 1.048269510269165\n",
      "Checkpoint is saved\n",
      "step: 6124, loss: 1.0885266065597534\n",
      "step: 6129, loss: 1.1233073472976685\n",
      "step: 6134, loss: 1.1409317255020142\n",
      "step: 6139, loss: 0.909546434879303\n",
      "Checkpoint is saved\n",
      "step: 6144, loss: 0.9266842603683472\n",
      "step: 6149, loss: 1.3277685642242432\n",
      "step: 6154, loss: 0.8767167329788208\n",
      "step: 6159, loss: 1.1054599285125732\n",
      "Checkpoint is saved\n",
      "step: 6164, loss: 1.0888900756835938\n",
      "step: 6169, loss: 1.0855611562728882\n",
      "step: 6174, loss: 1.1267244815826416\n",
      "step: 6179, loss: 0.7408119440078735\n",
      "Checkpoint is saved\n",
      "step: 6184, loss: 0.8824866414070129\n",
      "step: 6189, loss: 0.9131022691726685\n",
      "step: 6194, loss: 1.575855016708374\n",
      "step: 6199, loss: 0.6386508345603943\n",
      "Checkpoint is saved\n",
      "step: 6204, loss: 1.0536093711853027\n",
      "step: 6209, loss: 1.3097801208496094\n",
      "step: 6214, loss: 0.807373046875\n",
      "step: 6219, loss: 0.9042623043060303\n",
      "Checkpoint is saved\n",
      "step: 6224, loss: 1.2234699726104736\n",
      "step: 6229, loss: 0.7556862831115723\n",
      "step: 6234, loss: 0.999111533164978\n",
      "step: 6239, loss: 0.9644434452056885\n",
      "Checkpoint is saved\n",
      "step: 6244, loss: 0.9318796396255493\n",
      "step: 6249, loss: 1.2062551975250244\n",
      "step: 6254, loss: 1.036801815032959\n",
      "step: 6259, loss: 1.2941290140151978\n",
      "Checkpoint is saved\n",
      "step: 6264, loss: 0.9017426371574402\n",
      "step: 6269, loss: 1.0518150329589844\n",
      "step: 6274, loss: 1.0032806396484375\n",
      "step: 6279, loss: 0.6940229535102844\n",
      "Checkpoint is saved\n",
      "step: 6284, loss: 0.9541711807250977\n",
      "step: 6289, loss: 1.0256156921386719\n",
      "step: 6294, loss: 0.7274818420410156\n",
      "step: 6299, loss: 1.1024153232574463\n",
      "Checkpoint is saved\n",
      "step: 6304, loss: 1.0038998126983643\n",
      "step: 6309, loss: 1.1009629964828491\n",
      "step: 6314, loss: 1.1714305877685547\n",
      "step: 6319, loss: 0.9545910358428955\n",
      "Checkpoint is saved\n",
      "step: 6324, loss: 1.459108829498291\n",
      "step: 6329, loss: 1.2286494970321655\n",
      "step: 6334, loss: 1.2902239561080933\n",
      "step: 6339, loss: 0.7332580089569092\n",
      "Checkpoint is saved\n",
      "step: 6344, loss: 1.4535651206970215\n",
      "step: 6349, loss: 1.021978735923767\n",
      "step: 6354, loss: 0.8692176938056946\n",
      "step: 6359, loss: 1.3037002086639404\n",
      "Checkpoint is saved\n",
      "step: 6364, loss: 1.0462329387664795\n",
      "step: 6369, loss: 1.3698270320892334\n",
      "step: 6374, loss: 1.1460673809051514\n",
      "step: 6379, loss: 1.0294522047042847\n",
      "Checkpoint is saved\n",
      "step: 6384, loss: 0.8344587087631226\n",
      "step: 6389, loss: 0.9794881939888\n",
      "step: 6394, loss: 1.05946946144104\n",
      "step: 6399, loss: 0.8196429014205933\n",
      "Checkpoint is saved\n",
      "step: 6404, loss: 1.3581620454788208\n",
      "step: 6409, loss: 1.0282357931137085\n",
      "step: 6414, loss: 1.1757080554962158\n",
      "step: 6419, loss: 1.340636968612671\n",
      "Checkpoint is saved\n",
      "step: 6424, loss: 0.7735142111778259\n",
      "step: 6429, loss: 1.3378609418869019\n",
      "step: 6434, loss: 0.9314993023872375\n",
      "step: 6439, loss: 0.7793923020362854\n",
      "Checkpoint is saved\n",
      "step: 6444, loss: 1.569737434387207\n",
      "step: 6449, loss: 0.9492975473403931\n",
      "step: 6454, loss: 1.0183078050613403\n",
      "step: 6459, loss: 0.6495216488838196\n",
      "Checkpoint is saved\n",
      "step: 6464, loss: 1.2621829509735107\n",
      "step: 6469, loss: 1.105522871017456\n",
      "step: 6474, loss: 1.1670557260513306\n",
      "step: 6479, loss: 1.0138440132141113\n",
      "Checkpoint is saved\n",
      "step: 6484, loss: 1.1358972787857056\n",
      "step: 6489, loss: 1.024843454360962\n",
      "step: 6494, loss: 1.2864267826080322\n",
      "step: 6499, loss: 0.9730459451675415\n",
      "Checkpoint is saved\n",
      "step: 6504, loss: 0.7892237901687622\n",
      "step: 6509, loss: 1.3754146099090576\n",
      "step: 6514, loss: 1.171042561531067\n",
      "step: 6519, loss: 1.4485660791397095\n",
      "Checkpoint is saved\n",
      "step: 6524, loss: 1.3682405948638916\n",
      "step: 6529, loss: 1.1066136360168457\n",
      "step: 6534, loss: 0.9530421495437622\n",
      "step: 6539, loss: 0.9538722634315491\n",
      "Checkpoint is saved\n",
      "step: 6544, loss: 1.128033995628357\n",
      "step: 6549, loss: 1.0472317934036255\n",
      "step: 6554, loss: 1.011218547821045\n",
      "step: 6559, loss: 0.9531902074813843\n",
      "Checkpoint is saved\n",
      "step: 6564, loss: 0.8299518823623657\n",
      "step: 6569, loss: 1.283228874206543\n",
      "step: 6574, loss: 1.1023885011672974\n",
      "step: 6579, loss: 0.9196116328239441\n",
      "Checkpoint is saved\n",
      "step: 6584, loss: 1.0990842580795288\n",
      "step: 6589, loss: 0.652940034866333\n",
      "step: 6594, loss: 0.9557704925537109\n",
      "step: 6599, loss: 0.9595381021499634\n",
      "Checkpoint is saved\n",
      "step: 6604, loss: 0.8317775130271912\n",
      "step: 6609, loss: 0.886008083820343\n",
      "step: 6614, loss: 0.9033116102218628\n",
      "step: 6619, loss: 1.1001505851745605\n",
      "Checkpoint is saved\n",
      "step: 6624, loss: 0.8922126889228821\n",
      "step: 6629, loss: 1.1345312595367432\n",
      "step: 6634, loss: 1.22887122631073\n",
      "step: 6639, loss: 1.1842169761657715\n",
      "Checkpoint is saved\n",
      "step: 6644, loss: 0.563848078250885\n",
      "step: 6649, loss: 1.2783154249191284\n",
      "step: 6654, loss: 0.9945771098136902\n",
      "step: 6659, loss: 0.5988665819168091\n",
      "Checkpoint is saved\n",
      "step: 6664, loss: 1.6554536819458008\n",
      "step: 6669, loss: 0.8972154855728149\n",
      "step: 6674, loss: 1.7574902772903442\n",
      "step: 6679, loss: 0.5659759640693665\n",
      "Checkpoint is saved\n",
      "step: 6684, loss: 0.9854692816734314\n",
      "step: 6689, loss: 0.8926664590835571\n",
      "step: 6694, loss: 1.0610508918762207\n",
      "step: 6699, loss: 0.5700736045837402\n",
      "Checkpoint is saved\n",
      "step: 6704, loss: 1.005031704902649\n",
      "step: 6709, loss: 0.6558133363723755\n",
      "step: 6714, loss: 0.86473548412323\n",
      "step: 6719, loss: 0.6491656303405762\n",
      "Checkpoint is saved\n",
      "step: 6724, loss: 0.8903015851974487\n",
      "step: 6729, loss: 1.112607479095459\n",
      "step: 6734, loss: 1.1460210084915161\n",
      "step: 6739, loss: 0.7480372786521912\n",
      "Checkpoint is saved\n",
      "step: 6744, loss: 0.7373567223548889\n",
      "step: 6749, loss: 1.0192749500274658\n",
      "step: 6754, loss: 1.2615987062454224\n",
      "step: 6759, loss: 0.8583190441131592\n",
      "Checkpoint is saved\n",
      "step: 6764, loss: 0.8285410404205322\n",
      "step: 6769, loss: 1.0247265100479126\n",
      "step: 6774, loss: 1.2428181171417236\n",
      "step: 6779, loss: 1.5225435495376587\n",
      "Checkpoint is saved\n",
      "step: 6784, loss: 0.8945369720458984\n",
      "step: 6789, loss: 0.8095989227294922\n",
      "step: 6794, loss: 1.0414090156555176\n",
      "step: 6799, loss: 1.1667864322662354\n",
      "Checkpoint is saved\n",
      "step: 6804, loss: 0.747456431388855\n",
      "step: 6809, loss: 1.1798672676086426\n",
      "step: 6814, loss: 0.5555660724639893\n",
      "step: 6819, loss: 1.3847256898880005\n",
      "Checkpoint is saved\n",
      "step: 6824, loss: 1.3996481895446777\n",
      "step: 6829, loss: 0.5176293849945068\n",
      "step: 6834, loss: 0.9198850393295288\n",
      "step: 6839, loss: 0.9378602504730225\n",
      "Checkpoint is saved\n",
      "step: 6844, loss: 0.7136564254760742\n",
      "step: 6849, loss: 0.8009580373764038\n",
      "step: 6854, loss: 0.9130008816719055\n",
      "step: 6859, loss: 0.9169780015945435\n",
      "Checkpoint is saved\n",
      "step: 6864, loss: 0.8200893402099609\n",
      "step: 6869, loss: 0.7064125537872314\n",
      "step: 6874, loss: 1.3574020862579346\n",
      "step: 6879, loss: 1.121208667755127\n",
      "Checkpoint is saved\n",
      "step: 6884, loss: 1.2254719734191895\n",
      "step: 6889, loss: 0.7819939851760864\n",
      "step: 6894, loss: 1.1856886148452759\n",
      "step: 6899, loss: 0.9681646823883057\n",
      "Checkpoint is saved\n",
      "step: 6904, loss: 1.022265076637268\n",
      "step: 6909, loss: 1.092927098274231\n",
      "step: 6914, loss: 1.0467519760131836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6919, loss: 1.0533835887908936\n",
      "Checkpoint is saved\n",
      "step: 6924, loss: 1.2027292251586914\n",
      "step: 6929, loss: 1.0355868339538574\n",
      "step: 6934, loss: 1.168471097946167\n",
      "step: 6939, loss: 0.5679974555969238\n",
      "Checkpoint is saved\n",
      "step: 6944, loss: 0.9306551814079285\n",
      "step: 6949, loss: 1.0005804300308228\n",
      "step: 6954, loss: 0.6420421600341797\n",
      "step: 6959, loss: 1.194289207458496\n",
      "Checkpoint is saved\n",
      "step: 6964, loss: 1.4125635623931885\n",
      "step: 6969, loss: 0.9648751020431519\n",
      "step: 6974, loss: 1.2962141036987305\n",
      "step: 6979, loss: 0.8930322527885437\n",
      "Checkpoint is saved\n",
      "step: 6984, loss: 1.0830358266830444\n",
      "step: 6989, loss: 1.2783334255218506\n",
      "step: 6994, loss: 1.2778397798538208\n",
      "step: 6999, loss: 0.8431824445724487\n",
      "Checkpoint is saved\n",
      "step: 7004, loss: 0.8213510513305664\n",
      "step: 7009, loss: 1.2958475351333618\n",
      "step: 7014, loss: 1.3190011978149414\n",
      "step: 7019, loss: 1.3292648792266846\n",
      "Checkpoint is saved\n",
      "step: 7024, loss: 0.4912186861038208\n",
      "step: 7029, loss: 0.8499529361724854\n",
      "step: 7034, loss: 0.5768980979919434\n",
      "step: 7039, loss: 1.1543664932250977\n",
      "Checkpoint is saved\n",
      "step: 7044, loss: 1.269693374633789\n",
      "step: 7049, loss: 1.024885654449463\n",
      "step: 7054, loss: 1.5393919944763184\n",
      "step: 7059, loss: 0.8792674541473389\n",
      "Checkpoint is saved\n",
      "step: 7064, loss: 1.3127878904342651\n",
      "step: 7069, loss: 1.048866868019104\n",
      "step: 7074, loss: 1.0309593677520752\n",
      "step: 7079, loss: 0.9168481230735779\n",
      "Checkpoint is saved\n",
      "step: 7084, loss: 1.5216914415359497\n",
      "step: 7089, loss: 0.9559210538864136\n",
      "step: 7094, loss: 0.9036126136779785\n",
      "step: 7099, loss: 1.049910068511963\n",
      "Checkpoint is saved\n",
      "step: 7104, loss: 1.1873962879180908\n",
      "step: 7109, loss: 1.1609938144683838\n",
      "step: 7114, loss: 0.8470814228057861\n",
      "step: 7119, loss: 1.2686023712158203\n",
      "Checkpoint is saved\n",
      "step: 7124, loss: 0.5817031860351562\n",
      "step: 7129, loss: 1.3490402698516846\n",
      "step: 7134, loss: 1.2255949974060059\n",
      "step: 7139, loss: 1.119002103805542\n",
      "Checkpoint is saved\n",
      "step: 7144, loss: 0.9698059558868408\n",
      "step: 7149, loss: 1.03342866897583\n",
      "step: 7154, loss: 0.6942384243011475\n",
      "step: 7159, loss: 0.9379712343215942\n",
      "Checkpoint is saved\n",
      "step: 7164, loss: 1.5270183086395264\n",
      "step: 7169, loss: 1.051112413406372\n",
      "step: 7174, loss: 0.5536972284317017\n",
      "step: 7179, loss: 1.189408779144287\n",
      "Checkpoint is saved\n",
      "step: 7184, loss: 1.3501269817352295\n",
      "step: 7189, loss: 0.7455483675003052\n",
      "step: 7194, loss: 0.7795513272285461\n",
      "step: 7199, loss: 0.6370841264724731\n",
      "Checkpoint is saved\n",
      "step: 7204, loss: 1.175635814666748\n",
      "step: 7209, loss: 0.8263076543807983\n",
      "step: 7214, loss: 0.8848947286605835\n",
      "step: 7219, loss: 1.3148829936981201\n",
      "Checkpoint is saved\n",
      "step: 7224, loss: 0.8920781016349792\n",
      "step: 7229, loss: 1.2066060304641724\n",
      "step: 7234, loss: 0.969304084777832\n",
      "step: 7239, loss: 1.4948192834854126\n",
      "Checkpoint is saved\n",
      "step: 7244, loss: 1.020594835281372\n",
      "step: 7249, loss: 1.4156110286712646\n",
      "step: 7254, loss: 0.983528733253479\n",
      "step: 7259, loss: 0.8825751543045044\n",
      "Checkpoint is saved\n",
      "step: 7264, loss: 0.9149971008300781\n",
      "step: 7269, loss: 0.692979633808136\n",
      "step: 7274, loss: 0.8560435771942139\n",
      "step: 7279, loss: 1.0052284002304077\n",
      "Checkpoint is saved\n",
      "step: 7284, loss: 1.2999236583709717\n",
      "step: 7289, loss: 0.7544823288917542\n",
      "step: 7294, loss: 1.065549612045288\n",
      "step: 7299, loss: 0.7730700373649597\n",
      "Checkpoint is saved\n",
      "step: 7304, loss: 0.8009997606277466\n",
      "step: 7309, loss: 0.8529048562049866\n",
      "step: 7314, loss: 1.2789860963821411\n",
      "step: 7319, loss: 1.0211728811264038\n",
      "Checkpoint is saved\n",
      "step: 7324, loss: 0.7790764570236206\n",
      "step: 7329, loss: 0.5446107387542725\n",
      "step: 7334, loss: 1.4357441663742065\n",
      "step: 7339, loss: 1.276918649673462\n",
      "Checkpoint is saved\n",
      "step: 7344, loss: 1.0420360565185547\n",
      "step: 7349, loss: 1.0684289932250977\n",
      "step: 7354, loss: 0.7930039167404175\n",
      "step: 7359, loss: 1.1226630210876465\n",
      "Checkpoint is saved\n",
      "step: 7364, loss: 0.782548189163208\n",
      "step: 7369, loss: 0.635269820690155\n",
      "step: 7374, loss: 1.1203343868255615\n",
      "step: 7379, loss: 0.6552823781967163\n",
      "Checkpoint is saved\n",
      "step: 7384, loss: 0.833233118057251\n",
      "step: 7389, loss: 1.4184024333953857\n",
      "step: 7394, loss: 1.2519959211349487\n",
      "step: 7399, loss: 0.9299450516700745\n",
      "Checkpoint is saved\n",
      "step: 7404, loss: 1.2016552686691284\n",
      "step: 7409, loss: 1.039526343345642\n",
      "step: 7414, loss: 0.809173583984375\n",
      "step: 7419, loss: 0.588451623916626\n",
      "Checkpoint is saved\n",
      "step: 7424, loss: 1.290766954421997\n",
      "step: 7429, loss: 1.012314796447754\n",
      "step: 7434, loss: 1.2927918434143066\n",
      "step: 7439, loss: 1.0902321338653564\n",
      "Checkpoint is saved\n",
      "step: 7444, loss: 1.2219053506851196\n",
      "step: 7449, loss: 1.059120774269104\n",
      "step: 7454, loss: 1.1306443214416504\n",
      "step: 7459, loss: 0.9067674875259399\n",
      "Checkpoint is saved\n",
      "step: 7464, loss: 1.2598421573638916\n",
      "step: 7469, loss: 0.9250122904777527\n",
      "step: 7474, loss: 1.356711506843567\n",
      "step: 7479, loss: 1.1074209213256836\n",
      "Checkpoint is saved\n",
      "step: 7484, loss: 1.1754748821258545\n",
      "step: 7489, loss: 0.9119495153427124\n",
      "step: 7494, loss: 1.2147331237792969\n",
      "step: 7499, loss: 1.1270636320114136\n",
      "Checkpoint is saved\n",
      "step: 7504, loss: 0.8003172278404236\n",
      "step: 7509, loss: 1.1002984046936035\n",
      "step: 7514, loss: 1.1275871992111206\n",
      "step: 7519, loss: 1.3146414756774902\n",
      "Checkpoint is saved\n",
      "step: 7524, loss: 0.6304733753204346\n",
      "step: 7529, loss: 0.9134306311607361\n",
      "step: 7534, loss: 0.7889688014984131\n",
      "step: 7539, loss: 0.6140881776809692\n",
      "Checkpoint is saved\n",
      "step: 7544, loss: 1.0798524618148804\n",
      "step: 7549, loss: 0.8793470859527588\n",
      "step: 7554, loss: 0.8173525929450989\n",
      "step: 7559, loss: 1.0674793720245361\n",
      "Checkpoint is saved\n",
      "step: 7564, loss: 0.8275884389877319\n",
      "step: 7569, loss: 0.5373772382736206\n",
      "step: 7574, loss: 0.8813169598579407\n",
      "step: 7579, loss: 1.185774803161621\n",
      "Checkpoint is saved\n",
      "step: 7584, loss: 0.6854588985443115\n",
      "step: 7589, loss: 1.1318137645721436\n",
      "step: 7594, loss: 0.6935317516326904\n",
      "step: 7599, loss: 1.1524989604949951\n",
      "Checkpoint is saved\n",
      "step: 7604, loss: 1.1770310401916504\n",
      "step: 7609, loss: 1.1822004318237305\n",
      "step: 7614, loss: 1.0593218803405762\n",
      "step: 7619, loss: 1.5093624591827393\n",
      "Checkpoint is saved\n",
      "step: 7624, loss: 0.8787857890129089\n",
      "step: 7629, loss: 0.7850303649902344\n",
      "step: 7634, loss: 0.5974533557891846\n",
      "step: 7639, loss: 0.730031430721283\n",
      "Checkpoint is saved\n",
      "step: 7644, loss: 1.192789912223816\n",
      "step: 7649, loss: 1.2072426080703735\n",
      "step: 7654, loss: 0.8237532377243042\n",
      "step: 7659, loss: 1.0071392059326172\n",
      "Checkpoint is saved\n",
      "step: 7664, loss: 1.203786849975586\n",
      "step: 7669, loss: 1.515638828277588\n",
      "step: 7674, loss: 0.7745809555053711\n",
      "step: 7679, loss: 1.003128170967102\n",
      "Checkpoint is saved\n",
      "step: 7684, loss: 1.4281153678894043\n",
      "step: 7689, loss: 1.1077117919921875\n",
      "step: 7694, loss: 1.4540555477142334\n",
      "step: 7699, loss: 0.9926220178604126\n",
      "Checkpoint is saved\n",
      "step: 7704, loss: 1.4647023677825928\n",
      "step: 7709, loss: 0.847601056098938\n",
      "step: 7714, loss: 1.5357160568237305\n",
      "step: 7719, loss: 1.068514108657837\n",
      "Checkpoint is saved\n",
      "step: 7724, loss: 1.318577766418457\n",
      "step: 7729, loss: 0.6929718255996704\n",
      "step: 7734, loss: 1.0754766464233398\n",
      "step: 7739, loss: 1.2316007614135742\n",
      "Checkpoint is saved\n",
      "step: 7744, loss: 0.642094612121582\n",
      "step: 7749, loss: 1.2841944694519043\n",
      "step: 7754, loss: 0.9093947410583496\n",
      "step: 7759, loss: 1.5281014442443848\n",
      "Checkpoint is saved\n",
      "step: 7764, loss: 1.0617984533309937\n",
      "step: 7769, loss: 1.3452701568603516\n",
      "step: 7774, loss: 0.6883012652397156\n",
      "step: 7779, loss: 1.2259647846221924\n",
      "Checkpoint is saved\n",
      "step: 7784, loss: 0.7039033770561218\n",
      "step: 7789, loss: 1.2391031980514526\n",
      "step: 7794, loss: 0.8299747705459595\n",
      "step: 7799, loss: 1.2911083698272705\n",
      "Checkpoint is saved\n",
      "step: 7804, loss: 1.1970162391662598\n",
      "step: 7809, loss: 0.832582950592041\n",
      "step: 7814, loss: 0.9282053709030151\n",
      "step: 7819, loss: 1.2655119895935059\n",
      "Checkpoint is saved\n",
      "step: 7824, loss: 0.7654768228530884\n",
      "step: 7829, loss: 1.649554967880249\n",
      "step: 7834, loss: 0.731013298034668\n",
      "step: 7839, loss: 1.1954171657562256\n",
      "Checkpoint is saved\n",
      "step: 7844, loss: 0.992582380771637\n",
      "step: 7849, loss: 0.6978276968002319\n",
      "step: 7854, loss: 1.241374135017395\n",
      "step: 7859, loss: 0.7912154793739319\n",
      "Checkpoint is saved\n",
      "step: 7864, loss: 0.8316348791122437\n",
      "step: 7869, loss: 1.222909927368164\n",
      "step: 7874, loss: 1.1041936874389648\n",
      "step: 7879, loss: 1.1003001928329468\n",
      "Checkpoint is saved\n",
      "step: 7884, loss: 0.7549189329147339\n",
      "step: 7889, loss: 0.8577166795730591\n",
      "step: 7894, loss: 0.6708148717880249\n",
      "step: 7899, loss: 0.8458878397941589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 7904, loss: 0.6147723197937012\n",
      "step: 7909, loss: 1.175394892692566\n",
      "step: 7914, loss: 0.9935203194618225\n",
      "step: 7919, loss: 1.1117205619812012\n",
      "Checkpoint is saved\n",
      "step: 7924, loss: 0.9367967844009399\n",
      "step: 7929, loss: 1.1945490837097168\n",
      "step: 7934, loss: 1.1604137420654297\n",
      "step: 7939, loss: 0.5211679935455322\n",
      "Checkpoint is saved\n",
      "step: 7944, loss: 1.0398919582366943\n",
      "step: 7949, loss: 1.0889575481414795\n",
      "step: 7954, loss: 0.795534610748291\n",
      "step: 7959, loss: 1.0078699588775635\n",
      "Checkpoint is saved\n",
      "step: 7964, loss: 0.7315173149108887\n",
      "step: 7969, loss: 0.749164342880249\n",
      "step: 7974, loss: 0.7415153980255127\n",
      "step: 7979, loss: 0.4601001739501953\n",
      "Checkpoint is saved\n",
      "step: 7984, loss: 0.9252308011054993\n",
      "step: 7989, loss: 0.82505863904953\n",
      "step: 7994, loss: 1.68052077293396\n",
      "step: 7999, loss: 0.5873086452484131\n",
      "Checkpoint is saved\n",
      "step: 8004, loss: 1.0768752098083496\n",
      "step: 8009, loss: 0.867063045501709\n",
      "step: 8014, loss: 1.4756470918655396\n",
      "step: 8019, loss: 0.998810887336731\n",
      "Checkpoint is saved\n",
      "step: 8024, loss: 0.9269148111343384\n",
      "step: 8029, loss: 1.2318634986877441\n",
      "step: 8034, loss: 1.3033697605133057\n",
      "step: 8039, loss: 1.3207229375839233\n",
      "Checkpoint is saved\n",
      "step: 8044, loss: 1.237776756286621\n",
      "step: 8049, loss: 0.7344096302986145\n",
      "step: 8054, loss: 1.330277681350708\n",
      "step: 8059, loss: 0.9575095176696777\n",
      "Checkpoint is saved\n",
      "step: 8064, loss: 1.131396770477295\n",
      "step: 8069, loss: 0.8192851543426514\n",
      "step: 8074, loss: 0.6400154829025269\n",
      "step: 8079, loss: 1.7563718557357788\n",
      "Checkpoint is saved\n",
      "step: 8084, loss: 0.8509960174560547\n",
      "step: 8089, loss: 1.1975494623184204\n",
      "step: 8094, loss: 1.1194614171981812\n",
      "step: 8099, loss: 1.0869054794311523\n",
      "Checkpoint is saved\n",
      "step: 8104, loss: 1.0449461936950684\n",
      "step: 8109, loss: 0.7865535020828247\n",
      "step: 8114, loss: 0.763141393661499\n",
      "step: 8119, loss: 1.108161449432373\n",
      "Checkpoint is saved\n",
      "step: 8124, loss: 1.0441701412200928\n",
      "step: 8129, loss: 0.7642278075218201\n",
      "step: 8134, loss: 0.5722385048866272\n",
      "step: 8139, loss: 0.9878957271575928\n",
      "Checkpoint is saved\n",
      "step: 8144, loss: 1.120347499847412\n",
      "step: 8149, loss: 0.9761757850646973\n",
      "step: 8154, loss: 0.7502213716506958\n",
      "step: 8159, loss: 0.633147656917572\n",
      "Checkpoint is saved\n",
      "step: 8164, loss: 0.9078760147094727\n",
      "step: 8169, loss: 1.1982637643814087\n",
      "step: 8174, loss: 0.5675745606422424\n",
      "step: 8179, loss: 1.4271743297576904\n",
      "Checkpoint is saved\n",
      "step: 8184, loss: 0.8104705810546875\n",
      "step: 8189, loss: 1.1961727142333984\n",
      "step: 8194, loss: 1.1614058017730713\n",
      "step: 8199, loss: 1.3095126152038574\n",
      "Checkpoint is saved\n",
      "step: 8204, loss: 1.10591459274292\n",
      "step: 8209, loss: 0.8440345525741577\n",
      "step: 8214, loss: 0.9299674034118652\n",
      "step: 8219, loss: 1.0163506269454956\n",
      "Checkpoint is saved\n",
      "step: 8224, loss: 0.7874352931976318\n",
      "step: 8229, loss: 1.0773580074310303\n",
      "step: 8234, loss: 0.6869637966156006\n",
      "step: 8239, loss: 1.03984797000885\n",
      "Checkpoint is saved\n",
      "step: 8244, loss: 1.097350835800171\n",
      "step: 8249, loss: 0.555202841758728\n",
      "step: 8254, loss: 1.6245393753051758\n",
      "step: 8259, loss: 1.1679635047912598\n",
      "Checkpoint is saved\n",
      "step: 8264, loss: 1.220582127571106\n",
      "step: 8269, loss: 0.9982717633247375\n",
      "step: 8274, loss: 1.0868849754333496\n",
      "step: 8279, loss: 1.232722520828247\n",
      "Checkpoint is saved\n",
      "step: 8284, loss: 1.2632488012313843\n",
      "step: 8289, loss: 0.7516735792160034\n",
      "step: 8294, loss: 0.3239274024963379\n",
      "step: 8299, loss: 1.4296702146530151\n",
      "Checkpoint is saved\n",
      "step: 8304, loss: 1.6133989095687866\n",
      "step: 8309, loss: 1.1193029880523682\n",
      "step: 8314, loss: 1.4769296646118164\n",
      "step: 8319, loss: 0.839606761932373\n",
      "Checkpoint is saved\n",
      "step: 8324, loss: 0.69181227684021\n",
      "step: 8329, loss: 0.9984169006347656\n",
      "step: 8334, loss: 1.0215864181518555\n",
      "step: 8339, loss: 0.9043330550193787\n",
      "Checkpoint is saved\n",
      "step: 8344, loss: 1.0736188888549805\n",
      "step: 8349, loss: 0.7134885191917419\n",
      "step: 8354, loss: 1.1909631490707397\n",
      "step: 8359, loss: 1.1309945583343506\n",
      "Checkpoint is saved\n",
      "step: 8364, loss: 1.2150795459747314\n",
      "step: 8369, loss: 1.047024130821228\n",
      "step: 8374, loss: 1.1251499652862549\n",
      "step: 8379, loss: 1.3621041774749756\n",
      "Checkpoint is saved\n",
      "step: 8384, loss: 1.053236722946167\n",
      "step: 8389, loss: 0.7824807167053223\n",
      "step: 8394, loss: 0.6331587433815002\n",
      "step: 8399, loss: 0.874474287033081\n",
      "Checkpoint is saved\n",
      "step: 8404, loss: 1.0759685039520264\n",
      "step: 8409, loss: 0.8671000003814697\n",
      "step: 8414, loss: 0.6735765933990479\n",
      "step: 8419, loss: 0.7502273321151733\n",
      "Checkpoint is saved\n",
      "step: 8424, loss: 1.2781803607940674\n",
      "step: 8429, loss: 1.0523430109024048\n",
      "step: 8434, loss: 0.9476075172424316\n",
      "step: 8439, loss: 0.9851080179214478\n",
      "Checkpoint is saved\n",
      "step: 8444, loss: 0.676547646522522\n",
      "step: 8449, loss: 1.0643267631530762\n",
      "step: 8454, loss: 0.9416958093643188\n",
      "step: 8459, loss: 0.9693946838378906\n",
      "Checkpoint is saved\n",
      "step: 8464, loss: 0.7385889887809753\n",
      "step: 8469, loss: 0.9623105525970459\n",
      "step: 8474, loss: 0.7421597242355347\n",
      "step: 8479, loss: 0.9080226421356201\n",
      "Checkpoint is saved\n",
      "step: 8484, loss: 0.8976354002952576\n",
      "step: 8489, loss: 0.7961676716804504\n",
      "step: 8494, loss: 0.6812949180603027\n",
      "step: 8499, loss: 1.0129214525222778\n",
      "Checkpoint is saved\n",
      "step: 8504, loss: 0.9387475252151489\n",
      "step: 8509, loss: 1.202616810798645\n",
      "step: 8514, loss: 1.1549783945083618\n",
      "step: 8519, loss: 0.8023757934570312\n",
      "Checkpoint is saved\n",
      "step: 8524, loss: 1.051058053970337\n",
      "step: 8529, loss: 1.0889713764190674\n",
      "step: 8534, loss: 1.0113929510116577\n",
      "step: 8539, loss: 0.9672006964683533\n",
      "Checkpoint is saved\n",
      "step: 8544, loss: 0.9646612405776978\n",
      "step: 8549, loss: 0.8861158490180969\n",
      "step: 8554, loss: 1.5709458589553833\n",
      "step: 8559, loss: 1.2050342559814453\n",
      "Checkpoint is saved\n",
      "step: 8564, loss: 1.192310094833374\n",
      "step: 8569, loss: 1.0731080770492554\n",
      "step: 8574, loss: 0.9471476674079895\n",
      "step: 8579, loss: 0.6641959547996521\n",
      "Checkpoint is saved\n",
      "step: 8584, loss: 1.184201955795288\n",
      "step: 8589, loss: 1.0932637453079224\n",
      "step: 8594, loss: 1.0175330638885498\n",
      "step: 8599, loss: 0.8189448118209839\n",
      "Checkpoint is saved\n",
      "step: 8604, loss: 1.3303661346435547\n",
      "step: 8609, loss: 0.9011545181274414\n",
      "step: 8614, loss: 0.7515546679496765\n",
      "step: 8619, loss: 0.9135071039199829\n",
      "Checkpoint is saved\n",
      "step: 8624, loss: 0.7047058343887329\n",
      "step: 8629, loss: 0.8724156618118286\n",
      "step: 8634, loss: 1.0729608535766602\n",
      "step: 8639, loss: 1.0725922584533691\n",
      "Checkpoint is saved\n",
      "step: 8644, loss: 1.027015209197998\n",
      "step: 8649, loss: 1.1389434337615967\n",
      "step: 8654, loss: 0.6387052536010742\n",
      "step: 8659, loss: 1.0950202941894531\n",
      "Checkpoint is saved\n",
      "step: 8664, loss: 1.0563617944717407\n",
      "step: 8669, loss: 0.6884364485740662\n",
      "step: 8674, loss: 0.7566275596618652\n",
      "step: 8679, loss: 1.255128026008606\n",
      "Checkpoint is saved\n",
      "step: 8684, loss: 0.9005721807479858\n",
      "step: 8689, loss: 0.7731639742851257\n",
      "step: 8694, loss: 0.8671356439590454\n",
      "step: 8699, loss: 0.5921779870986938\n",
      "Checkpoint is saved\n",
      "step: 8704, loss: 0.7153000831604004\n",
      "step: 8709, loss: 0.7643787860870361\n",
      "step: 8714, loss: 0.9209669828414917\n",
      "step: 8719, loss: 0.48871710896492004\n",
      "Checkpoint is saved\n",
      "step: 8724, loss: 1.5869667530059814\n",
      "step: 8729, loss: 1.0536880493164062\n",
      "step: 8734, loss: 0.958236575126648\n",
      "step: 8739, loss: 0.6807843446731567\n",
      "Checkpoint is saved\n",
      "step: 8744, loss: 0.7196611166000366\n",
      "step: 8749, loss: 0.9469788670539856\n",
      "step: 8754, loss: 0.8793015480041504\n",
      "step: 8759, loss: 1.0120463371276855\n",
      "Checkpoint is saved\n",
      "step: 8764, loss: 0.7606364488601685\n",
      "step: 8769, loss: 0.6435142755508423\n",
      "step: 8774, loss: 0.8691625595092773\n",
      "step: 8779, loss: 0.8215827941894531\n",
      "Checkpoint is saved\n",
      "step: 8784, loss: 0.9011132717132568\n",
      "step: 8789, loss: 1.2958415746688843\n",
      "step: 8794, loss: 0.9610435962677002\n",
      "step: 8799, loss: 1.1451529264450073\n",
      "Checkpoint is saved\n",
      "step: 8804, loss: 1.4430317878723145\n",
      "step: 8809, loss: 0.6971415281295776\n",
      "step: 8814, loss: 0.7423899173736572\n",
      "step: 8819, loss: 0.6608681678771973\n",
      "Checkpoint is saved\n",
      "step: 8824, loss: 0.961336612701416\n",
      "step: 8829, loss: 1.229567527770996\n",
      "step: 8834, loss: 0.8306955099105835\n",
      "step: 8839, loss: 1.0806411504745483\n",
      "Checkpoint is saved\n",
      "step: 8844, loss: 1.007361888885498\n",
      "step: 8849, loss: 1.305374264717102\n",
      "step: 8854, loss: 0.8159624338150024\n",
      "step: 8859, loss: 0.7515788078308105\n",
      "Checkpoint is saved\n",
      "step: 8864, loss: 0.7607624530792236\n",
      "step: 8869, loss: 1.208156704902649\n",
      "step: 8874, loss: 0.6348652243614197\n",
      "step: 8879, loss: 0.8728101253509521\n",
      "Checkpoint is saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8884, loss: 1.0123462677001953\n",
      "step: 8889, loss: 0.9650840163230896\n",
      "step: 8894, loss: 1.06863272190094\n",
      "step: 8899, loss: 1.0493966341018677\n",
      "Checkpoint is saved\n",
      "step: 8904, loss: 0.8290486335754395\n",
      "step: 8909, loss: 0.6072676181793213\n",
      "step: 8914, loss: 0.8332551717758179\n",
      "step: 8919, loss: 1.103332757949829\n",
      "Checkpoint is saved\n",
      "step: 8924, loss: 0.9653955101966858\n",
      "step: 8929, loss: 1.1679010391235352\n",
      "step: 8934, loss: 1.4351719617843628\n",
      "step: 8939, loss: 0.5355228781700134\n",
      "Checkpoint is saved\n",
      "step: 8944, loss: 1.5139423608779907\n",
      "step: 8949, loss: 0.5000674724578857\n",
      "step: 8954, loss: 0.8774778246879578\n",
      "step: 8959, loss: 0.784223735332489\n",
      "Checkpoint is saved\n",
      "step: 8964, loss: 0.8920550346374512\n",
      "step: 8969, loss: 0.9840440154075623\n",
      "step: 8974, loss: 1.1405483484268188\n",
      "step: 8979, loss: 0.9723179340362549\n",
      "Checkpoint is saved\n",
      "step: 8984, loss: 1.050437569618225\n",
      "step: 8989, loss: 0.832327127456665\n",
      "step: 8994, loss: 0.790715754032135\n",
      "step: 8999, loss: 0.7447190880775452\n",
      "Checkpoint is saved\n",
      "step: 9004, loss: 0.6235649585723877\n",
      "step: 9009, loss: 1.1122462749481201\n",
      "step: 9014, loss: 1.0289287567138672\n",
      "step: 9019, loss: 1.0895012617111206\n",
      "Checkpoint is saved\n",
      "step: 9024, loss: 1.1467057466506958\n",
      "step: 9029, loss: 0.7997266054153442\n",
      "step: 9034, loss: 1.069392442703247\n",
      "step: 9039, loss: 0.5981112718582153\n",
      "Checkpoint is saved\n",
      "step: 9044, loss: 1.0759634971618652\n",
      "step: 9049, loss: 0.9797316789627075\n",
      "step: 9054, loss: 0.6313604116439819\n",
      "step: 9059, loss: 0.9057972431182861\n",
      "Checkpoint is saved\n",
      "step: 9064, loss: 1.1656880378723145\n",
      "step: 9069, loss: 0.8454060554504395\n",
      "step: 9074, loss: 0.6278268694877625\n",
      "step: 9079, loss: 1.0152486562728882\n",
      "Checkpoint is saved\n",
      "step: 9084, loss: 1.0722746849060059\n",
      "step: 9089, loss: 1.1790368556976318\n",
      "step: 9094, loss: 1.1294617652893066\n",
      "step: 9099, loss: 0.8033347129821777\n",
      "Checkpoint is saved\n",
      "step: 9104, loss: 0.8897068500518799\n",
      "step: 9109, loss: 0.8263750076293945\n",
      "step: 9114, loss: 0.9828411936759949\n",
      "step: 9119, loss: 1.205082893371582\n",
      "Checkpoint is saved\n",
      "step: 9124, loss: 0.9012740850448608\n",
      "step: 9129, loss: 0.896072506904602\n",
      "step: 9134, loss: 0.9931797385215759\n",
      "step: 9139, loss: 1.0592975616455078\n",
      "Checkpoint is saved\n",
      "step: 9144, loss: 0.6489893198013306\n",
      "step: 9149, loss: 1.2766484022140503\n",
      "step: 9154, loss: 1.2890254259109497\n",
      "step: 9159, loss: 0.8254357576370239\n",
      "Checkpoint is saved\n",
      "step: 9164, loss: 1.4229685068130493\n",
      "step: 9169, loss: 0.8554033041000366\n",
      "step: 9174, loss: 0.8174151182174683\n",
      "step: 9179, loss: 1.332845687866211\n",
      "Checkpoint is saved\n",
      "step: 9184, loss: 0.7613406777381897\n",
      "step: 9189, loss: 0.7708081603050232\n",
      "step: 9194, loss: 0.6871809363365173\n",
      "step: 9199, loss: 0.577150285243988\n",
      "Checkpoint is saved\n",
      "step: 9204, loss: 0.7631313800811768\n",
      "step: 9209, loss: 1.158294916152954\n",
      "step: 9214, loss: 1.1791146993637085\n",
      "step: 9219, loss: 1.2223905324935913\n",
      "Checkpoint is saved\n",
      "step: 9224, loss: 0.970137357711792\n",
      "step: 9229, loss: 1.0897917747497559\n",
      "step: 9234, loss: 1.282198190689087\n",
      "step: 9239, loss: 0.9613690972328186\n",
      "Checkpoint is saved\n",
      "step: 9244, loss: 1.0024914741516113\n",
      "step: 9249, loss: 1.2184929847717285\n",
      "step: 9254, loss: 0.5634433031082153\n",
      "step: 9259, loss: 0.7416139841079712\n",
      "Checkpoint is saved\n",
      "step: 9264, loss: 0.7537922859191895\n",
      "step: 9269, loss: 1.3201959133148193\n",
      "step: 9274, loss: 1.0020442008972168\n",
      "step: 9279, loss: 1.463245153427124\n",
      "Checkpoint is saved\n",
      "step: 9284, loss: 0.9073677062988281\n",
      "step: 9289, loss: 1.0098423957824707\n",
      "step: 9294, loss: 0.8107447028160095\n",
      "step: 9299, loss: 0.8354130983352661\n",
      "Checkpoint is saved\n",
      "step: 9304, loss: 1.1395161151885986\n",
      "step: 9309, loss: 1.16910719871521\n",
      "step: 9314, loss: 1.0554307699203491\n",
      "step: 9319, loss: 0.8550510406494141\n",
      "Checkpoint is saved\n",
      "step: 9324, loss: 0.7132872343063354\n",
      "step: 9329, loss: 0.992585301399231\n",
      "step: 9334, loss: 0.9452953934669495\n",
      "step: 9339, loss: 0.8656364679336548\n",
      "Checkpoint is saved\n",
      "step: 9344, loss: 0.9332981109619141\n",
      "step: 9349, loss: 1.0799777507781982\n",
      "step: 9354, loss: 1.0846972465515137\n",
      "step: 9359, loss: 0.8720228672027588\n",
      "Checkpoint is saved\n",
      "step: 9364, loss: 1.0804756879806519\n",
      "step: 9369, loss: 1.2017335891723633\n",
      "step: 9374, loss: 0.8784767389297485\n",
      "step: 9379, loss: 0.6122637391090393\n",
      "Checkpoint is saved\n",
      "step: 9384, loss: 1.0830652713775635\n",
      "step: 9389, loss: 0.5269926190376282\n",
      "step: 9394, loss: 1.439956784248352\n",
      "step: 9399, loss: 0.6126865148544312\n",
      "Checkpoint is saved\n",
      "step: 9404, loss: 1.2682584524154663\n",
      "step: 9409, loss: 0.8806147575378418\n",
      "step: 9414, loss: 1.0375659465789795\n",
      "step: 9419, loss: 0.930002748966217\n",
      "Checkpoint is saved\n",
      "step: 9424, loss: 0.49637076258659363\n",
      "step: 9429, loss: 0.4647122621536255\n",
      "step: 9434, loss: 0.5879805088043213\n",
      "step: 9439, loss: 1.2635529041290283\n",
      "Checkpoint is saved\n",
      "step: 9444, loss: 1.0580673217773438\n",
      "step: 9449, loss: 1.4877861738204956\n",
      "step: 9454, loss: 0.8724658489227295\n",
      "step: 9459, loss: 1.250254511833191\n",
      "Checkpoint is saved\n",
      "step: 9464, loss: 1.2535967826843262\n",
      "step: 9469, loss: 1.0977075099945068\n",
      "step: 9474, loss: 0.5635169148445129\n",
      "step: 9479, loss: 0.6078557968139648\n",
      "Checkpoint is saved\n",
      "step: 9484, loss: 0.9098231196403503\n",
      "step: 9489, loss: 0.9695447683334351\n",
      "step: 9494, loss: 0.7824808359146118\n",
      "step: 9499, loss: 1.0014657974243164\n",
      "Checkpoint is saved\n",
      "step: 9504, loss: 0.6626183390617371\n",
      "step: 9509, loss: 1.1118824481964111\n",
      "step: 9514, loss: 0.9284952282905579\n",
      "step: 9519, loss: 1.1287105083465576\n",
      "Checkpoint is saved\n",
      "step: 9524, loss: 0.9979942440986633\n",
      "step: 9529, loss: 0.8558695912361145\n",
      "step: 9534, loss: 1.0140225887298584\n",
      "step: 9539, loss: 0.8864495158195496\n",
      "Checkpoint is saved\n",
      "step: 9544, loss: 1.0575196743011475\n",
      "step: 9549, loss: 1.0230003595352173\n",
      "step: 9554, loss: 0.9683589935302734\n",
      "step: 9559, loss: 0.6805136203765869\n",
      "Checkpoint is saved\n",
      "step: 9564, loss: 1.1288429498672485\n",
      "step: 9569, loss: 0.7691376209259033\n",
      "step: 9574, loss: 1.3410676717758179\n",
      "step: 9579, loss: 1.3627922534942627\n",
      "Checkpoint is saved\n",
      "step: 9584, loss: 0.7207783460617065\n",
      "step: 9589, loss: 0.9642407894134521\n",
      "step: 9594, loss: 0.9405428767204285\n",
      "step: 9599, loss: 1.3981964588165283\n",
      "Checkpoint is saved\n",
      "step: 9604, loss: 0.9722784757614136\n",
      "step: 9609, loss: 0.844614565372467\n",
      "step: 9614, loss: 0.8272098898887634\n",
      "step: 9619, loss: 1.2589973211288452\n",
      "Checkpoint is saved\n",
      "step: 9624, loss: 0.7462730407714844\n",
      "step: 9629, loss: 0.9392489194869995\n",
      "step: 9634, loss: 0.7357380986213684\n",
      "step: 9639, loss: 1.0527104139328003\n",
      "Checkpoint is saved\n",
      "step: 9644, loss: 0.9565130472183228\n",
      "step: 9649, loss: 0.8935506939888\n",
      "step: 9654, loss: 1.1066759824752808\n",
      "step: 9659, loss: 0.7086005210876465\n",
      "Checkpoint is saved\n",
      "step: 9664, loss: 1.2498254776000977\n",
      "step: 9669, loss: 1.0764583349227905\n",
      "step: 9674, loss: 0.8313884139060974\n",
      "step: 9679, loss: 0.9212415218353271\n",
      "Checkpoint is saved\n",
      "step: 9684, loss: 0.5371957421302795\n",
      "step: 9689, loss: 0.8835597038269043\n",
      "step: 9694, loss: 1.050796627998352\n",
      "step: 9699, loss: 0.8553721904754639\n",
      "Checkpoint is saved\n",
      "step: 9704, loss: 1.1229064464569092\n",
      "step: 9709, loss: 0.7402524352073669\n",
      "step: 9714, loss: 0.8171200752258301\n",
      "step: 9719, loss: 1.3397276401519775\n",
      "Checkpoint is saved\n",
      "step: 9724, loss: 0.8037448525428772\n",
      "step: 9729, loss: 0.6729176044464111\n",
      "step: 9734, loss: 0.900088369846344\n",
      "step: 9739, loss: 1.0436804294586182\n",
      "Checkpoint is saved\n",
      "step: 9744, loss: 1.0207842588424683\n",
      "step: 9749, loss: 0.9084842205047607\n",
      "step: 9754, loss: 1.4063785076141357\n",
      "step: 9759, loss: 1.0326662063598633\n",
      "Checkpoint is saved\n",
      "step: 9764, loss: 1.692321538925171\n",
      "step: 9769, loss: 0.7747540473937988\n",
      "step: 9774, loss: 0.9947566986083984\n",
      "step: 9779, loss: 0.6744402647018433\n",
      "Checkpoint is saved\n",
      "step: 9784, loss: 0.8482006192207336\n",
      "step: 9789, loss: 0.959572434425354\n",
      "step: 9794, loss: 1.2234762907028198\n",
      "step: 9799, loss: 1.0652798414230347\n",
      "Checkpoint is saved\n",
      "step: 9804, loss: 0.9644715785980225\n",
      "step: 9809, loss: 0.8489137887954712\n",
      "step: 9814, loss: 0.8664311766624451\n",
      "step: 9819, loss: 1.081421136856079\n",
      "Checkpoint is saved\n",
      "step: 9824, loss: 0.8459823131561279\n",
      "step: 9829, loss: 1.242558240890503\n",
      "step: 9834, loss: 1.055816888809204\n",
      "step: 9839, loss: 0.8838717937469482\n",
      "Checkpoint is saved\n",
      "step: 9844, loss: 1.1431630849838257\n",
      "step: 9849, loss: 1.016393780708313\n",
      "step: 9854, loss: 1.205585241317749\n",
      "step: 9859, loss: 1.1753474473953247\n",
      "Checkpoint is saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9864, loss: 1.029496431350708\n",
      "step: 9869, loss: 0.9324119091033936\n",
      "step: 9874, loss: 0.9224984645843506\n",
      "step: 9879, loss: 1.1108629703521729\n",
      "Checkpoint is saved\n",
      "step: 9884, loss: 0.9730492830276489\n",
      "step: 9889, loss: 0.8683243989944458\n",
      "step: 9894, loss: 1.2768023014068604\n",
      "step: 9899, loss: 0.7172696590423584\n",
      "Checkpoint is saved\n",
      "step: 9904, loss: 0.4464309811592102\n",
      "step: 9909, loss: 1.1212024688720703\n",
      "step: 9914, loss: 1.042930245399475\n",
      "step: 9919, loss: 0.9968346953392029\n",
      "Checkpoint is saved\n",
      "step: 9924, loss: 0.6107493042945862\n",
      "step: 9929, loss: 0.976186215877533\n",
      "step: 9934, loss: 0.7159473896026611\n",
      "step: 9939, loss: 0.9058041572570801\n",
      "Checkpoint is saved\n",
      "step: 9944, loss: 0.8771075010299683\n",
      "step: 9949, loss: 1.3517614603042603\n",
      "step: 9954, loss: 1.2066824436187744\n",
      "step: 9959, loss: 2.0511012077331543\n",
      "Checkpoint is saved\n",
      "step: 9964, loss: 0.7736345529556274\n",
      "step: 9969, loss: 0.465017169713974\n",
      "step: 9974, loss: 0.8831412196159363\n",
      "step: 9979, loss: 0.5410085916519165\n",
      "Checkpoint is saved\n",
      "step: 9984, loss: 0.5628269910812378\n",
      "step: 9989, loss: 1.0130109786987305\n",
      "step: 9994, loss: 0.7749713659286499\n",
      "step: 9999, loss: 0.4319687485694885\n",
      "Checkpoint is saved\n",
      "step: 10004, loss: 0.6110315322875977\n",
      "step: 10009, loss: 0.9207878112792969\n",
      "step: 10014, loss: 0.5455199480056763\n",
      "step: 10019, loss: 0.7081071138381958\n",
      "Checkpoint is saved\n",
      "step: 10024, loss: 0.815727174282074\n",
      "step: 10029, loss: 0.7879360318183899\n",
      "step: 10034, loss: 1.1781302690505981\n",
      "step: 10039, loss: 0.7677085995674133\n",
      "Checkpoint is saved\n",
      "step: 10044, loss: 1.1641004085540771\n",
      "step: 10049, loss: 0.6874434947967529\n",
      "step: 10054, loss: 1.1336219310760498\n",
      "step: 10059, loss: 0.5854491591453552\n",
      "Checkpoint is saved\n",
      "step: 10064, loss: 1.3656020164489746\n",
      "step: 10069, loss: 1.389687180519104\n",
      "step: 10074, loss: 1.3155485391616821\n",
      "step: 10079, loss: 0.8663808107376099\n",
      "Checkpoint is saved\n",
      "step: 10084, loss: 0.9243658781051636\n",
      "step: 10089, loss: 0.4784134030342102\n",
      "step: 10094, loss: 0.7983471155166626\n",
      "step: 10099, loss: 0.8856682777404785\n",
      "Checkpoint is saved\n",
      "step: 10104, loss: 0.833060622215271\n",
      "step: 10109, loss: 0.8097918033599854\n",
      "step: 10114, loss: 0.9491029977798462\n",
      "step: 10119, loss: 1.2187201976776123\n",
      "Checkpoint is saved\n",
      "step: 10124, loss: 0.9025867581367493\n",
      "step: 10129, loss: 0.9264249801635742\n",
      "step: 10134, loss: 0.9200803637504578\n",
      "step: 10139, loss: 0.5595144033432007\n",
      "Checkpoint is saved\n",
      "step: 10144, loss: 1.2696940898895264\n",
      "step: 10149, loss: 0.4898355305194855\n",
      "step: 10154, loss: 0.9445496201515198\n",
      "step: 10159, loss: 0.7837530374526978\n",
      "Checkpoint is saved\n",
      "step: 10164, loss: 1.229475736618042\n",
      "step: 10169, loss: 1.3414312601089478\n",
      "step: 10174, loss: 0.9154455661773682\n",
      "step: 10179, loss: 0.8159899115562439\n",
      "Checkpoint is saved\n",
      "step: 10184, loss: 0.6918160915374756\n",
      "step: 10189, loss: 0.5605267286300659\n",
      "step: 10194, loss: 0.7474346160888672\n",
      "step: 10199, loss: 0.7450368404388428\n",
      "Checkpoint is saved\n",
      "step: 10204, loss: 0.8865933418273926\n",
      "step: 10209, loss: 0.5370762944221497\n",
      "step: 10214, loss: 0.8611021041870117\n",
      "step: 10219, loss: 0.9082662463188171\n",
      "Checkpoint is saved\n",
      "step: 10224, loss: 0.995373010635376\n",
      "step: 10229, loss: 0.7833912372589111\n",
      "step: 10234, loss: 0.986066460609436\n",
      "step: 10239, loss: 1.17097806930542\n",
      "Checkpoint is saved\n",
      "step: 10244, loss: 0.9261980056762695\n",
      "step: 10249, loss: 1.021742820739746\n",
      "step: 10254, loss: 0.7966321706771851\n",
      "step: 10259, loss: 1.0950583219528198\n",
      "Checkpoint is saved\n",
      "step: 10264, loss: 1.231332778930664\n",
      "step: 10269, loss: 0.9397358894348145\n",
      "step: 10274, loss: 1.3248358964920044\n",
      "step: 10279, loss: 0.8011573553085327\n",
      "Checkpoint is saved\n",
      "step: 10284, loss: 1.1993695497512817\n",
      "step: 10289, loss: 1.4033538103103638\n",
      "step: 10294, loss: 1.2616536617279053\n",
      "step: 10299, loss: 0.8353586196899414\n",
      "Checkpoint is saved\n",
      "step: 10304, loss: 1.413456678390503\n",
      "step: 10309, loss: 0.8681704998016357\n",
      "step: 10314, loss: 0.9855851531028748\n",
      "step: 10319, loss: 0.9282193183898926\n",
      "Checkpoint is saved\n",
      "step: 10324, loss: 0.5766326189041138\n",
      "step: 10329, loss: 0.6446558237075806\n",
      "step: 10334, loss: 1.1215606927871704\n",
      "step: 10339, loss: 0.9936413168907166\n",
      "Checkpoint is saved\n",
      "step: 10344, loss: 0.8771263360977173\n",
      "step: 10349, loss: 1.5861876010894775\n",
      "step: 10354, loss: 0.5003790259361267\n",
      "step: 10359, loss: 0.6355783343315125\n",
      "Checkpoint is saved\n",
      "step: 10364, loss: 0.8361562490463257\n",
      "step: 10369, loss: 1.0004204511642456\n",
      "step: 10374, loss: 1.1153768301010132\n",
      "step: 10379, loss: 1.0962224006652832\n",
      "Checkpoint is saved\n",
      "step: 10384, loss: 0.883191704750061\n",
      "step: 10389, loss: 1.2983332872390747\n",
      "step: 10394, loss: 0.8557201623916626\n",
      "step: 10399, loss: 0.568869948387146\n",
      "Checkpoint is saved\n",
      "step: 10404, loss: 0.6492745876312256\n",
      "step: 10409, loss: 0.9421871304512024\n",
      "step: 10414, loss: 1.000985026359558\n",
      "step: 10419, loss: 1.2498033046722412\n",
      "Checkpoint is saved\n",
      "step: 10424, loss: 0.7549921274185181\n",
      "step: 10429, loss: 1.116632103919983\n",
      "step: 10434, loss: 0.5231223106384277\n",
      "step: 10439, loss: 0.9084972739219666\n",
      "Checkpoint is saved\n",
      "step: 10444, loss: 1.078879952430725\n",
      "step: 10449, loss: 1.4751300811767578\n",
      "step: 10454, loss: 1.1589313745498657\n",
      "step: 10459, loss: 0.9072341918945312\n",
      "Checkpoint is saved\n",
      "step: 10464, loss: 0.8460637331008911\n",
      "step: 10469, loss: 1.117149829864502\n",
      "step: 10474, loss: 0.7937010526657104\n",
      "step: 10479, loss: 1.0901061296463013\n",
      "Checkpoint is saved\n",
      "step: 10484, loss: 0.48222100734710693\n",
      "step: 10489, loss: 0.8084783554077148\n",
      "step: 10494, loss: 0.6533043384552002\n",
      "step: 10499, loss: 0.9263628721237183\n",
      "Checkpoint is saved\n",
      "step: 10504, loss: 0.6633684635162354\n",
      "step: 10509, loss: 1.013828158378601\n",
      "step: 10514, loss: 0.9282443523406982\n",
      "step: 10519, loss: 0.5775333642959595\n",
      "Checkpoint is saved\n",
      "step: 10524, loss: 0.5006644129753113\n",
      "step: 10529, loss: 0.7857163548469543\n",
      "step: 10534, loss: 1.1057398319244385\n",
      "step: 10539, loss: 1.0450774431228638\n",
      "Checkpoint is saved\n",
      "step: 10544, loss: 1.081197738647461\n",
      "step: 10549, loss: 0.7715328931808472\n",
      "step: 10554, loss: 0.967739462852478\n",
      "step: 10559, loss: 1.1302560567855835\n",
      "Checkpoint is saved\n",
      "step: 10564, loss: 1.0261857509613037\n",
      "step: 10569, loss: 0.6840777397155762\n",
      "step: 10574, loss: 0.73500657081604\n",
      "step: 10579, loss: 0.8304942846298218\n",
      "Checkpoint is saved\n",
      "step: 10584, loss: 0.8920600414276123\n",
      "step: 10589, loss: 0.6753973960876465\n",
      "step: 10594, loss: 1.024896264076233\n",
      "step: 10599, loss: 0.7513702511787415\n",
      "Checkpoint is saved\n",
      "step: 10604, loss: 1.2250301837921143\n",
      "step: 10609, loss: 1.0823458433151245\n",
      "step: 10614, loss: 0.6439564228057861\n",
      "step: 10619, loss: 1.031285285949707\n",
      "Checkpoint is saved\n",
      "step: 10624, loss: 0.738978385925293\n",
      "step: 10629, loss: 0.7134002447128296\n",
      "step: 10634, loss: 1.0221748352050781\n",
      "step: 10639, loss: 1.0595476627349854\n",
      "Checkpoint is saved\n",
      "step: 10644, loss: 0.851392388343811\n",
      "step: 10649, loss: 0.8369699716567993\n",
      "step: 10654, loss: 0.9047746062278748\n",
      "step: 10659, loss: 0.7233851552009583\n",
      "Checkpoint is saved\n",
      "step: 10664, loss: 1.1195471286773682\n",
      "step: 10669, loss: 0.45827817916870117\n",
      "step: 10674, loss: 0.7553527355194092\n",
      "step: 10679, loss: 0.7907112836837769\n",
      "Checkpoint is saved\n",
      "step: 10684, loss: 1.054494857788086\n",
      "step: 10689, loss: 1.1695716381072998\n",
      "step: 10694, loss: 0.6666660308837891\n",
      "step: 10699, loss: 1.3495041131973267\n",
      "Checkpoint is saved\n",
      "step: 10704, loss: 1.084864854812622\n",
      "step: 10709, loss: 0.5718023777008057\n",
      "step: 10714, loss: 1.2853055000305176\n",
      "step: 10719, loss: 0.9378907680511475\n",
      "Checkpoint is saved\n",
      "step: 10724, loss: 0.775367259979248\n",
      "step: 10729, loss: 1.0117826461791992\n",
      "step: 10734, loss: 0.6538057327270508\n",
      "step: 10739, loss: 1.0109753608703613\n",
      "Checkpoint is saved\n",
      "step: 10744, loss: 0.8560703992843628\n",
      "step: 10749, loss: 0.6775449514389038\n",
      "step: 10754, loss: 0.8356595635414124\n",
      "step: 10759, loss: 1.0955878496170044\n",
      "Checkpoint is saved\n",
      "step: 10764, loss: 1.0475963354110718\n",
      "step: 10769, loss: 0.7289321422576904\n",
      "step: 10774, loss: 0.7985838651657104\n",
      "step: 10779, loss: 0.5476634502410889\n",
      "Checkpoint is saved\n",
      "step: 10784, loss: 1.1048675775527954\n",
      "step: 10789, loss: 0.7727645635604858\n",
      "step: 10794, loss: 1.0229511260986328\n",
      "step: 10799, loss: 1.128288984298706\n",
      "Checkpoint is saved\n",
      "step: 10804, loss: 1.4561034440994263\n",
      "step: 10809, loss: 1.108410120010376\n",
      "step: 10814, loss: 1.0723892450332642\n",
      "step: 10819, loss: 0.6639871597290039\n",
      "Checkpoint is saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10824, loss: 0.8719905614852905\n",
      "step: 10829, loss: 0.8883224725723267\n",
      "step: 10834, loss: 0.8379189968109131\n",
      "step: 10839, loss: 0.7399140000343323\n",
      "Checkpoint is saved\n",
      "step: 10844, loss: 1.0417792797088623\n",
      "step: 10849, loss: 0.7928673028945923\n",
      "step: 10854, loss: 1.2563025951385498\n",
      "step: 10859, loss: 0.8395694494247437\n",
      "Checkpoint is saved\n",
      "step: 10864, loss: 0.9596725702285767\n",
      "step: 10869, loss: 0.8137016296386719\n",
      "step: 10874, loss: 1.1050007343292236\n",
      "step: 10879, loss: 1.3012685775756836\n",
      "Checkpoint is saved\n",
      "step: 10884, loss: 0.9190876483917236\n",
      "step: 10889, loss: 0.8168199062347412\n",
      "step: 10894, loss: 0.8748268485069275\n",
      "step: 10899, loss: 1.0156028270721436\n",
      "Checkpoint is saved\n",
      "step: 10904, loss: 0.8912628293037415\n",
      "step: 10909, loss: 1.0757572650909424\n",
      "step: 10914, loss: 0.9134460687637329\n",
      "step: 10919, loss: 0.7481503486633301\n",
      "Checkpoint is saved\n",
      "step: 10924, loss: 0.7481271028518677\n",
      "step: 10929, loss: 0.9143467545509338\n",
      "step: 10934, loss: 0.7540530562400818\n",
      "step: 10939, loss: 0.9783885478973389\n",
      "Checkpoint is saved\n",
      "step: 10944, loss: 0.6204593181610107\n",
      "step: 10949, loss: 0.6048279404640198\n",
      "step: 10954, loss: 0.9769756197929382\n",
      "step: 10959, loss: 1.2676646709442139\n",
      "Checkpoint is saved\n",
      "step: 10964, loss: 0.6063743829727173\n",
      "step: 10969, loss: 1.0585696697235107\n",
      "step: 10974, loss: 0.6597995162010193\n",
      "step: 10979, loss: 0.5576386451721191\n",
      "Checkpoint is saved\n",
      "step: 10984, loss: 1.3036174774169922\n",
      "step: 10989, loss: 0.6736108660697937\n",
      "step: 10994, loss: 0.9459046125411987\n",
      "step: 10999, loss: 0.7922256588935852\n",
      "Checkpoint is saved\n",
      "step: 11004, loss: 0.7167137861251831\n",
      "step: 11009, loss: 1.5036134719848633\n",
      "step: 11014, loss: 0.9616501331329346\n",
      "step: 11019, loss: 0.872128427028656\n",
      "Checkpoint is saved\n",
      "step: 11024, loss: 1.3245501518249512\n",
      "step: 11029, loss: 0.9380334615707397\n",
      "step: 11034, loss: 0.8028997778892517\n",
      "step: 11039, loss: 0.4756844639778137\n",
      "Checkpoint is saved\n",
      "step: 11044, loss: 0.8515905737876892\n",
      "step: 11049, loss: 1.2307121753692627\n",
      "step: 11054, loss: 0.9441935420036316\n",
      "step: 11059, loss: 0.8614596128463745\n",
      "Checkpoint is saved\n",
      "step: 11064, loss: 1.0639833211898804\n",
      "step: 11069, loss: 1.0183360576629639\n",
      "step: 11074, loss: 0.5672432780265808\n",
      "step: 11079, loss: 1.16209077835083\n",
      "Checkpoint is saved\n",
      "step: 11084, loss: 0.8537710309028625\n",
      "step: 11089, loss: 0.6463003754615784\n",
      "step: 11094, loss: 0.827613890171051\n",
      "step: 11099, loss: 1.1562342643737793\n",
      "Checkpoint is saved\n",
      "step: 11104, loss: 0.7537276744842529\n",
      "step: 11109, loss: 1.0508333444595337\n",
      "step: 11114, loss: 0.475485622882843\n",
      "step: 11119, loss: 0.7695425152778625\n",
      "Checkpoint is saved\n",
      "step: 11124, loss: 0.674963116645813\n",
      "step: 11129, loss: 1.0991215705871582\n",
      "step: 11134, loss: 0.6953320503234863\n",
      "step: 11139, loss: 0.6214082837104797\n",
      "Checkpoint is saved\n",
      "step: 11144, loss: 0.6707543134689331\n",
      "step: 11149, loss: 0.614890456199646\n",
      "step: 11154, loss: 0.649861216545105\n",
      "step: 11159, loss: 1.204967975616455\n",
      "Checkpoint is saved\n",
      "step: 11164, loss: 1.201451301574707\n",
      "step: 11169, loss: 0.9611383676528931\n",
      "step: 11174, loss: 1.2066080570220947\n",
      "step: 11179, loss: 1.2558233737945557\n",
      "Checkpoint is saved\n",
      "step: 11184, loss: 1.169438362121582\n",
      "step: 11189, loss: 1.0412287712097168\n",
      "step: 11194, loss: 0.8795420527458191\n",
      "step: 11199, loss: 1.1665284633636475\n",
      "Checkpoint is saved\n",
      "step: 11204, loss: 1.106938123703003\n",
      "step: 11209, loss: 0.8929651975631714\n",
      "step: 11214, loss: 0.7733076810836792\n",
      "step: 11219, loss: 1.02720308303833\n",
      "Checkpoint is saved\n",
      "step: 11224, loss: 1.2199995517730713\n",
      "step: 11229, loss: 1.0271662473678589\n",
      "step: 11234, loss: 0.7253919839859009\n",
      "step: 11239, loss: 0.7248174548149109\n",
      "Checkpoint is saved\n",
      "step: 11244, loss: 0.5033402442932129\n",
      "step: 11249, loss: 1.1064057350158691\n",
      "step: 11254, loss: 1.010296106338501\n",
      "step: 11259, loss: 0.8618536591529846\n",
      "Checkpoint is saved\n",
      "step: 11264, loss: 0.9013246297836304\n",
      "step: 11269, loss: 0.9857460260391235\n",
      "step: 11274, loss: 0.697380542755127\n",
      "step: 11279, loss: 0.6066814661026001\n",
      "Checkpoint is saved\n",
      "step: 11284, loss: 0.9351405501365662\n",
      "step: 11289, loss: 1.0487682819366455\n",
      "step: 11294, loss: 0.9356831312179565\n",
      "step: 11299, loss: 0.768996000289917\n",
      "Checkpoint is saved\n",
      "step: 11304, loss: 0.5099912285804749\n",
      "step: 11309, loss: 1.180838704109192\n",
      "step: 11314, loss: 0.9384044408798218\n",
      "step: 11319, loss: 1.3315354585647583\n",
      "Checkpoint is saved\n",
      "step: 11324, loss: 1.2528812885284424\n",
      "step: 11329, loss: 0.949925422668457\n",
      "step: 11334, loss: 0.8134593963623047\n",
      "step: 11339, loss: 1.2263331413269043\n",
      "Checkpoint is saved\n",
      "step: 11344, loss: 0.8846524953842163\n",
      "step: 11349, loss: 0.9039492607116699\n",
      "step: 11354, loss: 0.8647040128707886\n",
      "step: 11359, loss: 1.2669681310653687\n",
      "Checkpoint is saved\n",
      "step: 11364, loss: 0.9107299447059631\n",
      "step: 11369, loss: 0.6280728578567505\n",
      "step: 11374, loss: 1.3756077289581299\n",
      "step: 11379, loss: 0.9872846007347107\n",
      "Checkpoint is saved\n",
      "step: 11384, loss: 0.8261483907699585\n",
      "step: 11389, loss: 0.5738658308982849\n",
      "step: 11394, loss: 1.1655150651931763\n",
      "step: 11399, loss: 0.9601136445999146\n",
      "Checkpoint is saved\n",
      "step: 11404, loss: 0.5054926872253418\n",
      "step: 11409, loss: 0.6381019949913025\n",
      "step: 11414, loss: 1.1556172370910645\n",
      "step: 11419, loss: 1.2480342388153076\n",
      "Checkpoint is saved\n",
      "step: 11424, loss: 0.5888227224349976\n",
      "step: 11429, loss: 0.7053723931312561\n",
      "step: 11434, loss: 0.9525765180587769\n",
      "step: 11439, loss: 0.8220276236534119\n",
      "Checkpoint is saved\n",
      "step: 11444, loss: 0.5661273002624512\n",
      "step: 11449, loss: 0.8310997486114502\n",
      "step: 11454, loss: 0.837586522102356\n",
      "step: 11459, loss: 0.7639460563659668\n",
      "Checkpoint is saved\n",
      "step: 11464, loss: 0.8682464361190796\n",
      "step: 11469, loss: 0.7791825532913208\n",
      "step: 11474, loss: 0.9148677587509155\n",
      "step: 11479, loss: 0.7935250997543335\n",
      "Checkpoint is saved\n",
      "step: 11484, loss: 0.44481632113456726\n",
      "step: 11489, loss: 1.0035207271575928\n",
      "step: 11494, loss: 0.6267250776290894\n",
      "step: 11499, loss: 0.6255375742912292\n",
      "Checkpoint is saved\n",
      "step: 11504, loss: 0.3036365211009979\n",
      "step: 11509, loss: 0.6416401863098145\n",
      "step: 11514, loss: 1.0628474950790405\n",
      "step: 11519, loss: 0.7709985971450806\n",
      "Checkpoint is saved\n",
      "step: 11524, loss: 0.9627565145492554\n",
      "step: 11529, loss: 1.0584814548492432\n",
      "step: 11534, loss: 0.7601399421691895\n",
      "step: 11539, loss: 0.8650668263435364\n",
      "Checkpoint is saved\n",
      "step: 11544, loss: 0.7947179675102234\n",
      "step: 11549, loss: 0.862332820892334\n",
      "step: 11554, loss: 0.6783386468887329\n",
      "step: 11559, loss: 1.120596170425415\n",
      "Checkpoint is saved\n",
      "step: 11564, loss: 1.0269891023635864\n",
      "step: 11569, loss: 0.7852058410644531\n",
      "step: 11574, loss: 0.8453941941261292\n",
      "step: 11579, loss: 0.8581833839416504\n",
      "Checkpoint is saved\n",
      "step: 11584, loss: 1.013134241104126\n",
      "step: 11589, loss: 0.9959936738014221\n",
      "step: 11594, loss: 1.1303131580352783\n",
      "step: 11599, loss: 0.9179626703262329\n",
      "Checkpoint is saved\n",
      "step: 11604, loss: 1.2065370082855225\n",
      "step: 11609, loss: 0.9140868186950684\n",
      "step: 11614, loss: 1.0252339839935303\n",
      "step: 11619, loss: 0.9720819592475891\n",
      "Checkpoint is saved\n",
      "step: 11624, loss: 0.7699998617172241\n",
      "step: 11629, loss: 0.9449312686920166\n",
      "step: 11634, loss: 1.2048273086547852\n",
      "step: 11639, loss: 0.8114407062530518\n",
      "Checkpoint is saved\n",
      "step: 11644, loss: 0.8579983711242676\n",
      "step: 11649, loss: 1.2245142459869385\n",
      "step: 11654, loss: 1.1326673030853271\n",
      "step: 11659, loss: 0.9009355902671814\n",
      "Checkpoint is saved\n",
      "step: 11664, loss: 0.873772919178009\n",
      "step: 11669, loss: 0.7791328430175781\n",
      "step: 11674, loss: 1.2136728763580322\n",
      "step: 11679, loss: 0.9687449932098389\n",
      "Checkpoint is saved\n",
      "step: 11684, loss: 1.0968141555786133\n",
      "step: 11689, loss: 0.6938861608505249\n",
      "step: 11694, loss: 0.6477165222167969\n",
      "step: 11699, loss: 0.8076314926147461\n",
      "Checkpoint is saved\n",
      "step: 11704, loss: 1.0240029096603394\n",
      "step: 11709, loss: 1.1175428628921509\n",
      "step: 11714, loss: 1.204749345779419\n",
      "step: 11719, loss: 0.7925804853439331\n",
      "Checkpoint is saved\n",
      "step: 11724, loss: 0.9817368984222412\n",
      "step: 11729, loss: 0.9596842527389526\n",
      "step: 11734, loss: 0.8323287963867188\n",
      "step: 11739, loss: 0.9060726165771484\n",
      "Checkpoint is saved\n",
      "step: 11744, loss: 0.848984956741333\n",
      "step: 11749, loss: 0.849174439907074\n",
      "step: 11754, loss: 0.7118310928344727\n",
      "step: 11759, loss: 0.9944096803665161\n",
      "Checkpoint is saved\n",
      "step: 11764, loss: 1.369260311126709\n",
      "step: 11769, loss: 0.6775836944580078\n",
      "step: 11774, loss: 0.9579187631607056\n",
      "step: 11779, loss: 1.0969518423080444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 11784, loss: 0.8717961311340332\n",
      "step: 11789, loss: 1.140575647354126\n",
      "step: 11794, loss: 1.110039472579956\n",
      "step: 11799, loss: 1.1824527978897095\n",
      "Checkpoint is saved\n",
      "step: 11804, loss: 1.066908359527588\n",
      "step: 11809, loss: 0.9665595293045044\n",
      "step: 11814, loss: 0.983407199382782\n",
      "step: 11819, loss: 0.6841031312942505\n",
      "Checkpoint is saved\n",
      "step: 11824, loss: 0.8069016337394714\n",
      "step: 11829, loss: 1.1851341724395752\n",
      "step: 11834, loss: 0.7396728992462158\n",
      "step: 11839, loss: 0.7766300439834595\n",
      "Checkpoint is saved\n",
      "step: 11844, loss: 0.8453339338302612\n",
      "step: 11849, loss: 0.8363645076751709\n",
      "step: 11854, loss: 0.9834473133087158\n",
      "step: 11859, loss: 0.824800431728363\n",
      "Checkpoint is saved\n",
      "step: 11864, loss: 0.8523104786872864\n",
      "step: 11869, loss: 0.6447172164916992\n",
      "step: 11874, loss: 1.0137662887573242\n",
      "step: 11879, loss: 0.6954642534255981\n",
      "Checkpoint is saved\n",
      "step: 11884, loss: 1.449315071105957\n",
      "step: 11889, loss: 1.0923606157302856\n",
      "step: 11894, loss: 0.9676165580749512\n",
      "step: 11899, loss: 0.9601349234580994\n",
      "Checkpoint is saved\n",
      "step: 11904, loss: 0.7816519737243652\n",
      "step: 11909, loss: 1.1300206184387207\n",
      "step: 11914, loss: 1.1075658798217773\n",
      "step: 11919, loss: 1.0954174995422363\n",
      "Checkpoint is saved\n",
      "step: 11924, loss: 0.8603700399398804\n",
      "step: 11929, loss: 0.5714789628982544\n",
      "step: 11934, loss: 0.7563527822494507\n",
      "step: 11939, loss: 0.9004145860671997\n",
      "Checkpoint is saved\n",
      "step: 11944, loss: 1.038496494293213\n",
      "step: 11949, loss: 0.3782655596733093\n",
      "step: 11954, loss: 0.696070671081543\n",
      "step: 11959, loss: 1.024989128112793\n",
      "Checkpoint is saved\n",
      "step: 11964, loss: 1.182029366493225\n",
      "step: 11969, loss: 1.071325421333313\n",
      "step: 11974, loss: 0.5408550500869751\n",
      "step: 11979, loss: 0.7021401524543762\n",
      "Checkpoint is saved\n",
      "step: 11984, loss: 0.7689877152442932\n",
      "step: 11989, loss: 0.9118263721466064\n",
      "step: 11994, loss: 0.9797914028167725\n",
      "step: 11999, loss: 1.1242188215255737\n",
      "Checkpoint is saved\n",
      "step: 12004, loss: 0.9180371761322021\n",
      "step: 12009, loss: 0.5739587545394897\n",
      "step: 12014, loss: 0.5780359506607056\n",
      "step: 12019, loss: 1.0266777276992798\n",
      "Checkpoint is saved\n",
      "step: 12024, loss: 1.2153983116149902\n",
      "step: 12029, loss: 1.0452930927276611\n",
      "step: 12034, loss: 1.0889475345611572\n",
      "step: 12039, loss: 1.0353398323059082\n",
      "Checkpoint is saved\n",
      "step: 12044, loss: 0.8572981357574463\n",
      "step: 12049, loss: 0.8316789865493774\n",
      "step: 12054, loss: 1.2701832056045532\n",
      "step: 12059, loss: 0.6513290405273438\n",
      "Checkpoint is saved\n",
      "step: 12064, loss: 0.9001663327217102\n",
      "step: 12069, loss: 1.0859086513519287\n",
      "step: 12074, loss: 0.39144963026046753\n",
      "step: 12079, loss: 0.49919259548187256\n",
      "Checkpoint is saved\n",
      "step: 12084, loss: 0.6864275932312012\n",
      "step: 12089, loss: 1.0444680452346802\n",
      "step: 12094, loss: 1.112248420715332\n",
      "step: 12099, loss: 0.6887795925140381\n",
      "Checkpoint is saved\n",
      "step: 12104, loss: 1.433412790298462\n",
      "step: 12109, loss: 1.085200548171997\n",
      "step: 12114, loss: 1.3210980892181396\n",
      "step: 12119, loss: 0.7982460856437683\n",
      "Checkpoint is saved\n",
      "step: 12124, loss: 1.0047314167022705\n",
      "step: 12129, loss: 0.9160633087158203\n",
      "step: 12134, loss: 0.9173754453659058\n",
      "step: 12139, loss: 0.8669361472129822\n",
      "Checkpoint is saved\n",
      "step: 12144, loss: 0.7573351263999939\n",
      "step: 12149, loss: 0.39315927028656006\n",
      "step: 12154, loss: 0.7608753442764282\n",
      "step: 12159, loss: 0.9011597037315369\n",
      "Checkpoint is saved\n",
      "step: 12164, loss: 0.7906983494758606\n",
      "step: 12169, loss: 1.132798671722412\n",
      "step: 12174, loss: 0.6001001000404358\n",
      "step: 12179, loss: 0.8534590601921082\n",
      "Checkpoint is saved\n",
      "step: 12184, loss: 0.7320009469985962\n",
      "step: 12189, loss: 0.742145299911499\n",
      "step: 12194, loss: 0.9901561737060547\n",
      "step: 12199, loss: 1.2117741107940674\n",
      "Checkpoint is saved\n",
      "step: 12204, loss: 1.104198694229126\n",
      "step: 12209, loss: 0.9931581616401672\n",
      "step: 12214, loss: 0.9646426439285278\n",
      "step: 12219, loss: 0.9545596837997437\n",
      "Checkpoint is saved\n",
      "step: 12224, loss: 0.735347330570221\n",
      "step: 12229, loss: 1.111509084701538\n",
      "step: 12234, loss: 0.6642338633537292\n",
      "step: 12239, loss: 0.8753432035446167\n",
      "Checkpoint is saved\n",
      "step: 12244, loss: 0.8861278295516968\n",
      "step: 12249, loss: 0.913297712802887\n",
      "step: 12254, loss: 1.1902332305908203\n",
      "step: 12259, loss: 0.8677230477333069\n",
      "Checkpoint is saved\n",
      "step: 12264, loss: 0.6502280831336975\n",
      "step: 12269, loss: 0.8041304349899292\n",
      "step: 12274, loss: 0.7152921557426453\n",
      "step: 12279, loss: 1.293427586555481\n",
      "Checkpoint is saved\n",
      "step: 12284, loss: 1.1238042116165161\n",
      "step: 12289, loss: 0.8964440226554871\n",
      "step: 12294, loss: 1.2648966312408447\n",
      "step: 12299, loss: 0.594233512878418\n",
      "Checkpoint is saved\n",
      "step: 12304, loss: 0.8402349948883057\n",
      "step: 12309, loss: 0.8953304290771484\n",
      "step: 12314, loss: 0.5818719863891602\n",
      "step: 12319, loss: 0.9588254690170288\n",
      "Checkpoint is saved\n",
      "step: 12324, loss: 1.4964783191680908\n",
      "step: 12329, loss: 1.0177876949310303\n",
      "step: 12334, loss: 0.5607580542564392\n",
      "step: 12339, loss: 0.7533901333808899\n",
      "Checkpoint is saved\n",
      "step: 12344, loss: 1.1825510263442993\n",
      "step: 12349, loss: 0.8756734728813171\n",
      "step: 12354, loss: 0.7234762907028198\n",
      "step: 12359, loss: 0.8098355531692505\n",
      "Checkpoint is saved\n",
      "step: 12364, loss: 0.8910292387008667\n",
      "step: 12369, loss: 0.7541308403015137\n",
      "step: 12374, loss: 1.1133155822753906\n",
      "step: 12379, loss: 1.2842432260513306\n",
      "Checkpoint is saved\n",
      "step: 12384, loss: 0.780133843421936\n",
      "step: 12389, loss: 0.9243749380111694\n",
      "step: 12394, loss: 0.6325385570526123\n",
      "step: 12399, loss: 1.0991114377975464\n",
      "Checkpoint is saved\n",
      "step: 12404, loss: 1.1932337284088135\n",
      "step: 12409, loss: 1.0257790088653564\n",
      "step: 12414, loss: 0.3487325608730316\n",
      "step: 12419, loss: 0.9116856455802917\n",
      "Checkpoint is saved\n",
      "step: 12424, loss: 0.8954721689224243\n",
      "step: 12429, loss: 0.9080970287322998\n",
      "step: 12434, loss: 1.3974933624267578\n",
      "step: 12439, loss: 0.9109612703323364\n",
      "Checkpoint is saved\n",
      "step: 12444, loss: 1.1896573305130005\n",
      "step: 12449, loss: 0.7679895758628845\n",
      "step: 12454, loss: 1.0261133909225464\n",
      "step: 12459, loss: 1.217018723487854\n",
      "Checkpoint is saved\n",
      "step: 12464, loss: 0.580184817314148\n",
      "step: 12469, loss: 0.7041954398155212\n",
      "step: 12474, loss: 0.8156359195709229\n",
      "step: 12479, loss: 1.1053125858306885\n",
      "Checkpoint is saved\n",
      "step: 12484, loss: 0.7628634572029114\n",
      "step: 12489, loss: 0.7025995850563049\n",
      "step: 12494, loss: 0.7596787214279175\n",
      "step: 12499, loss: 0.8708440065383911\n",
      "Checkpoint is saved\n",
      "step: 12504, loss: 1.385047197341919\n",
      "step: 12509, loss: 0.6390190124511719\n",
      "step: 12514, loss: 0.9801006317138672\n",
      "step: 12519, loss: 0.6799800992012024\n",
      "Checkpoint is saved\n",
      "step: 12524, loss: 1.0455487966537476\n",
      "step: 12529, loss: 0.8616198301315308\n",
      "step: 12534, loss: 0.8700352907180786\n",
      "step: 12539, loss: 0.7011975646018982\n",
      "Checkpoint is saved\n",
      "step: 12544, loss: 1.2116620540618896\n",
      "step: 12549, loss: 1.0641674995422363\n",
      "step: 12554, loss: 1.2787487506866455\n",
      "step: 12559, loss: 1.0795713663101196\n",
      "Checkpoint is saved\n",
      "step: 12564, loss: 1.3443162441253662\n",
      "step: 12569, loss: 0.9976621270179749\n",
      "step: 12574, loss: 0.9917640686035156\n",
      "step: 12579, loss: 0.9609811305999756\n",
      "Checkpoint is saved\n",
      "step: 12584, loss: 0.9650741815567017\n",
      "step: 12589, loss: 1.1434545516967773\n",
      "step: 12594, loss: 0.9511218070983887\n",
      "step: 12599, loss: 0.5723986029624939\n",
      "Checkpoint is saved\n",
      "step: 12604, loss: 0.8042885661125183\n",
      "step: 12609, loss: 0.8597080707550049\n",
      "step: 12614, loss: 0.8563933372497559\n",
      "step: 12619, loss: 1.225663661956787\n",
      "Checkpoint is saved\n",
      "step: 12624, loss: 0.9309216737747192\n",
      "step: 12629, loss: 1.1233865022659302\n",
      "step: 12634, loss: 0.8594806790351868\n",
      "step: 12639, loss: 0.9317699670791626\n",
      "Checkpoint is saved\n",
      "step: 12644, loss: 0.9475563764572144\n",
      "step: 12649, loss: 1.0857645273208618\n",
      "step: 12654, loss: 1.1735888719558716\n",
      "step: 12659, loss: 0.8150912523269653\n",
      "Checkpoint is saved\n",
      "step: 12664, loss: 0.801347017288208\n",
      "step: 12669, loss: 0.4460966885089874\n",
      "step: 12674, loss: 1.0209674835205078\n",
      "step: 12679, loss: 0.7110241651535034\n",
      "Checkpoint is saved\n",
      "step: 12684, loss: 0.9323767423629761\n",
      "step: 12689, loss: 0.8268500566482544\n",
      "step: 12694, loss: 0.5363494157791138\n",
      "step: 12699, loss: 1.103381633758545\n",
      "Checkpoint is saved\n",
      "step: 12704, loss: 1.3461101055145264\n",
      "step: 12709, loss: 1.1712188720703125\n",
      "step: 12714, loss: 0.664243221282959\n",
      "step: 12719, loss: 0.5369847416877747\n",
      "Checkpoint is saved\n",
      "step: 12724, loss: 0.9188347458839417\n",
      "step: 12729, loss: 0.5581594109535217\n",
      "step: 12734, loss: 0.9036698341369629\n",
      "step: 12739, loss: 0.7500951886177063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 12744, loss: 0.9564204216003418\n",
      "step: 12749, loss: 0.9879786372184753\n",
      "step: 12754, loss: 1.0926861763000488\n",
      "step: 12759, loss: 0.6957106590270996\n",
      "Checkpoint is saved\n",
      "step: 12764, loss: 0.8831201791763306\n",
      "step: 12769, loss: 0.9543434381484985\n",
      "step: 12774, loss: 0.8374883532524109\n",
      "step: 12779, loss: 0.6548848748207092\n",
      "Checkpoint is saved\n",
      "step: 12784, loss: 0.9457458257675171\n",
      "step: 12789, loss: 0.798856258392334\n",
      "step: 12794, loss: 1.0335874557495117\n",
      "step: 12799, loss: 0.6851345300674438\n",
      "Checkpoint is saved\n",
      "step: 12804, loss: 1.0322656631469727\n",
      "step: 12809, loss: 0.9724435806274414\n",
      "step: 12814, loss: 0.7974026203155518\n",
      "step: 12819, loss: 0.8145291805267334\n",
      "Checkpoint is saved\n",
      "step: 12824, loss: 1.006697654724121\n",
      "step: 12829, loss: 1.237776517868042\n",
      "step: 12834, loss: 1.2477949857711792\n",
      "step: 12839, loss: 0.9192085266113281\n",
      "Checkpoint is saved\n",
      "step: 12844, loss: 1.1614868640899658\n",
      "step: 12849, loss: 1.256323218345642\n",
      "step: 12854, loss: 0.6193825602531433\n",
      "step: 12859, loss: 0.7907407283782959\n",
      "Checkpoint is saved\n",
      "step: 12864, loss: 0.910362958908081\n",
      "step: 12869, loss: 0.8442031741142273\n",
      "step: 12874, loss: 0.6978613138198853\n",
      "step: 12879, loss: 0.9046543836593628\n",
      "Checkpoint is saved\n",
      "step: 12884, loss: 0.9197694063186646\n",
      "step: 12889, loss: 0.7562391757965088\n",
      "step: 12894, loss: 1.049696445465088\n",
      "step: 12899, loss: 0.961603581905365\n",
      "Checkpoint is saved\n",
      "step: 12904, loss: 0.963461697101593\n",
      "step: 12909, loss: 0.6706421375274658\n",
      "step: 12914, loss: 0.5800050497055054\n",
      "step: 12919, loss: 0.8119029402732849\n",
      "Checkpoint is saved\n",
      "step: 12924, loss: 0.6761670112609863\n",
      "step: 12929, loss: 0.811575174331665\n",
      "step: 12934, loss: 1.328904151916504\n",
      "step: 12939, loss: 1.17135751247406\n",
      "Checkpoint is saved\n",
      "step: 12944, loss: 0.8443423509597778\n",
      "step: 12949, loss: 0.6908087730407715\n",
      "step: 12954, loss: 0.9527165293693542\n",
      "step: 12959, loss: 0.9934773445129395\n",
      "Checkpoint is saved\n",
      "step: 12964, loss: 1.4225821495056152\n",
      "step: 12969, loss: 0.9691128730773926\n",
      "step: 12974, loss: 1.3245697021484375\n",
      "step: 12979, loss: 1.1662534475326538\n",
      "Checkpoint is saved\n",
      "step: 12984, loss: 1.0060155391693115\n",
      "step: 12989, loss: 0.7362695932388306\n",
      "step: 12994, loss: 1.1991697549819946\n",
      "step: 12999, loss: 1.1735825538635254\n",
      "Checkpoint is saved\n",
      "step: 13004, loss: 0.505723237991333\n",
      "step: 13009, loss: 0.8845350742340088\n",
      "step: 13014, loss: 0.8819454908370972\n",
      "step: 13019, loss: 0.9967232942581177\n",
      "Checkpoint is saved\n",
      "step: 13024, loss: 0.9686919450759888\n",
      "step: 13029, loss: 0.6396177411079407\n",
      "step: 13034, loss: 0.985882580280304\n",
      "step: 13039, loss: 0.7534877061843872\n",
      "Checkpoint is saved\n",
      "step: 13044, loss: 1.3008376359939575\n",
      "step: 13049, loss: 0.5687345266342163\n",
      "step: 13054, loss: 1.3832294940948486\n",
      "step: 13059, loss: 1.1094071865081787\n",
      "Checkpoint is saved\n",
      "step: 13064, loss: 1.0327746868133545\n",
      "step: 13069, loss: 0.5771270990371704\n",
      "step: 13074, loss: 0.830175518989563\n",
      "step: 13079, loss: 0.883466899394989\n",
      "Checkpoint is saved\n",
      "step: 13084, loss: 0.9897158145904541\n",
      "step: 13089, loss: 1.1281530857086182\n",
      "step: 13094, loss: 0.7698633670806885\n",
      "step: 13099, loss: 0.7261387705802917\n",
      "Checkpoint is saved\n",
      "step: 13104, loss: 1.148616909980774\n",
      "step: 13109, loss: 0.7450475692749023\n",
      "step: 13114, loss: 0.7792593836784363\n",
      "step: 13119, loss: 0.6244916915893555\n",
      "Checkpoint is saved\n",
      "step: 13124, loss: 0.6872793436050415\n",
      "step: 13129, loss: 1.4752082824707031\n",
      "step: 13134, loss: 0.9987558722496033\n",
      "step: 13139, loss: 0.6460936069488525\n",
      "Checkpoint is saved\n",
      "step: 13144, loss: 0.6225082278251648\n",
      "step: 13149, loss: 0.9479215741157532\n",
      "step: 13154, loss: 0.7984885573387146\n",
      "step: 13159, loss: 1.0559842586517334\n",
      "Checkpoint is saved\n",
      "step: 13164, loss: 1.204314112663269\n",
      "step: 13169, loss: 0.8164449334144592\n",
      "step: 13174, loss: 1.0811296701431274\n",
      "step: 13179, loss: 0.969943106174469\n",
      "Checkpoint is saved\n",
      "step: 13184, loss: 1.2889081239700317\n",
      "step: 13189, loss: 0.9507940411567688\n",
      "step: 13194, loss: 0.3928632140159607\n",
      "step: 13199, loss: 0.9750409722328186\n",
      "Checkpoint is saved\n",
      "step: 13204, loss: 1.033546805381775\n",
      "step: 13209, loss: 0.8222706913948059\n",
      "step: 13214, loss: 0.7898056507110596\n",
      "step: 13219, loss: 1.0697896480560303\n",
      "Checkpoint is saved\n",
      "step: 13224, loss: 1.002216100692749\n",
      "step: 13229, loss: 0.958071231842041\n",
      "step: 13234, loss: 0.7274126410484314\n",
      "step: 13239, loss: 0.9645546674728394\n",
      "Checkpoint is saved\n",
      "step: 13244, loss: 1.255961298942566\n",
      "step: 13249, loss: 1.1742496490478516\n",
      "step: 13254, loss: 1.038651466369629\n",
      "step: 13259, loss: 0.5984550714492798\n",
      "Checkpoint is saved\n",
      "step: 13264, loss: 0.8942068219184875\n",
      "step: 13269, loss: 0.9936477541923523\n",
      "step: 13274, loss: 0.7003484964370728\n",
      "step: 13279, loss: 1.0590691566467285\n",
      "Checkpoint is saved\n",
      "step: 13284, loss: 1.0630645751953125\n",
      "step: 13289, loss: 0.51092529296875\n",
      "step: 13294, loss: 0.9203460216522217\n",
      "step: 13299, loss: 0.8223365545272827\n",
      "Checkpoint is saved\n",
      "step: 13304, loss: 0.616665244102478\n",
      "step: 13309, loss: 0.5610924959182739\n",
      "step: 13314, loss: 1.006619930267334\n",
      "step: 13319, loss: 0.964120090007782\n",
      "Checkpoint is saved\n",
      "step: 13324, loss: 0.8715454339981079\n",
      "step: 13329, loss: 0.7941150069236755\n",
      "step: 13334, loss: 0.5936048030853271\n",
      "step: 13339, loss: 0.6972700357437134\n",
      "Checkpoint is saved\n",
      "step: 13344, loss: 0.6070469617843628\n",
      "step: 13349, loss: 0.8199034333229065\n",
      "step: 13354, loss: 0.802431583404541\n",
      "step: 13359, loss: 0.7541764378547668\n",
      "Checkpoint is saved\n",
      "step: 13364, loss: 0.9172479510307312\n",
      "step: 13369, loss: 0.7995128631591797\n",
      "step: 13374, loss: 1.0065137147903442\n",
      "step: 13379, loss: 0.8876297473907471\n",
      "Checkpoint is saved\n",
      "step: 13384, loss: 0.5966117978096008\n",
      "step: 13389, loss: 1.0760633945465088\n",
      "step: 13394, loss: 1.2657029628753662\n",
      "step: 13399, loss: 1.0984077453613281\n",
      "Checkpoint is saved\n",
      "step: 13404, loss: 0.9036233425140381\n",
      "step: 13409, loss: 0.8711918592453003\n",
      "step: 13414, loss: 1.0092411041259766\n",
      "step: 13419, loss: 0.979461669921875\n",
      "Checkpoint is saved\n",
      "step: 13424, loss: 0.8638496398925781\n",
      "step: 13429, loss: 0.9186336398124695\n",
      "step: 13434, loss: 0.6462303400039673\n",
      "step: 13439, loss: 1.1592037677764893\n",
      "Checkpoint is saved\n",
      "step: 13444, loss: 0.46362191438674927\n",
      "step: 13449, loss: 0.841821551322937\n",
      "step: 13454, loss: 0.7937471866607666\n",
      "step: 13459, loss: 0.8493402600288391\n",
      "Checkpoint is saved\n",
      "step: 13464, loss: 0.9425360560417175\n",
      "step: 13469, loss: 0.8037341833114624\n",
      "step: 13474, loss: 0.6870047450065613\n",
      "step: 13479, loss: 0.9580410718917847\n",
      "Checkpoint is saved\n",
      "step: 13484, loss: 1.131288766860962\n",
      "step: 13489, loss: 0.6822319030761719\n",
      "step: 13494, loss: 1.3666636943817139\n",
      "step: 13499, loss: 0.8686387538909912\n",
      "Checkpoint is saved\n",
      "step: 13504, loss: 0.7424904108047485\n",
      "step: 13509, loss: 0.7279248833656311\n",
      "step: 13514, loss: 1.2162169218063354\n",
      "step: 13519, loss: 0.6680957674980164\n",
      "Checkpoint is saved\n",
      "step: 13524, loss: 1.080755591392517\n",
      "step: 13529, loss: 1.1069927215576172\n",
      "step: 13534, loss: 0.5655280947685242\n",
      "step: 13539, loss: 1.1563773155212402\n",
      "Checkpoint is saved\n",
      "step: 13544, loss: 1.2534105777740479\n",
      "step: 13549, loss: 0.8498351573944092\n",
      "step: 13554, loss: 0.6839967966079712\n",
      "step: 13559, loss: 0.8852707147598267\n",
      "Checkpoint is saved\n",
      "step: 13564, loss: 0.7682631611824036\n",
      "step: 13569, loss: 0.9507593512535095\n",
      "step: 13574, loss: 1.06436288356781\n",
      "step: 13579, loss: 0.9603782892227173\n",
      "Checkpoint is saved\n",
      "step: 13584, loss: 0.6371870636940002\n",
      "step: 13589, loss: 1.315516471862793\n",
      "step: 13594, loss: 0.6883594989776611\n",
      "step: 13599, loss: 0.8071038722991943\n",
      "Checkpoint is saved\n",
      "step: 13604, loss: 1.0578522682189941\n",
      "step: 13609, loss: 0.746566891670227\n",
      "step: 13614, loss: 1.0617809295654297\n",
      "step: 13619, loss: 0.6928575038909912\n",
      "Checkpoint is saved\n",
      "step: 13624, loss: 1.1483700275421143\n",
      "step: 13629, loss: 0.7424253821372986\n",
      "step: 13634, loss: 0.8361389636993408\n",
      "step: 13639, loss: 0.6096931099891663\n",
      "Checkpoint is saved\n",
      "step: 13644, loss: 1.059308648109436\n",
      "step: 13649, loss: 0.6100858449935913\n",
      "step: 13654, loss: 0.8817725777626038\n",
      "step: 13659, loss: 0.7815245389938354\n",
      "Checkpoint is saved\n",
      "step: 13664, loss: 0.5424769520759583\n",
      "step: 13669, loss: 0.9303607940673828\n",
      "step: 13674, loss: 0.5774592757225037\n",
      "step: 13679, loss: 1.3550457954406738\n",
      "Checkpoint is saved\n",
      "step: 13684, loss: 0.7460514307022095\n",
      "step: 13689, loss: 1.2631829977035522\n",
      "step: 13694, loss: 0.8107659816741943\n",
      "step: 13699, loss: 1.0246553421020508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 13704, loss: 0.9047930240631104\n",
      "step: 13709, loss: 0.6390922665596008\n",
      "step: 13714, loss: 0.9076205492019653\n",
      "step: 13719, loss: 0.8135183453559875\n",
      "Checkpoint is saved\n",
      "step: 13724, loss: 0.9250400066375732\n",
      "step: 13729, loss: 0.9478870630264282\n",
      "step: 13734, loss: 0.963775098323822\n",
      "step: 13739, loss: 0.9233943223953247\n",
      "Checkpoint is saved\n",
      "step: 13744, loss: 0.7636398673057556\n",
      "step: 13749, loss: 0.7346165180206299\n",
      "step: 13754, loss: 1.1638199090957642\n",
      "step: 13759, loss: 0.6606742739677429\n",
      "Checkpoint is saved\n",
      "step: 13764, loss: 1.1558727025985718\n",
      "step: 13769, loss: 0.919478178024292\n",
      "step: 13774, loss: 0.8477303981781006\n",
      "step: 13779, loss: 1.086552619934082\n",
      "Checkpoint is saved\n",
      "step: 13784, loss: 0.7880396842956543\n",
      "step: 13789, loss: 0.8669055700302124\n",
      "step: 13794, loss: 0.6122064590454102\n",
      "step: 13799, loss: 1.1410988569259644\n",
      "Checkpoint is saved\n",
      "step: 13804, loss: 1.0847283601760864\n",
      "step: 13809, loss: 1.057399034500122\n",
      "step: 13814, loss: 0.7447716593742371\n",
      "step: 13819, loss: 0.8860841989517212\n",
      "Checkpoint is saved\n",
      "step: 13824, loss: 0.8228968381881714\n",
      "step: 13829, loss: 0.6665247678756714\n",
      "step: 13834, loss: 0.5779699087142944\n",
      "step: 13839, loss: 0.655738115310669\n",
      "Checkpoint is saved\n",
      "step: 13844, loss: 0.6789282560348511\n",
      "step: 13849, loss: 1.4020164012908936\n",
      "step: 13854, loss: 1.2297247648239136\n",
      "step: 13859, loss: 0.5231796503067017\n",
      "Checkpoint is saved\n",
      "step: 13864, loss: 0.9337656497955322\n",
      "step: 13869, loss: 1.1253352165222168\n",
      "step: 13874, loss: 0.822434663772583\n",
      "step: 13879, loss: 0.7115939855575562\n",
      "Checkpoint is saved\n",
      "step: 13884, loss: 1.0620027780532837\n",
      "step: 13889, loss: 0.726211428642273\n",
      "step: 13894, loss: 0.9513612389564514\n",
      "step: 13899, loss: 1.0372569561004639\n",
      "Checkpoint is saved\n",
      "step: 13904, loss: 0.5754016637802124\n",
      "step: 13909, loss: 0.5824117660522461\n",
      "step: 13914, loss: 0.554814338684082\n",
      "step: 13919, loss: 0.9682859182357788\n",
      "Checkpoint is saved\n",
      "step: 13924, loss: 0.9678341150283813\n",
      "step: 13929, loss: 0.6897708773612976\n",
      "step: 13934, loss: 0.7139519453048706\n",
      "step: 13939, loss: 0.6414551138877869\n",
      "Checkpoint is saved\n",
      "step: 13944, loss: 1.185492753982544\n",
      "step: 13949, loss: 0.7298686504364014\n",
      "step: 13954, loss: 0.4635362923145294\n",
      "step: 13959, loss: 1.1486506462097168\n",
      "Checkpoint is saved\n",
      "step: 13964, loss: 0.5705378651618958\n",
      "step: 13969, loss: 1.0859946012496948\n",
      "step: 13974, loss: 1.011877179145813\n",
      "step: 13979, loss: 0.7080360651016235\n",
      "Checkpoint is saved\n",
      "step: 13984, loss: 0.8058546185493469\n",
      "step: 13989, loss: 0.8716744184494019\n",
      "step: 13994, loss: 0.9503283500671387\n",
      "step: 13999, loss: 0.9546349048614502\n",
      "Checkpoint is saved\n",
      "step: 14004, loss: 0.7225145697593689\n",
      "step: 14009, loss: 0.8391571044921875\n",
      "step: 14014, loss: 0.9152866005897522\n",
      "step: 14019, loss: 0.6352208256721497\n",
      "Checkpoint is saved\n",
      "step: 14024, loss: 0.7828541994094849\n",
      "step: 14029, loss: 0.7296170592308044\n",
      "step: 14034, loss: 0.6618353128433228\n",
      "step: 14039, loss: 1.2413480281829834\n",
      "Checkpoint is saved\n",
      "step: 14044, loss: 1.1037468910217285\n",
      "step: 14049, loss: 0.8375903367996216\n",
      "step: 14054, loss: 0.6878365278244019\n",
      "step: 14059, loss: 1.4209198951721191\n",
      "Checkpoint is saved\n",
      "step: 14064, loss: 0.8184667825698853\n",
      "step: 14069, loss: 0.856170117855072\n",
      "step: 14074, loss: 1.1819446086883545\n",
      "step: 14079, loss: 0.8595757484436035\n",
      "Checkpoint is saved\n",
      "step: 14084, loss: 1.639550805091858\n",
      "step: 14089, loss: 0.8936642408370972\n",
      "step: 14094, loss: 0.970025897026062\n",
      "step: 14099, loss: 0.8673535585403442\n",
      "Checkpoint is saved\n",
      "step: 14104, loss: 1.2062573432922363\n",
      "step: 14109, loss: 0.8597455620765686\n",
      "step: 14114, loss: 0.8543403148651123\n",
      "step: 14119, loss: 0.42970511317253113\n",
      "Checkpoint is saved\n",
      "step: 14124, loss: 0.9837623834609985\n",
      "step: 14129, loss: 1.1524349451065063\n",
      "step: 14134, loss: 0.8023378252983093\n",
      "step: 14139, loss: 0.7161328792572021\n",
      "Checkpoint is saved\n",
      "step: 14144, loss: 0.9721931219100952\n",
      "step: 14149, loss: 0.9033159017562866\n",
      "step: 14154, loss: 0.7592589855194092\n",
      "step: 14159, loss: 0.8270604610443115\n",
      "Checkpoint is saved\n",
      "step: 14164, loss: 0.9925816059112549\n",
      "step: 14169, loss: 1.075042486190796\n",
      "step: 14174, loss: 0.9674912095069885\n",
      "step: 14179, loss: 0.8001436591148376\n",
      "Checkpoint is saved\n",
      "step: 14184, loss: 0.7349718809127808\n",
      "step: 14189, loss: 0.45406830310821533\n",
      "step: 14194, loss: 0.641610860824585\n",
      "step: 14199, loss: 0.8137661814689636\n",
      "Checkpoint is saved\n",
      "step: 14204, loss: 0.6551914215087891\n",
      "step: 14209, loss: 1.3356982469558716\n",
      "step: 14214, loss: 0.851629376411438\n",
      "step: 14219, loss: 1.0979890823364258\n",
      "Checkpoint is saved\n",
      "step: 14224, loss: 0.8869608044624329\n",
      "step: 14229, loss: 0.5313156843185425\n",
      "step: 14234, loss: 1.129981279373169\n",
      "step: 14239, loss: 0.7040098905563354\n",
      "Checkpoint is saved\n",
      "step: 14244, loss: 1.4392507076263428\n",
      "step: 14249, loss: 0.7093054056167603\n",
      "step: 14254, loss: 1.1184513568878174\n",
      "step: 14259, loss: 0.6366555094718933\n",
      "Checkpoint is saved\n",
      "step: 14264, loss: 0.45511841773986816\n",
      "step: 14269, loss: 0.46818292140960693\n",
      "step: 14274, loss: 0.9559128284454346\n",
      "step: 14279, loss: 0.8687058687210083\n",
      "Checkpoint is saved\n",
      "step: 14284, loss: 1.0064903497695923\n",
      "step: 14289, loss: 0.9259873628616333\n",
      "step: 14294, loss: 0.964828610420227\n",
      "step: 14299, loss: 1.0169529914855957\n",
      "Checkpoint is saved\n",
      "step: 14304, loss: 1.1632899045944214\n",
      "step: 14309, loss: 0.5692169666290283\n",
      "step: 14314, loss: 0.6944214105606079\n",
      "step: 14319, loss: 0.6603685617446899\n",
      "Checkpoint is saved\n",
      "step: 14324, loss: 1.0802334547042847\n",
      "step: 14329, loss: 0.7083562612533569\n",
      "step: 14334, loss: 0.8996337652206421\n",
      "step: 14339, loss: 1.0028870105743408\n",
      "Checkpoint is saved\n",
      "step: 14344, loss: 0.7829445600509644\n",
      "step: 14349, loss: 0.906471848487854\n",
      "step: 14354, loss: 0.7836079001426697\n",
      "step: 14359, loss: 1.000158667564392\n",
      "Checkpoint is saved\n",
      "step: 14364, loss: 1.0989118814468384\n",
      "step: 14369, loss: 0.6220656037330627\n",
      "step: 14374, loss: 1.1296947002410889\n",
      "step: 14379, loss: 0.6499324440956116\n",
      "Checkpoint is saved\n",
      "step: 14384, loss: 0.7766127586364746\n",
      "step: 14389, loss: 1.0669357776641846\n",
      "step: 14394, loss: 1.0758495330810547\n",
      "step: 14399, loss: 0.9793131351470947\n",
      "Checkpoint is saved\n",
      "step: 14404, loss: 0.8873209953308105\n",
      "step: 14409, loss: 0.7934346795082092\n",
      "step: 14414, loss: 0.6248632669448853\n",
      "step: 14419, loss: 0.8322492837905884\n",
      "Checkpoint is saved\n",
      "step: 14424, loss: 1.0825742483139038\n",
      "step: 14429, loss: 0.6317184567451477\n",
      "step: 14434, loss: 0.8692309856414795\n",
      "step: 14439, loss: 0.7817107439041138\n",
      "Checkpoint is saved\n",
      "step: 14444, loss: 1.4812419414520264\n",
      "step: 14449, loss: 1.2800601720809937\n",
      "step: 14454, loss: 0.9684908986091614\n",
      "step: 14459, loss: 0.987006664276123\n",
      "Checkpoint is saved\n",
      "step: 14464, loss: 1.0479648113250732\n",
      "step: 14469, loss: 0.7965549230575562\n",
      "step: 14474, loss: 0.8588353991508484\n",
      "step: 14479, loss: 0.5100577473640442\n",
      "Checkpoint is saved\n",
      "step: 14484, loss: 0.9772112965583801\n",
      "step: 14489, loss: 0.49816542863845825\n",
      "step: 14494, loss: 0.7975881099700928\n",
      "step: 14499, loss: 1.4550549983978271\n",
      "Checkpoint is saved\n",
      "step: 14504, loss: 1.3214128017425537\n",
      "step: 14509, loss: 1.317373514175415\n",
      "step: 14514, loss: 1.1376831531524658\n",
      "step: 14519, loss: 0.5653586387634277\n",
      "Checkpoint is saved\n",
      "step: 14524, loss: 0.9349877834320068\n",
      "step: 14529, loss: 0.89142906665802\n",
      "step: 14534, loss: 0.9598126411437988\n",
      "step: 14539, loss: 0.9084040522575378\n",
      "Checkpoint is saved\n",
      "step: 14544, loss: 1.2737911939620972\n",
      "step: 14549, loss: 0.7703861594200134\n",
      "step: 14554, loss: 0.6470288038253784\n",
      "step: 14559, loss: 0.6135643720626831\n",
      "Checkpoint is saved\n",
      "step: 14564, loss: 0.7586468458175659\n",
      "step: 14569, loss: 0.4226192831993103\n",
      "step: 14574, loss: 0.4752753674983978\n",
      "step: 14579, loss: 0.733736515045166\n",
      "Checkpoint is saved\n",
      "step: 14584, loss: 1.2380940914154053\n",
      "step: 14589, loss: 0.7709476351737976\n",
      "step: 14594, loss: 1.032591462135315\n",
      "step: 14599, loss: 1.0822761058807373\n",
      "Checkpoint is saved\n",
      "step: 14604, loss: 0.9064255952835083\n",
      "step: 14609, loss: 0.8121053576469421\n",
      "step: 14614, loss: 0.8671553730964661\n",
      "step: 14619, loss: 0.9778753519058228\n",
      "Checkpoint is saved\n",
      "step: 14624, loss: 0.6605718731880188\n",
      "step: 14629, loss: 1.1575264930725098\n",
      "step: 14634, loss: 0.6182141900062561\n",
      "step: 14639, loss: 0.8979816436767578\n",
      "Checkpoint is saved\n",
      "step: 14644, loss: 0.6068581342697144\n",
      "step: 14649, loss: 0.8115320205688477\n",
      "step: 14654, loss: 0.7228495478630066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14659, loss: 0.9498496055603027\n",
      "Checkpoint is saved\n",
      "step: 14664, loss: 0.8410230875015259\n",
      "step: 14669, loss: 0.49838975071907043\n",
      "step: 14674, loss: 1.1226774454116821\n",
      "step: 14679, loss: 0.8106410503387451\n",
      "Checkpoint is saved\n",
      "step: 14684, loss: 0.6019956469535828\n",
      "step: 14689, loss: 0.6143685579299927\n",
      "step: 14694, loss: 0.9637335538864136\n",
      "step: 14699, loss: 0.7659050226211548\n",
      "Checkpoint is saved\n",
      "step: 14704, loss: 0.726588249206543\n",
      "step: 14709, loss: 0.6001591086387634\n",
      "step: 14714, loss: 1.0482068061828613\n",
      "step: 14719, loss: 0.8267421126365662\n",
      "Checkpoint is saved\n",
      "step: 14724, loss: 0.7846963405609131\n",
      "step: 14729, loss: 1.049147129058838\n",
      "step: 14734, loss: 0.7995488047599792\n",
      "step: 14739, loss: 0.7504047751426697\n",
      "Checkpoint is saved\n",
      "step: 14744, loss: 1.355010747909546\n",
      "step: 14749, loss: 0.793358564376831\n",
      "step: 14754, loss: 1.0793510675430298\n",
      "step: 14759, loss: 1.312143325805664\n",
      "Checkpoint is saved\n",
      "step: 14764, loss: 0.7844924926757812\n",
      "step: 14769, loss: 0.6497007608413696\n",
      "step: 14774, loss: 0.8943462371826172\n",
      "step: 14779, loss: 0.5427745580673218\n",
      "Checkpoint is saved\n",
      "step: 14784, loss: 0.5876321196556091\n",
      "step: 14789, loss: 1.0864293575286865\n",
      "step: 14794, loss: 0.7076559662818909\n",
      "step: 14799, loss: 1.0419241189956665\n",
      "Checkpoint is saved\n",
      "step: 14804, loss: 1.1051181554794312\n",
      "step: 14809, loss: 0.5992082357406616\n",
      "step: 14814, loss: 1.2718347311019897\n",
      "step: 14819, loss: 0.7709068059921265\n",
      "Checkpoint is saved\n",
      "step: 14824, loss: 0.6646759510040283\n",
      "step: 14829, loss: 0.5323240756988525\n",
      "step: 14834, loss: 0.7187230587005615\n",
      "step: 14839, loss: 0.8422249555587769\n",
      "Checkpoint is saved\n",
      "step: 14844, loss: 0.7855680584907532\n",
      "step: 14849, loss: 1.096963882446289\n",
      "step: 14854, loss: 0.5934194922447205\n",
      "step: 14859, loss: 1.4102853536605835\n",
      "Checkpoint is saved\n",
      "step: 14864, loss: 0.9808741211891174\n",
      "step: 14869, loss: 0.9924785494804382\n",
      "step: 14874, loss: 0.6144723892211914\n",
      "step: 14879, loss: 1.4182533025741577\n",
      "Checkpoint is saved\n",
      "step: 14884, loss: 0.877082347869873\n",
      "step: 14889, loss: 0.6929435729980469\n",
      "step: 14894, loss: 0.5479962825775146\n",
      "step: 14899, loss: 1.104966163635254\n",
      "Checkpoint is saved\n",
      "step: 14904, loss: 0.7487651109695435\n",
      "step: 14909, loss: 0.8401944041252136\n",
      "step: 14914, loss: 0.9180028438568115\n",
      "step: 14919, loss: 1.109866738319397\n",
      "Checkpoint is saved\n",
      "step: 14924, loss: 0.6826984882354736\n",
      "step: 14929, loss: 0.6898379325866699\n",
      "step: 14934, loss: 1.2268779277801514\n",
      "step: 14939, loss: 0.8167027235031128\n",
      "Checkpoint is saved\n",
      "step: 14944, loss: 0.8885343074798584\n",
      "step: 14949, loss: 1.2074798345565796\n",
      "step: 14954, loss: 0.977497398853302\n",
      "step: 14959, loss: 0.7708362340927124\n",
      "Checkpoint is saved\n",
      "step: 14964, loss: 0.6267815232276917\n",
      "step: 14969, loss: 0.45382386445999146\n",
      "step: 14974, loss: 1.0460965633392334\n",
      "step: 14979, loss: 0.7858641147613525\n",
      "Checkpoint is saved\n",
      "step: 14984, loss: 1.06662917137146\n",
      "step: 14989, loss: 0.9141318798065186\n",
      "step: 14994, loss: 0.6658997535705566\n",
      "step: 14999, loss: 0.44546982645988464\n",
      "Checkpoint is saved\n",
      "step: 15004, loss: 1.0468437671661377\n",
      "step: 15009, loss: 1.2082347869873047\n",
      "step: 15014, loss: 0.86696857213974\n",
      "step: 15019, loss: 0.7613029479980469\n",
      "Checkpoint is saved\n",
      "step: 15024, loss: 0.9580039978027344\n",
      "step: 15029, loss: 0.6700869798660278\n",
      "step: 15034, loss: 0.8469570875167847\n",
      "step: 15039, loss: 0.5248024463653564\n",
      "Checkpoint is saved\n",
      "step: 15044, loss: 0.6002799868583679\n",
      "step: 15049, loss: 0.8313125371932983\n",
      "step: 15054, loss: 0.41150811314582825\n",
      "step: 15059, loss: 0.4194912612438202\n",
      "Checkpoint is saved\n",
      "step: 15064, loss: 0.8171508312225342\n",
      "step: 15069, loss: 0.7163211703300476\n",
      "step: 15074, loss: 0.6717021465301514\n",
      "step: 15079, loss: 0.74434494972229\n",
      "Checkpoint is saved\n",
      "step: 15084, loss: 0.7353183031082153\n",
      "step: 15089, loss: 0.6679498553276062\n",
      "step: 15094, loss: 1.053360939025879\n",
      "step: 15099, loss: 0.9104634523391724\n",
      "Checkpoint is saved\n",
      "step: 15104, loss: 0.7678694725036621\n",
      "step: 15109, loss: 0.7546895146369934\n",
      "step: 15114, loss: 0.9066100716590881\n",
      "step: 15119, loss: 0.7175365686416626\n",
      "Checkpoint is saved\n",
      "step: 15124, loss: 1.0705876350402832\n",
      "step: 15129, loss: 0.48584580421447754\n",
      "step: 15134, loss: 0.6176912188529968\n",
      "step: 15139, loss: 1.3076318502426147\n",
      "Checkpoint is saved\n",
      "step: 15144, loss: 0.884769856929779\n",
      "step: 15149, loss: 0.7315894365310669\n",
      "step: 15154, loss: 0.5708149671554565\n",
      "step: 15159, loss: 0.9733423590660095\n",
      "Checkpoint is saved\n",
      "step: 15164, loss: 1.0197069644927979\n",
      "step: 15169, loss: 0.9633216857910156\n",
      "step: 15174, loss: 0.7899803519248962\n",
      "step: 15179, loss: 0.8076263666152954\n",
      "Checkpoint is saved\n",
      "step: 15184, loss: 0.8307662010192871\n",
      "step: 15189, loss: 1.0187709331512451\n",
      "step: 15194, loss: 0.8393167853355408\n",
      "step: 15199, loss: 0.9302983283996582\n",
      "Checkpoint is saved\n",
      "step: 15204, loss: 0.8706533908843994\n",
      "step: 15209, loss: 0.9054034948348999\n",
      "step: 15214, loss: 1.0555295944213867\n",
      "step: 15219, loss: 0.833969235420227\n",
      "Checkpoint is saved\n",
      "step: 15224, loss: 1.0695569515228271\n",
      "step: 15229, loss: 1.139296531677246\n",
      "step: 15234, loss: 0.760545551776886\n",
      "step: 15239, loss: 0.7652409076690674\n",
      "Checkpoint is saved\n",
      "step: 15244, loss: 0.8666640520095825\n",
      "step: 15249, loss: 0.6096703410148621\n",
      "step: 15254, loss: 0.4734227657318115\n",
      "step: 15259, loss: 1.0524375438690186\n",
      "Checkpoint is saved\n",
      "step: 15264, loss: 0.8457404375076294\n",
      "step: 15269, loss: 0.4872535765171051\n",
      "step: 15274, loss: 1.0707463026046753\n",
      "step: 15279, loss: 0.7308430671691895\n",
      "Checkpoint is saved\n",
      "step: 15284, loss: 0.6065629720687866\n",
      "step: 15289, loss: 0.6660857200622559\n",
      "step: 15294, loss: 1.0585567951202393\n",
      "step: 15299, loss: 0.8949058055877686\n",
      "Checkpoint is saved\n",
      "step: 15304, loss: 0.7402543425559998\n",
      "step: 15309, loss: 0.5075721740722656\n",
      "step: 15314, loss: 0.7663958668708801\n",
      "step: 15319, loss: 1.1080570220947266\n",
      "Checkpoint is saved\n",
      "step: 15324, loss: 0.7488791346549988\n",
      "step: 15329, loss: 1.0046457052230835\n",
      "step: 15334, loss: 0.9037774801254272\n",
      "step: 15339, loss: 1.0233347415924072\n",
      "Checkpoint is saved\n",
      "step: 15344, loss: 0.7291195392608643\n",
      "step: 15349, loss: 0.8387719988822937\n",
      "step: 15354, loss: 0.5737067461013794\n",
      "step: 15359, loss: 0.9248732328414917\n",
      "Checkpoint is saved\n",
      "step: 15364, loss: 0.7994303703308105\n",
      "step: 15369, loss: 0.9355810284614563\n",
      "step: 15374, loss: 0.7804988026618958\n",
      "step: 15379, loss: 0.8152357339859009\n",
      "Checkpoint is saved\n",
      "step: 15384, loss: 0.9317641258239746\n",
      "step: 15389, loss: 0.5813900828361511\n",
      "step: 15394, loss: 1.0269160270690918\n",
      "step: 15399, loss: 1.1100881099700928\n",
      "Checkpoint is saved\n",
      "step: 15404, loss: 1.0130314826965332\n",
      "step: 15409, loss: 0.9496028423309326\n",
      "step: 15414, loss: 0.7364863157272339\n",
      "step: 15419, loss: 0.7896678447723389\n",
      "Checkpoint is saved\n",
      "step: 15424, loss: 0.7912187576293945\n",
      "step: 15429, loss: 0.7635995149612427\n",
      "step: 15434, loss: 0.6780518889427185\n",
      "step: 15439, loss: 0.8719869256019592\n",
      "Checkpoint is saved\n",
      "step: 15444, loss: 1.0110890865325928\n",
      "step: 15449, loss: 0.8519918918609619\n",
      "step: 15454, loss: 0.7145943641662598\n",
      "step: 15459, loss: 0.5867743492126465\n",
      "Checkpoint is saved\n",
      "step: 15464, loss: 0.9644591808319092\n",
      "step: 15469, loss: 0.9043354392051697\n",
      "step: 15474, loss: 0.9950935244560242\n",
      "step: 15479, loss: 0.8053165674209595\n",
      "Checkpoint is saved\n",
      "step: 15484, loss: 0.8781763315200806\n",
      "step: 15489, loss: 0.8949634432792664\n",
      "step: 15494, loss: 0.8178315162658691\n",
      "step: 15499, loss: 1.287146806716919\n",
      "Checkpoint is saved\n",
      "step: 15504, loss: 0.7141622304916382\n",
      "step: 15509, loss: 0.7454637289047241\n",
      "step: 15514, loss: 1.20430326461792\n",
      "step: 15519, loss: 0.44964736700057983\n",
      "Checkpoint is saved\n",
      "step: 15524, loss: 0.8196238279342651\n",
      "step: 15529, loss: 0.9157698154449463\n",
      "step: 15534, loss: 0.5272219181060791\n",
      "step: 15539, loss: 0.4333019256591797\n",
      "Checkpoint is saved\n",
      "step: 15544, loss: 0.7754135131835938\n",
      "step: 15549, loss: 1.042346477508545\n",
      "step: 15554, loss: 1.0625112056732178\n",
      "step: 15559, loss: 1.0066840648651123\n",
      "Checkpoint is saved\n",
      "step: 15564, loss: 0.8206375241279602\n",
      "step: 15569, loss: 0.9019153118133545\n",
      "step: 15574, loss: 0.8743189573287964\n",
      "step: 15579, loss: 0.4261161983013153\n",
      "Checkpoint is saved\n",
      "step: 15584, loss: 1.1346943378448486\n",
      "step: 15589, loss: 1.0969069004058838\n",
      "step: 15594, loss: 0.9235583543777466\n",
      "step: 15599, loss: 1.2680108547210693\n",
      "Checkpoint is saved\n",
      "step: 15604, loss: 0.6119740009307861\n",
      "step: 15609, loss: 0.791824460029602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15614, loss: 1.0803767442703247\n",
      "step: 15619, loss: 1.2280733585357666\n",
      "Checkpoint is saved\n",
      "step: 15624, loss: 0.5965579748153687\n",
      "step: 15629, loss: 0.8066095113754272\n",
      "step: 15634, loss: 1.1677318811416626\n",
      "step: 15639, loss: 0.7797979116439819\n",
      "Checkpoint is saved\n",
      "step: 15644, loss: 0.5689419507980347\n",
      "step: 15649, loss: 0.7816677689552307\n",
      "step: 15654, loss: 1.0065561532974243\n",
      "step: 15659, loss: 1.0250182151794434\n",
      "Checkpoint is saved\n",
      "step: 15664, loss: 0.6323757171630859\n",
      "step: 15669, loss: 0.7402093410491943\n",
      "step: 15674, loss: 0.524827778339386\n",
      "step: 15679, loss: 0.6540477275848389\n",
      "Checkpoint is saved\n",
      "step: 15684, loss: 1.0604805946350098\n",
      "step: 15689, loss: 0.813093900680542\n",
      "step: 15694, loss: 1.1747338771820068\n",
      "step: 15699, loss: 0.907885730266571\n",
      "Checkpoint is saved\n",
      "step: 15704, loss: 0.7862274646759033\n",
      "step: 15709, loss: 0.5046315789222717\n",
      "step: 15714, loss: 0.8521928191184998\n",
      "step: 15719, loss: 0.9840314388275146\n",
      "Checkpoint is saved\n",
      "step: 15724, loss: 0.8372880816459656\n",
      "step: 15729, loss: 0.7243843674659729\n",
      "step: 15734, loss: 0.626737117767334\n",
      "step: 15739, loss: 0.9811493158340454\n",
      "Checkpoint is saved\n",
      "step: 15744, loss: 0.8782402276992798\n",
      "step: 15749, loss: 0.7984285354614258\n",
      "step: 15754, loss: 0.30685728788375854\n",
      "step: 15759, loss: 0.6372730731964111\n",
      "Checkpoint is saved\n",
      "step: 15764, loss: 0.9826825261116028\n",
      "step: 15769, loss: 1.0583370923995972\n",
      "step: 15774, loss: 1.2581400871276855\n",
      "step: 15779, loss: 1.0356028079986572\n",
      "Checkpoint is saved\n",
      "step: 15784, loss: 0.9944056868553162\n",
      "step: 15789, loss: 0.885621190071106\n",
      "step: 15794, loss: 0.8932657837867737\n",
      "step: 15799, loss: 1.1261913776397705\n",
      "Checkpoint is saved\n",
      "step: 15804, loss: 0.8644919991493225\n",
      "step: 15809, loss: 1.2435935735702515\n",
      "step: 15814, loss: 0.6836200952529907\n",
      "step: 15819, loss: 0.8947462439537048\n",
      "Checkpoint is saved\n",
      "step: 15824, loss: 0.8811349868774414\n",
      "step: 15829, loss: 1.370456576347351\n",
      "step: 15834, loss: 1.391517996788025\n",
      "step: 15839, loss: 0.7661054730415344\n",
      "Checkpoint is saved\n",
      "step: 15844, loss: 1.3986594676971436\n",
      "step: 15849, loss: 0.8237078189849854\n",
      "step: 15854, loss: 0.9682178497314453\n",
      "step: 15859, loss: 0.9768544435501099\n",
      "Checkpoint is saved\n",
      "step: 15864, loss: 0.9880009889602661\n",
      "step: 15869, loss: 0.9779676795005798\n",
      "step: 15874, loss: 0.8145018815994263\n",
      "step: 15879, loss: 1.2852916717529297\n",
      "Checkpoint is saved\n",
      "step: 15884, loss: 0.7022466063499451\n",
      "step: 15889, loss: 0.6447737216949463\n",
      "step: 15894, loss: 0.8787894248962402\n",
      "step: 15899, loss: 1.1567153930664062\n",
      "Checkpoint is saved\n",
      "step: 15904, loss: 0.6192341446876526\n",
      "step: 15909, loss: 0.7980717420578003\n",
      "step: 15914, loss: 1.2045989036560059\n",
      "step: 15919, loss: 0.9354597926139832\n",
      "Checkpoint is saved\n",
      "step: 15924, loss: 0.6801141500473022\n",
      "step: 15929, loss: 0.8970121145248413\n",
      "step: 15934, loss: 0.5342282056808472\n",
      "step: 15939, loss: 0.8521944284439087\n",
      "Checkpoint is saved\n",
      "step: 15944, loss: 0.7804296612739563\n",
      "step: 15949, loss: 0.957594096660614\n",
      "step: 15954, loss: 0.7319669127464294\n",
      "step: 15959, loss: 0.8171565532684326\n",
      "Checkpoint is saved\n",
      "step: 15964, loss: 0.8950388431549072\n",
      "step: 15969, loss: 1.116631269454956\n",
      "step: 15974, loss: 0.7801578044891357\n",
      "step: 15979, loss: 0.6955814361572266\n",
      "Checkpoint is saved\n",
      "step: 15984, loss: 0.8990628719329834\n",
      "step: 15989, loss: 0.7205606698989868\n",
      "step: 15994, loss: 0.7112767696380615\n",
      "step: 15999, loss: 0.9425484538078308\n",
      "Checkpoint is saved\n",
      "step: 16004, loss: 0.8854568004608154\n",
      "step: 16009, loss: 0.6946914196014404\n",
      "step: 16014, loss: 0.41912567615509033\n",
      "step: 16019, loss: 0.6769583225250244\n",
      "Checkpoint is saved\n",
      "step: 16024, loss: 0.6624178886413574\n",
      "step: 16029, loss: 0.7910702228546143\n",
      "step: 16034, loss: 0.7591021060943604\n",
      "step: 16039, loss: 1.0049245357513428\n",
      "Checkpoint is saved\n",
      "step: 16044, loss: 0.8273157477378845\n",
      "step: 16049, loss: 0.4078999161720276\n",
      "step: 16054, loss: 0.7081623077392578\n",
      "step: 16059, loss: 0.575569212436676\n",
      "Checkpoint is saved\n",
      "step: 16064, loss: 0.8013908863067627\n",
      "step: 16069, loss: 0.7993160486221313\n",
      "step: 16074, loss: 1.4107143878936768\n",
      "step: 16079, loss: 0.8553590774536133\n",
      "Checkpoint is saved\n",
      "step: 16084, loss: 1.1359469890594482\n",
      "step: 16089, loss: 0.913427472114563\n",
      "step: 16094, loss: 0.8846373558044434\n",
      "step: 16099, loss: 0.6505246162414551\n",
      "Checkpoint is saved\n",
      "step: 16104, loss: 0.7167341709136963\n",
      "step: 16109, loss: 1.0414059162139893\n",
      "step: 16114, loss: 0.6762315630912781\n",
      "step: 16119, loss: 0.9579324126243591\n",
      "Checkpoint is saved\n",
      "step: 16124, loss: 0.4034930169582367\n",
      "step: 16129, loss: 1.1771389245986938\n",
      "step: 16134, loss: 1.080582857131958\n",
      "step: 16139, loss: 1.0365393161773682\n",
      "Checkpoint is saved\n",
      "step: 16144, loss: 0.92100590467453\n",
      "step: 16149, loss: 0.6467304229736328\n",
      "step: 16154, loss: 0.6877864599227905\n",
      "step: 16159, loss: 0.7654401659965515\n",
      "Checkpoint is saved\n",
      "step: 16164, loss: 0.6786363124847412\n",
      "step: 16169, loss: 1.1203007698059082\n",
      "step: 16174, loss: 0.8790676593780518\n",
      "step: 16179, loss: 0.8749879598617554\n",
      "Checkpoint is saved\n",
      "step: 16184, loss: 0.8181506395339966\n",
      "step: 16189, loss: 1.0618544816970825\n",
      "step: 16194, loss: 0.7570940256118774\n",
      "step: 16199, loss: 0.8625519275665283\n",
      "Checkpoint is saved\n",
      "step: 16204, loss: 0.9819918870925903\n",
      "step: 16209, loss: 0.7623739242553711\n",
      "step: 16214, loss: 1.1230988502502441\n",
      "step: 16219, loss: 1.0156278610229492\n",
      "Checkpoint is saved\n",
      "step: 16224, loss: 1.5926250219345093\n",
      "step: 16229, loss: 0.7320444583892822\n",
      "step: 16234, loss: 0.94028639793396\n",
      "step: 16239, loss: 0.9951471090316772\n",
      "Checkpoint is saved\n",
      "step: 16244, loss: 0.7467477321624756\n",
      "step: 16249, loss: 0.6497640609741211\n",
      "step: 16254, loss: 1.255542516708374\n",
      "step: 16259, loss: 1.030437707901001\n",
      "Checkpoint is saved\n",
      "step: 16264, loss: 1.1767621040344238\n",
      "step: 16269, loss: 0.9377826452255249\n",
      "step: 16274, loss: 1.2758395671844482\n",
      "step: 16279, loss: 0.7873495817184448\n",
      "Checkpoint is saved\n",
      "step: 16284, loss: 1.025920033454895\n",
      "step: 16289, loss: 0.9215443730354309\n",
      "step: 16294, loss: 0.8343130350112915\n",
      "step: 16299, loss: 0.6143097877502441\n",
      "Checkpoint is saved\n",
      "step: 16304, loss: 1.4303641319274902\n",
      "step: 16309, loss: 1.087707281112671\n",
      "step: 16314, loss: 0.8541889190673828\n",
      "step: 16319, loss: 0.6298637390136719\n",
      "Checkpoint is saved\n",
      "step: 16324, loss: 1.2354261875152588\n",
      "step: 16329, loss: 0.7986800670623779\n",
      "step: 16334, loss: 0.7484694719314575\n",
      "step: 16339, loss: 1.0554389953613281\n",
      "Checkpoint is saved\n",
      "step: 16344, loss: 0.681540310382843\n",
      "step: 16349, loss: 0.8323283195495605\n",
      "step: 16354, loss: 1.2849280834197998\n",
      "step: 16359, loss: 0.8256031274795532\n",
      "Checkpoint is saved\n",
      "step: 16364, loss: 0.6817104816436768\n",
      "step: 16369, loss: 0.9429042339324951\n",
      "step: 16374, loss: 0.6745022535324097\n",
      "step: 16379, loss: 0.8436586260795593\n",
      "Checkpoint is saved\n",
      "step: 16384, loss: 0.73155277967453\n",
      "step: 16389, loss: 0.6015200018882751\n",
      "step: 16394, loss: 1.1518135070800781\n",
      "step: 16399, loss: 0.9182671904563904\n",
      "Checkpoint is saved\n",
      "step: 16404, loss: 0.601660430431366\n",
      "step: 16409, loss: 0.8009561896324158\n",
      "step: 16414, loss: 1.0215942859649658\n",
      "step: 16419, loss: 0.6868124008178711\n",
      "Checkpoint is saved\n",
      "step: 16424, loss: 0.904487133026123\n",
      "step: 16429, loss: 0.7008394598960876\n",
      "step: 16434, loss: 0.8790827393531799\n",
      "step: 16439, loss: 1.1169764995574951\n",
      "Checkpoint is saved\n",
      "step: 16444, loss: 0.8911572098731995\n",
      "step: 16449, loss: 0.7740533351898193\n",
      "step: 16454, loss: 1.0676491260528564\n",
      "step: 16459, loss: 0.8104082345962524\n",
      "Checkpoint is saved\n",
      "step: 16464, loss: 1.1033269166946411\n",
      "step: 16469, loss: 1.095025897026062\n",
      "step: 16474, loss: 0.8165398836135864\n",
      "step: 16479, loss: 1.2450752258300781\n",
      "Checkpoint is saved\n",
      "step: 16484, loss: 1.273943305015564\n",
      "step: 16489, loss: 0.5905652046203613\n",
      "step: 16494, loss: 0.8458103537559509\n",
      "step: 16499, loss: 0.8157540559768677\n",
      "Checkpoint is saved\n",
      "step: 16504, loss: 0.7186766862869263\n",
      "step: 16509, loss: 1.3110625743865967\n",
      "step: 16514, loss: 1.391304850578308\n",
      "step: 16519, loss: 0.7309602499008179\n",
      "Checkpoint is saved\n",
      "step: 16524, loss: 1.2986271381378174\n",
      "step: 16529, loss: 0.5660756826400757\n",
      "step: 16534, loss: 0.6960029602050781\n",
      "step: 16539, loss: 0.9130334854125977\n",
      "Checkpoint is saved\n",
      "step: 16544, loss: 0.7243748903274536\n",
      "step: 16549, loss: 0.6189526915550232\n",
      "step: 16554, loss: 0.6020735502243042\n",
      "step: 16559, loss: 0.3456704020500183\n",
      "Checkpoint is saved\n",
      "step: 16564, loss: 1.1323630809783936\n",
      "step: 16569, loss: 0.8000848293304443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16574, loss: 0.9195646643638611\n",
      "step: 16579, loss: 0.4947715401649475\n",
      "Checkpoint is saved\n",
      "step: 16584, loss: 0.9557505249977112\n",
      "step: 16589, loss: 1.3073620796203613\n",
      "step: 16594, loss: 0.8180909752845764\n",
      "step: 16599, loss: 1.0238887071609497\n",
      "Checkpoint is saved\n",
      "step: 16604, loss: 0.8164774179458618\n",
      "step: 16609, loss: 0.5736369490623474\n",
      "step: 16614, loss: 1.1192104816436768\n",
      "step: 16619, loss: 0.9626692533493042\n",
      "Checkpoint is saved\n",
      "step: 16624, loss: 0.801546573638916\n",
      "step: 16629, loss: 1.0272771120071411\n",
      "step: 16634, loss: 0.9885581135749817\n",
      "step: 16639, loss: 0.9503051042556763\n",
      "Checkpoint is saved\n",
      "step: 16644, loss: 0.8323407173156738\n",
      "step: 16649, loss: 0.7148851156234741\n",
      "step: 16654, loss: 0.8592113256454468\n",
      "step: 16659, loss: 0.9049516320228577\n",
      "Checkpoint is saved\n",
      "step: 16664, loss: 1.0250928401947021\n",
      "step: 16669, loss: 1.27864408493042\n",
      "step: 16674, loss: 0.8857086896896362\n",
      "step: 16679, loss: 0.827054500579834\n",
      "Checkpoint is saved\n",
      "step: 16684, loss: 0.6470524668693542\n",
      "step: 16689, loss: 0.696481466293335\n",
      "step: 16694, loss: 0.7738025784492493\n",
      "step: 16699, loss: 0.5564981698989868\n",
      "Checkpoint is saved\n",
      "step: 16704, loss: 0.6005679368972778\n",
      "step: 16709, loss: 0.5324989557266235\n",
      "step: 16714, loss: 0.40031319856643677\n",
      "step: 16719, loss: 0.529116153717041\n",
      "Checkpoint is saved\n",
      "step: 16724, loss: 0.6114416718482971\n",
      "step: 16729, loss: 0.3857315182685852\n",
      "step: 16734, loss: 0.8444857001304626\n",
      "step: 16739, loss: 0.8625738620758057\n",
      "Checkpoint is saved\n",
      "step: 16744, loss: 0.7757236957550049\n",
      "step: 16749, loss: 0.6638951301574707\n",
      "step: 16754, loss: 0.7939150929450989\n",
      "step: 16759, loss: 0.621381402015686\n",
      "Checkpoint is saved\n",
      "step: 16764, loss: 0.5802721977233887\n",
      "step: 16769, loss: 0.49537211656570435\n",
      "step: 16774, loss: 0.556784451007843\n",
      "step: 16779, loss: 0.6977548003196716\n",
      "Checkpoint is saved\n",
      "step: 16784, loss: 1.38667631149292\n",
      "step: 16789, loss: 0.9715563654899597\n",
      "step: 16794, loss: 1.0968279838562012\n",
      "step: 16799, loss: 0.8343293070793152\n",
      "Checkpoint is saved\n",
      "step: 16804, loss: 0.6427459716796875\n",
      "step: 16809, loss: 0.9196328520774841\n",
      "step: 16814, loss: 0.5673644542694092\n",
      "step: 16819, loss: 0.9764341711997986\n",
      "Checkpoint is saved\n",
      "step: 16824, loss: 0.9395134449005127\n",
      "step: 16829, loss: 1.279800534248352\n",
      "step: 16834, loss: 0.9662531018257141\n",
      "step: 16839, loss: 1.0009074211120605\n",
      "Checkpoint is saved\n",
      "step: 16844, loss: 0.5446449518203735\n",
      "step: 16849, loss: 1.013964295387268\n",
      "step: 16854, loss: 1.4723414182662964\n",
      "step: 16859, loss: 0.559290885925293\n",
      "Checkpoint is saved\n",
      "step: 16864, loss: 0.9468245506286621\n",
      "step: 16869, loss: 0.7310349941253662\n",
      "step: 16874, loss: 0.8088184595108032\n",
      "step: 16879, loss: 0.7374416589736938\n",
      "Checkpoint is saved\n",
      "step: 16884, loss: 1.005460262298584\n",
      "step: 16889, loss: 0.5012635588645935\n",
      "step: 16894, loss: 0.8514859676361084\n",
      "step: 16899, loss: 0.9990031719207764\n",
      "Checkpoint is saved\n",
      "step: 16904, loss: 0.9775035381317139\n",
      "step: 16909, loss: 0.9284765720367432\n",
      "step: 16914, loss: 1.2102291584014893\n",
      "step: 16919, loss: 0.6599431037902832\n",
      "Checkpoint is saved\n",
      "step: 16924, loss: 1.2660915851593018\n",
      "step: 16929, loss: 0.9139935970306396\n",
      "step: 16934, loss: 0.855932354927063\n",
      "step: 16939, loss: 1.0354245901107788\n",
      "Checkpoint is saved\n",
      "step: 16944, loss: 0.672615647315979\n",
      "step: 16949, loss: 1.0107269287109375\n",
      "step: 16954, loss: 0.6846216320991516\n",
      "step: 16959, loss: 1.0117645263671875\n",
      "Checkpoint is saved\n",
      "step: 16964, loss: 0.9075775742530823\n",
      "step: 16969, loss: 0.7947536110877991\n",
      "step: 16974, loss: 0.9532139301300049\n",
      "step: 16979, loss: 0.6322441101074219\n",
      "Checkpoint is saved\n",
      "step: 16984, loss: 1.035201072692871\n",
      "step: 16989, loss: 0.9303023815155029\n",
      "step: 16994, loss: 1.2557094097137451\n",
      "step: 16999, loss: 1.1972870826721191\n",
      "Checkpoint is saved\n",
      "step: 17004, loss: 0.6458820104598999\n",
      "step: 17009, loss: 0.8710987567901611\n",
      "step: 17014, loss: 1.4358028173446655\n",
      "step: 17019, loss: 0.8174954652786255\n",
      "Checkpoint is saved\n",
      "step: 17024, loss: 0.7320736646652222\n",
      "step: 17029, loss: 0.48854437470436096\n",
      "step: 17034, loss: 0.9481033682823181\n",
      "step: 17039, loss: 0.5056976079940796\n",
      "Checkpoint is saved\n",
      "step: 17044, loss: 0.7941070795059204\n",
      "step: 17049, loss: 0.7391735315322876\n",
      "step: 17054, loss: 0.7344748973846436\n",
      "step: 17059, loss: 0.9163140058517456\n",
      "Checkpoint is saved\n",
      "step: 17064, loss: 0.5090054273605347\n",
      "step: 17069, loss: 0.7512319087982178\n",
      "step: 17074, loss: 0.7223373651504517\n",
      "step: 17079, loss: 0.9659298062324524\n",
      "Checkpoint is saved\n",
      "step: 17084, loss: 0.6800289750099182\n",
      "step: 17089, loss: 0.49851036071777344\n",
      "step: 17094, loss: 0.8176470398902893\n",
      "step: 17099, loss: 0.8112141489982605\n",
      "Checkpoint is saved\n",
      "step: 17104, loss: 0.5783672332763672\n",
      "step: 17109, loss: 0.7778468728065491\n",
      "step: 17114, loss: 0.9075275659561157\n",
      "step: 17119, loss: 0.7060402035713196\n",
      "Checkpoint is saved\n",
      "step: 17124, loss: 0.4768854081630707\n",
      "step: 17129, loss: 0.8115293383598328\n",
      "step: 17134, loss: 0.5699198246002197\n",
      "step: 17139, loss: 0.6266320943832397\n",
      "Checkpoint is saved\n",
      "step: 17144, loss: 1.208577036857605\n",
      "step: 17149, loss: 0.5490099787712097\n",
      "step: 17154, loss: 0.9169089794158936\n",
      "step: 17159, loss: 0.6039156913757324\n",
      "Checkpoint is saved\n",
      "step: 17164, loss: 0.7532680630683899\n",
      "step: 17169, loss: 1.209201693534851\n",
      "step: 17174, loss: 0.6694865226745605\n",
      "step: 17179, loss: 1.2279105186462402\n",
      "Checkpoint is saved\n",
      "step: 17184, loss: 0.697126030921936\n",
      "step: 17189, loss: 0.9859440326690674\n",
      "step: 17194, loss: 1.0188030004501343\n",
      "step: 17199, loss: 0.5088880658149719\n",
      "Checkpoint is saved\n",
      "step: 17204, loss: 0.6879832148551941\n",
      "step: 17209, loss: 0.9936548471450806\n",
      "step: 17214, loss: 0.9391683340072632\n",
      "step: 17219, loss: 0.8499513268470764\n",
      "Checkpoint is saved\n",
      "step: 17224, loss: 0.7975553274154663\n",
      "step: 17229, loss: 0.8814027905464172\n",
      "step: 17234, loss: 1.2788304090499878\n",
      "step: 17239, loss: 1.2732181549072266\n",
      "Checkpoint is saved\n",
      "step: 17244, loss: 0.7602633833885193\n",
      "step: 17249, loss: 0.715505838394165\n",
      "step: 17254, loss: 0.6217329502105713\n",
      "step: 17259, loss: 0.8425893187522888\n",
      "Checkpoint is saved\n",
      "step: 17264, loss: 0.7840012311935425\n",
      "step: 17269, loss: 0.9702935814857483\n",
      "step: 17274, loss: 0.8239409327507019\n",
      "step: 17279, loss: 0.6955928802490234\n",
      "Checkpoint is saved\n",
      "step: 17284, loss: 0.4817126393318176\n",
      "step: 17289, loss: 1.027055263519287\n",
      "step: 17294, loss: 0.802609920501709\n",
      "step: 17299, loss: 0.7943462133407593\n",
      "Checkpoint is saved\n",
      "step: 17304, loss: 0.9456162452697754\n",
      "step: 17309, loss: 0.8222050666809082\n",
      "step: 17314, loss: 0.5992336273193359\n",
      "step: 17319, loss: 1.2768921852111816\n",
      "Checkpoint is saved\n",
      "step: 17324, loss: 0.8671671152114868\n",
      "step: 17329, loss: 1.0838391780853271\n",
      "step: 17334, loss: 1.1436669826507568\n",
      "step: 17339, loss: 1.0955467224121094\n",
      "Checkpoint is saved\n",
      "step: 17344, loss: 0.9908503890037537\n",
      "step: 17349, loss: 0.8634387254714966\n",
      "step: 17354, loss: 0.972942590713501\n",
      "step: 17359, loss: 0.6785043478012085\n",
      "Checkpoint is saved\n",
      "step: 17364, loss: 0.8759499788284302\n",
      "step: 17369, loss: 1.106391191482544\n",
      "step: 17374, loss: 0.8695909380912781\n",
      "step: 17379, loss: 0.7720234990119934\n",
      "Checkpoint is saved\n",
      "step: 17384, loss: 0.9271584749221802\n",
      "step: 17389, loss: 1.1509110927581787\n",
      "step: 17394, loss: 1.0148887634277344\n",
      "step: 17399, loss: 0.6694449186325073\n",
      "Checkpoint is saved\n",
      "step: 17404, loss: 0.7388518452644348\n",
      "step: 17409, loss: 0.7814862132072449\n",
      "step: 17414, loss: 0.6807100772857666\n",
      "step: 17419, loss: 1.041856050491333\n",
      "Checkpoint is saved\n",
      "step: 17424, loss: 0.7927600145339966\n",
      "step: 17429, loss: 0.6644324660301208\n",
      "step: 17434, loss: 1.1842750310897827\n",
      "step: 17439, loss: 0.7075636386871338\n",
      "Checkpoint is saved\n",
      "step: 17444, loss: 1.0572097301483154\n",
      "step: 17449, loss: 0.41577625274658203\n",
      "step: 17454, loss: 0.9233421683311462\n",
      "step: 17459, loss: 0.8214656114578247\n",
      "Checkpoint is saved\n",
      "step: 17464, loss: 0.4347842335700989\n",
      "step: 17469, loss: 1.167290449142456\n",
      "step: 17474, loss: 0.7058687210083008\n",
      "step: 17479, loss: 0.8179575204849243\n",
      "Checkpoint is saved\n",
      "step: 17484, loss: 0.8941885828971863\n",
      "step: 17489, loss: 0.8491421937942505\n",
      "step: 17494, loss: 0.48508045077323914\n",
      "step: 17499, loss: 0.6894863843917847\n",
      "Checkpoint is saved\n",
      "step: 17504, loss: 1.2028813362121582\n",
      "step: 17509, loss: 0.883579432964325\n",
      "step: 17514, loss: 0.9950010776519775\n",
      "step: 17519, loss: 0.7522599697113037\n",
      "Checkpoint is saved\n",
      "step: 17524, loss: 0.9048436880111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17529, loss: 0.5564239025115967\n",
      "step: 17534, loss: 1.0864888429641724\n",
      "step: 17539, loss: 1.1981308460235596\n",
      "Checkpoint is saved\n",
      "step: 17544, loss: 0.7818748354911804\n",
      "step: 17549, loss: 0.8157140016555786\n",
      "step: 17554, loss: 1.1281359195709229\n",
      "step: 17559, loss: 0.731758713722229\n",
      "Checkpoint is saved\n",
      "step: 17564, loss: 1.3128249645233154\n",
      "step: 17569, loss: 0.714983344078064\n",
      "step: 17574, loss: 0.37687262892723083\n",
      "step: 17579, loss: 0.5707132816314697\n",
      "Checkpoint is saved\n",
      "step: 17584, loss: 0.9951068758964539\n",
      "step: 17589, loss: 0.8952689170837402\n",
      "step: 17594, loss: 0.8899741768836975\n",
      "step: 17599, loss: 0.48262497782707214\n",
      "Checkpoint is saved\n",
      "step: 17604, loss: 0.6194101572036743\n",
      "step: 17609, loss: 0.8741213083267212\n",
      "step: 17614, loss: 0.8897741436958313\n",
      "step: 17619, loss: 0.46569350361824036\n",
      "Checkpoint is saved\n",
      "step: 17624, loss: 0.8012378215789795\n",
      "step: 17629, loss: 0.6660382747650146\n",
      "step: 17634, loss: 1.0473803281784058\n",
      "step: 17639, loss: 0.7519859075546265\n",
      "Checkpoint is saved\n",
      "step: 17644, loss: 1.9078693389892578\n",
      "step: 17649, loss: 0.8592249751091003\n",
      "step: 17654, loss: 0.8370912075042725\n",
      "step: 17659, loss: 0.803692102432251\n",
      "Checkpoint is saved\n",
      "step: 17664, loss: 0.6636292934417725\n",
      "step: 17669, loss: 0.7934435606002808\n",
      "step: 17674, loss: 0.4718475937843323\n",
      "step: 17679, loss: 1.1164501905441284\n",
      "Checkpoint is saved\n",
      "step: 17684, loss: 0.8892462849617004\n",
      "step: 17689, loss: 0.8885787725448608\n",
      "step: 17694, loss: 0.9986408948898315\n",
      "step: 17699, loss: 0.6241121888160706\n",
      "Checkpoint is saved\n",
      "step: 17704, loss: 0.9017015695571899\n",
      "step: 17709, loss: 0.6835553646087646\n",
      "step: 17714, loss: 0.9015071988105774\n",
      "step: 17719, loss: 0.40403613448143005\n",
      "Checkpoint is saved\n",
      "step: 17724, loss: 1.2577528953552246\n",
      "step: 17729, loss: 0.8088808655738831\n",
      "step: 17734, loss: 0.8873748779296875\n",
      "step: 17739, loss: 1.0653843879699707\n",
      "Checkpoint is saved\n",
      "step: 17744, loss: 0.6097558736801147\n",
      "step: 17749, loss: 0.7132049798965454\n",
      "step: 17754, loss: 0.6417391300201416\n",
      "step: 17759, loss: 0.5757832527160645\n",
      "Checkpoint is saved\n",
      "step: 17764, loss: 0.7256017923355103\n",
      "step: 17769, loss: 1.2563263177871704\n",
      "step: 17774, loss: 0.6249471306800842\n",
      "step: 17779, loss: 1.0630663633346558\n",
      "Checkpoint is saved\n",
      "step: 17784, loss: 1.3420610427856445\n",
      "step: 17789, loss: 0.6762608289718628\n",
      "step: 17794, loss: 0.9231710433959961\n",
      "step: 17799, loss: 0.6032625436782837\n",
      "Checkpoint is saved\n",
      "step: 17804, loss: 0.6711063385009766\n",
      "step: 17809, loss: 0.6360957622528076\n",
      "step: 17814, loss: 1.0517988204956055\n",
      "step: 17819, loss: 0.9884408712387085\n",
      "Checkpoint is saved\n",
      "step: 17824, loss: 0.5424150228500366\n",
      "step: 17829, loss: 0.8951370716094971\n",
      "step: 17834, loss: 0.9542770385742188\n",
      "step: 17839, loss: 0.6441912651062012\n",
      "Checkpoint is saved\n",
      "step: 17844, loss: 0.785172700881958\n",
      "step: 17849, loss: 1.2468626499176025\n",
      "step: 17854, loss: 0.6967265009880066\n",
      "step: 17859, loss: 0.554989218711853\n",
      "Checkpoint is saved\n",
      "step: 17864, loss: 0.5445526242256165\n",
      "step: 17869, loss: 1.2284033298492432\n",
      "step: 17874, loss: 0.6712108850479126\n",
      "step: 17879, loss: 1.1783417463302612\n",
      "Checkpoint is saved\n",
      "step: 17884, loss: 0.7745559811592102\n",
      "step: 17889, loss: 1.0477347373962402\n",
      "step: 17894, loss: 0.5858272314071655\n",
      "step: 17899, loss: 0.6076734066009521\n",
      "Checkpoint is saved\n",
      "step: 17904, loss: 1.3184051513671875\n",
      "step: 17909, loss: 0.6409838199615479\n",
      "step: 17914, loss: 0.8519766330718994\n",
      "step: 17919, loss: 1.008743405342102\n",
      "Checkpoint is saved\n",
      "step: 17924, loss: 1.1659990549087524\n",
      "step: 17929, loss: 0.7078810334205627\n",
      "step: 17934, loss: 0.955795407295227\n",
      "step: 17939, loss: 0.6471832394599915\n",
      "Checkpoint is saved\n",
      "step: 17944, loss: 0.79780113697052\n",
      "step: 17949, loss: 0.9959286451339722\n",
      "step: 17954, loss: 0.9201074838638306\n",
      "step: 17959, loss: 1.129982829093933\n",
      "Checkpoint is saved\n",
      "step: 17964, loss: 0.7433731555938721\n",
      "step: 17969, loss: 0.7205629944801331\n",
      "step: 17974, loss: 1.0947076082229614\n",
      "step: 17979, loss: 1.1580772399902344\n",
      "Checkpoint is saved\n",
      "step: 17984, loss: 1.1901131868362427\n",
      "step: 17989, loss: 0.6868704557418823\n",
      "step: 17994, loss: 1.041811227798462\n",
      "step: 17999, loss: 0.8993335962295532\n",
      "Checkpoint is saved\n",
      "step: 18004, loss: 0.8517813682556152\n",
      "step: 18009, loss: 1.1113656759262085\n",
      "step: 18014, loss: 0.8188341856002808\n",
      "step: 18019, loss: 0.9330926537513733\n",
      "Checkpoint is saved\n",
      "step: 18024, loss: 0.7831510305404663\n",
      "step: 18029, loss: 1.0007973909378052\n",
      "step: 18034, loss: 0.8871047496795654\n",
      "step: 18039, loss: 0.5967432260513306\n",
      "Checkpoint is saved\n",
      "step: 18044, loss: 0.5689687728881836\n",
      "step: 18049, loss: 0.8527553081512451\n",
      "step: 18054, loss: 1.012547254562378\n",
      "step: 18059, loss: 0.6465276479721069\n",
      "Checkpoint is saved\n",
      "step: 18064, loss: 0.849543035030365\n",
      "step: 18069, loss: 1.1741704940795898\n",
      "step: 18074, loss: 0.520743727684021\n",
      "step: 18079, loss: 1.020662784576416\n",
      "Checkpoint is saved\n",
      "step: 18084, loss: 0.6085056662559509\n",
      "step: 18089, loss: 0.7128896713256836\n",
      "step: 18094, loss: 0.7855451703071594\n",
      "step: 18099, loss: 0.2746230363845825\n",
      "Checkpoint is saved\n",
      "step: 18104, loss: 1.075478196144104\n",
      "step: 18109, loss: 0.6992529034614563\n",
      "step: 18114, loss: 0.9555414319038391\n",
      "step: 18119, loss: 0.4960562586784363\n",
      "Checkpoint is saved\n",
      "step: 18124, loss: 1.1791977882385254\n",
      "step: 18129, loss: 0.958764374256134\n",
      "step: 18134, loss: 1.0420663356781006\n",
      "step: 18139, loss: 0.9308390021324158\n",
      "Checkpoint is saved\n",
      "step: 18144, loss: 0.9819162487983704\n",
      "step: 18149, loss: 0.7811312675476074\n",
      "step: 18154, loss: 0.9383683204650879\n",
      "step: 18159, loss: 0.8762811422348022\n",
      "Checkpoint is saved\n",
      "step: 18164, loss: 0.7741868495941162\n",
      "step: 18169, loss: 0.644376277923584\n",
      "step: 18174, loss: 1.4955389499664307\n",
      "step: 18179, loss: 0.6262763738632202\n",
      "Checkpoint is saved\n",
      "step: 18184, loss: 0.7806174755096436\n",
      "step: 18189, loss: 0.4653354287147522\n",
      "step: 18194, loss: 0.8993284106254578\n",
      "step: 18199, loss: 0.5455454587936401\n",
      "Checkpoint is saved\n",
      "step: 18204, loss: 1.346158742904663\n",
      "step: 18209, loss: 0.5235780477523804\n",
      "step: 18214, loss: 0.5509086847305298\n",
      "step: 18219, loss: 0.9999324083328247\n",
      "Checkpoint is saved\n",
      "step: 18224, loss: 0.9635988473892212\n",
      "step: 18229, loss: 1.0031824111938477\n",
      "step: 18234, loss: 1.17598295211792\n",
      "step: 18239, loss: 0.8255521059036255\n",
      "Checkpoint is saved\n",
      "step: 18244, loss: 1.005225658416748\n",
      "step: 18249, loss: 1.109812617301941\n",
      "step: 18254, loss: 0.6959846019744873\n",
      "step: 18259, loss: 0.6747450828552246\n",
      "Checkpoint is saved\n",
      "step: 18264, loss: 0.8638472557067871\n",
      "step: 18269, loss: 0.648141622543335\n",
      "step: 18274, loss: 1.0071016550064087\n",
      "step: 18279, loss: 0.5206575989723206\n",
      "Checkpoint is saved\n",
      "step: 18284, loss: 0.877872109413147\n",
      "step: 18289, loss: 0.7094555497169495\n",
      "step: 18294, loss: 0.947279691696167\n",
      "step: 18299, loss: 0.6940745115280151\n",
      "Checkpoint is saved\n",
      "step: 18304, loss: 0.9927167892456055\n",
      "step: 18309, loss: 0.7068564891815186\n",
      "step: 18314, loss: 0.7043992280960083\n",
      "step: 18319, loss: 0.5971398949623108\n",
      "Checkpoint is saved\n",
      "step: 18324, loss: 0.8015617728233337\n",
      "step: 18329, loss: 0.6084082126617432\n",
      "step: 18334, loss: 0.7033025622367859\n",
      "step: 18339, loss: 0.7520277500152588\n",
      "Checkpoint is saved\n",
      "step: 18344, loss: 0.6846354007720947\n",
      "step: 18349, loss: 0.7746988534927368\n",
      "step: 18354, loss: 1.024965763092041\n",
      "step: 18359, loss: 0.9083131551742554\n",
      "Checkpoint is saved\n",
      "step: 18364, loss: 0.8214676976203918\n",
      "step: 18369, loss: 1.0071961879730225\n",
      "step: 18374, loss: 0.8190860152244568\n",
      "step: 18379, loss: 0.9049707651138306\n",
      "Checkpoint is saved\n",
      "step: 18384, loss: 0.7928522825241089\n",
      "step: 18389, loss: 1.2117524147033691\n",
      "step: 18394, loss: 1.1044888496398926\n",
      "step: 18399, loss: 1.1816227436065674\n",
      "Checkpoint is saved\n",
      "step: 18404, loss: 0.9402492046356201\n",
      "step: 18409, loss: 0.654574990272522\n",
      "step: 18414, loss: 1.2137436866760254\n",
      "step: 18419, loss: 0.47965335845947266\n",
      "Checkpoint is saved\n",
      "step: 18424, loss: 0.9018492698669434\n",
      "step: 18429, loss: 0.7888776063919067\n",
      "step: 18434, loss: 1.0322692394256592\n",
      "step: 18439, loss: 1.0849064588546753\n",
      "Checkpoint is saved\n",
      "step: 18444, loss: 0.7238109111785889\n",
      "step: 18449, loss: 1.0180528163909912\n",
      "step: 18454, loss: 0.5450217723846436\n",
      "step: 18459, loss: 1.14414381980896\n",
      "Checkpoint is saved\n",
      "step: 18464, loss: 1.1231456995010376\n",
      "step: 18469, loss: 0.8388099074363708\n",
      "step: 18474, loss: 0.7889382839202881\n",
      "step: 18479, loss: 0.8251174092292786\n",
      "Checkpoint is saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18484, loss: 1.254422664642334\n",
      "step: 18489, loss: 1.1821067333221436\n",
      "step: 18494, loss: 0.4801452159881592\n",
      "step: 18499, loss: 0.9158874750137329\n",
      "Checkpoint is saved\n",
      "step: 18504, loss: 0.8065182566642761\n",
      "step: 18509, loss: 0.7782660722732544\n",
      "step: 18514, loss: 0.7126179933547974\n",
      "step: 18519, loss: 0.8997440338134766\n",
      "Checkpoint is saved\n",
      "step: 18524, loss: 0.5480421781539917\n",
      "step: 18529, loss: 0.9407538771629333\n",
      "step: 18534, loss: 0.5773470401763916\n",
      "step: 18539, loss: 0.6310689449310303\n",
      "Checkpoint is saved\n",
      "step: 18544, loss: 0.7966049909591675\n",
      "step: 18549, loss: 0.4808625876903534\n",
      "step: 18554, loss: 1.1617915630340576\n",
      "step: 18559, loss: 1.025353193283081\n",
      "Checkpoint is saved\n",
      "step: 18564, loss: 0.6853886842727661\n",
      "step: 18569, loss: 0.4999716579914093\n",
      "step: 18574, loss: 0.6322155594825745\n",
      "step: 18579, loss: 0.8705796003341675\n",
      "Checkpoint is saved\n",
      "step: 18584, loss: 0.5912383794784546\n",
      "step: 18589, loss: 0.7145078182220459\n",
      "step: 18594, loss: 0.6421555280685425\n",
      "step: 18599, loss: 1.3467919826507568\n",
      "Checkpoint is saved\n",
      "step: 18604, loss: 1.1755248308181763\n",
      "step: 18609, loss: 0.7958552241325378\n",
      "step: 18614, loss: 1.286306619644165\n",
      "step: 18619, loss: 0.9809735417366028\n",
      "Checkpoint is saved\n",
      "step: 18624, loss: 0.961250364780426\n",
      "step: 18629, loss: 0.8027623891830444\n",
      "step: 18634, loss: 0.9873595237731934\n",
      "step: 18639, loss: 0.7448419332504272\n",
      "Checkpoint is saved\n",
      "step: 18644, loss: 0.5963484644889832\n",
      "step: 18649, loss: 0.4694724678993225\n",
      "step: 18654, loss: 0.7200183868408203\n",
      "step: 18659, loss: 0.8307939767837524\n",
      "Checkpoint is saved\n",
      "step: 18664, loss: 0.5223058462142944\n",
      "step: 18669, loss: 0.7550112009048462\n",
      "step: 18674, loss: 0.8145837187767029\n",
      "step: 18679, loss: 0.6434354782104492\n",
      "Checkpoint is saved\n",
      "step: 18684, loss: 0.813319981098175\n",
      "step: 18689, loss: 0.7705883383750916\n",
      "step: 18694, loss: 1.313803791999817\n",
      "step: 18699, loss: 0.6115702986717224\n",
      "Checkpoint is saved\n",
      "step: 18704, loss: 0.7739514112472534\n",
      "step: 18709, loss: 0.7452743053436279\n",
      "step: 18714, loss: 0.822130560874939\n",
      "step: 18719, loss: 0.6167148351669312\n",
      "Checkpoint is saved\n",
      "step: 18724, loss: 0.49237293004989624\n",
      "step: 18729, loss: 0.4292564392089844\n",
      "step: 18734, loss: 0.828294038772583\n",
      "step: 18739, loss: 0.8497703075408936\n",
      "Checkpoint is saved\n",
      "step: 18744, loss: 1.1180262565612793\n",
      "step: 18749, loss: 0.8457895517349243\n",
      "step: 18754, loss: 0.875392735004425\n",
      "step: 18759, loss: 0.970434308052063\n",
      "Checkpoint is saved\n",
      "step: 18764, loss: 1.0271389484405518\n",
      "step: 18769, loss: 0.8614353537559509\n",
      "step: 18774, loss: 0.9255371689796448\n",
      "step: 18779, loss: 1.0824781656265259\n",
      "Checkpoint is saved\n",
      "step: 18784, loss: 1.0181090831756592\n",
      "step: 18789, loss: 1.2097853422164917\n",
      "step: 18794, loss: 1.139211893081665\n",
      "step: 18799, loss: 0.9801632165908813\n",
      "Checkpoint is saved\n",
      "step: 18804, loss: 0.3272261917591095\n",
      "step: 18809, loss: 0.8568699955940247\n",
      "step: 18814, loss: 0.9960249066352844\n",
      "step: 18819, loss: 0.6476408243179321\n",
      "Checkpoint is saved\n",
      "step: 18824, loss: 1.033921480178833\n",
      "step: 18829, loss: 0.5775631070137024\n",
      "step: 18834, loss: 0.6817504167556763\n",
      "step: 18839, loss: 1.0638322830200195\n",
      "Checkpoint is saved\n",
      "step: 18844, loss: 0.7640610933303833\n",
      "step: 18849, loss: 1.0070924758911133\n",
      "step: 18854, loss: 0.6081444621086121\n",
      "step: 18859, loss: 1.0710145235061646\n",
      "Checkpoint is saved\n",
      "step: 18864, loss: 1.119014024734497\n",
      "step: 18869, loss: 0.6805847883224487\n",
      "step: 18874, loss: 0.7999739646911621\n",
      "step: 18879, loss: 0.6685729622840881\n",
      "Checkpoint is saved\n",
      "step: 18884, loss: 0.5983791351318359\n",
      "step: 18889, loss: 1.2024306058883667\n",
      "step: 18894, loss: 0.5801976323127747\n",
      "step: 18899, loss: 0.4937499761581421\n",
      "Checkpoint is saved\n",
      "step: 18904, loss: 0.43268001079559326\n",
      "step: 18909, loss: 1.021195411682129\n",
      "step: 18914, loss: 1.0284407138824463\n",
      "step: 18919, loss: 0.9977099895477295\n",
      "Checkpoint is saved\n",
      "step: 18924, loss: 0.5978914499282837\n",
      "step: 18929, loss: 0.4989316463470459\n",
      "step: 18934, loss: 1.029735803604126\n",
      "step: 18939, loss: 1.0248699188232422\n",
      "Checkpoint is saved\n",
      "step: 18944, loss: 0.921656608581543\n",
      "step: 18949, loss: 0.7100347280502319\n",
      "step: 18954, loss: 0.6562014818191528\n",
      "step: 18959, loss: 0.766060471534729\n",
      "Checkpoint is saved\n",
      "step: 18964, loss: 0.7843301296234131\n",
      "step: 18969, loss: 0.6212655305862427\n",
      "step: 18974, loss: 0.5593971014022827\n",
      "step: 18979, loss: 0.785088062286377\n",
      "Checkpoint is saved\n",
      "step: 18984, loss: 0.944259762763977\n",
      "step: 18989, loss: 0.9343615174293518\n",
      "step: 18994, loss: 0.7994778752326965\n",
      "step: 18999, loss: 0.8634037971496582\n",
      "Checkpoint is saved\n",
      "step: 19004, loss: 1.0988620519638062\n",
      "step: 19009, loss: 0.6198998093605042\n",
      "step: 19014, loss: 0.6998030543327332\n",
      "step: 19019, loss: 0.7578017115592957\n",
      "Checkpoint is saved\n",
      "step: 19024, loss: 0.9850121736526489\n",
      "step: 19029, loss: 0.7819234132766724\n",
      "step: 19034, loss: 0.6250028610229492\n",
      "step: 19039, loss: 1.055116057395935\n",
      "Checkpoint is saved\n",
      "step: 19044, loss: 0.7588748335838318\n",
      "step: 19049, loss: 0.4279032051563263\n",
      "step: 19054, loss: 0.5222992300987244\n",
      "step: 19059, loss: 0.7022969722747803\n",
      "Checkpoint is saved\n",
      "step: 19064, loss: 1.0957276821136475\n",
      "step: 19069, loss: 0.9490132331848145\n",
      "step: 19074, loss: 0.5493342876434326\n",
      "step: 19079, loss: 0.7455660700798035\n",
      "Checkpoint is saved\n",
      "step: 19084, loss: 0.5753057599067688\n",
      "step: 19089, loss: 0.7947278022766113\n",
      "step: 19094, loss: 0.5130980014801025\n",
      "step: 19099, loss: 1.2714762687683105\n",
      "Checkpoint is saved\n",
      "step: 19104, loss: 0.8596445322036743\n",
      "step: 19109, loss: 0.8932696580886841\n",
      "step: 19114, loss: 0.6219655275344849\n",
      "step: 19119, loss: 0.8189464807510376\n",
      "Checkpoint is saved\n",
      "step: 19124, loss: 0.8304680585861206\n",
      "step: 19129, loss: 0.7944610118865967\n",
      "step: 19134, loss: 0.5877388119697571\n",
      "step: 19139, loss: 0.6988929510116577\n",
      "Checkpoint is saved\n",
      "step: 19144, loss: 1.2009296417236328\n",
      "step: 19149, loss: 0.933380126953125\n",
      "step: 19154, loss: 1.1089729070663452\n",
      "step: 19159, loss: 0.9960755109786987\n",
      "Checkpoint is saved\n",
      "step: 19164, loss: 0.5798219442367554\n",
      "step: 19169, loss: 0.5471394658088684\n",
      "step: 19174, loss: 1.445989966392517\n",
      "step: 19179, loss: 0.8353340029716492\n",
      "Checkpoint is saved\n",
      "step: 19184, loss: 0.8378150463104248\n",
      "step: 19189, loss: 0.81209397315979\n",
      "step: 19194, loss: 0.8478492498397827\n",
      "step: 19199, loss: 1.1912262439727783\n",
      "Checkpoint is saved\n",
      "step: 19204, loss: 0.8609058856964111\n",
      "step: 19209, loss: 0.9414387941360474\n",
      "step: 19214, loss: 0.6846343278884888\n",
      "step: 19219, loss: 1.017099142074585\n",
      "Checkpoint is saved\n",
      "step: 19224, loss: 1.046883225440979\n",
      "step: 19229, loss: 0.39713072776794434\n",
      "step: 19234, loss: 0.7786877155303955\n",
      "step: 19239, loss: 1.3185490369796753\n",
      "Checkpoint is saved\n",
      "step: 19244, loss: 1.361108660697937\n",
      "step: 19249, loss: 0.9858787059783936\n",
      "step: 19254, loss: 0.6159592866897583\n",
      "step: 19259, loss: 0.9048622846603394\n",
      "Checkpoint is saved\n",
      "step: 19264, loss: 0.641294002532959\n",
      "step: 19269, loss: 0.6212916374206543\n",
      "step: 19274, loss: 0.7154840230941772\n",
      "step: 19279, loss: 0.38168278336524963\n",
      "Checkpoint is saved\n",
      "step: 19284, loss: 0.35740000009536743\n",
      "step: 19289, loss: 0.8894373774528503\n",
      "step: 19294, loss: 0.9107168316841125\n",
      "step: 19299, loss: 0.6322352290153503\n",
      "Checkpoint is saved\n",
      "step: 19304, loss: 1.38142991065979\n",
      "step: 19309, loss: 0.8109301924705505\n",
      "step: 19314, loss: 0.58286452293396\n",
      "step: 19319, loss: 0.6817286014556885\n",
      "Checkpoint is saved\n",
      "step: 19324, loss: 0.6549631357192993\n",
      "step: 19329, loss: 0.6963975429534912\n",
      "step: 19334, loss: 0.4726310968399048\n",
      "step: 19339, loss: 0.6613852977752686\n",
      "Checkpoint is saved\n",
      "step: 19344, loss: 0.5363637804985046\n",
      "step: 19349, loss: 0.7612819075584412\n",
      "step: 19354, loss: 1.1844520568847656\n",
      "step: 19359, loss: 0.6805459856987\n",
      "Checkpoint is saved\n",
      "step: 19364, loss: 0.5965153574943542\n",
      "step: 19369, loss: 0.4424283802509308\n",
      "step: 19374, loss: 1.051795482635498\n",
      "step: 19379, loss: 1.1278080940246582\n",
      "Checkpoint is saved\n",
      "step: 19384, loss: 1.0582304000854492\n",
      "step: 19389, loss: 0.741578221321106\n",
      "step: 19394, loss: 0.8848958611488342\n",
      "step: 19399, loss: 0.4993942379951477\n",
      "Checkpoint is saved\n",
      "step: 19404, loss: 1.3386611938476562\n",
      "step: 19409, loss: 0.8120880126953125\n",
      "step: 19414, loss: 0.6986761093139648\n",
      "step: 19419, loss: 0.8921258449554443\n",
      "Checkpoint is saved\n",
      "step: 19424, loss: 0.6158024072647095\n",
      "step: 19429, loss: 0.8037406206130981\n",
      "step: 19434, loss: 0.6408118009567261\n",
      "step: 19439, loss: 0.7120466232299805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 19444, loss: 0.39312034845352173\n",
      "step: 19449, loss: 0.9983335733413696\n",
      "step: 19454, loss: 0.9726832509040833\n",
      "step: 19459, loss: 1.2510075569152832\n",
      "Checkpoint is saved\n",
      "step: 19464, loss: 1.2728900909423828\n",
      "step: 19469, loss: 0.816279947757721\n",
      "step: 19474, loss: 0.646870493888855\n",
      "step: 19479, loss: 1.4541826248168945\n",
      "Checkpoint is saved\n",
      "step: 19484, loss: 1.0687402486801147\n",
      "step: 19489, loss: 1.1506597995758057\n",
      "step: 19494, loss: 0.692036509513855\n",
      "step: 19499, loss: 1.1839573383331299\n",
      "Checkpoint is saved\n",
      "step: 19504, loss: 1.2361409664154053\n",
      "step: 19509, loss: 0.49167895317077637\n",
      "step: 19514, loss: 0.8509073257446289\n",
      "step: 19519, loss: 0.6192077398300171\n",
      "Checkpoint is saved\n",
      "step: 19524, loss: 0.6289862394332886\n",
      "step: 19529, loss: 1.3136467933654785\n",
      "step: 19534, loss: 0.9721652269363403\n",
      "step: 19539, loss: 1.2099584341049194\n",
      "Checkpoint is saved\n",
      "step: 19544, loss: 0.49020814895629883\n",
      "step: 19549, loss: 0.6874526739120483\n",
      "step: 19554, loss: 1.010270357131958\n",
      "step: 19559, loss: 1.0041965246200562\n",
      "Checkpoint is saved\n",
      "step: 19564, loss: 1.2728686332702637\n",
      "step: 19569, loss: 0.7876129746437073\n",
      "step: 19574, loss: 1.050992488861084\n",
      "step: 19579, loss: 0.723587691783905\n",
      "Checkpoint is saved\n",
      "step: 19584, loss: 0.6262420415878296\n",
      "step: 19589, loss: 0.3260926306247711\n",
      "step: 19594, loss: 1.0220484733581543\n",
      "step: 19599, loss: 0.6490107774734497\n",
      "Checkpoint is saved\n",
      "step: 19604, loss: 0.8923590779304504\n",
      "step: 19609, loss: 0.7843521237373352\n",
      "step: 19614, loss: 0.8262835144996643\n",
      "step: 19619, loss: 0.62043297290802\n",
      "Checkpoint is saved\n",
      "step: 19624, loss: 0.7796642184257507\n",
      "step: 19629, loss: 0.926506757736206\n",
      "step: 19634, loss: 0.9178755879402161\n",
      "step: 19639, loss: 1.0579562187194824\n",
      "Checkpoint is saved\n",
      "step: 19644, loss: 0.848305881023407\n",
      "step: 19649, loss: 1.0835351943969727\n",
      "step: 19654, loss: 0.7170932292938232\n",
      "step: 19659, loss: 0.8685830235481262\n",
      "Checkpoint is saved\n",
      "step: 19664, loss: 0.8649124503135681\n",
      "step: 19669, loss: 1.2309279441833496\n",
      "step: 19674, loss: 0.8800175786018372\n",
      "step: 19679, loss: 0.6342573165893555\n",
      "Checkpoint is saved\n",
      "step: 19684, loss: 0.8837990760803223\n",
      "step: 19689, loss: 0.5521172881126404\n",
      "step: 19694, loss: 0.8535539507865906\n",
      "step: 19699, loss: 1.1716177463531494\n",
      "Checkpoint is saved\n",
      "step: 19704, loss: 0.6684435606002808\n",
      "step: 19709, loss: 1.2331876754760742\n",
      "step: 19714, loss: 0.6911685466766357\n",
      "step: 19719, loss: 0.9464749693870544\n",
      "Checkpoint is saved\n",
      "step: 19724, loss: 0.772667407989502\n",
      "step: 19729, loss: 0.5344635248184204\n",
      "step: 19734, loss: 0.6920173168182373\n",
      "step: 19739, loss: 0.5006215572357178\n",
      "Checkpoint is saved\n",
      "step: 19744, loss: 0.6838811635971069\n",
      "step: 19749, loss: 0.7929631471633911\n",
      "step: 19754, loss: 1.2468955516815186\n",
      "step: 19759, loss: 0.9457674026489258\n",
      "Checkpoint is saved\n",
      "step: 19764, loss: 0.6215311884880066\n",
      "step: 19769, loss: 0.7104671001434326\n",
      "step: 19774, loss: 0.5730389356613159\n",
      "step: 19779, loss: 0.7041784524917603\n",
      "Checkpoint is saved\n",
      "step: 19784, loss: 0.8863605260848999\n",
      "step: 19789, loss: 0.45136067271232605\n",
      "step: 19794, loss: 1.448349118232727\n",
      "step: 19799, loss: 0.99215167760849\n",
      "Checkpoint is saved\n",
      "step: 19804, loss: 0.47871100902557373\n",
      "step: 19809, loss: 1.0326135158538818\n",
      "step: 19814, loss: 1.0660463571548462\n",
      "step: 19819, loss: 0.6412827968597412\n",
      "Checkpoint is saved\n",
      "step: 19824, loss: 1.1299388408660889\n",
      "step: 19829, loss: 0.9322040677070618\n",
      "step: 19834, loss: 0.648138165473938\n",
      "step: 19839, loss: 0.8274062871932983\n",
      "Checkpoint is saved\n",
      "step: 19844, loss: 1.0447115898132324\n",
      "step: 19849, loss: 0.7817496657371521\n",
      "step: 19854, loss: 1.439100742340088\n",
      "step: 19859, loss: 0.5868337750434875\n",
      "Checkpoint is saved\n",
      "step: 19864, loss: 0.6835176944732666\n",
      "step: 19869, loss: 0.9448046684265137\n",
      "step: 19874, loss: 0.9815871715545654\n",
      "step: 19879, loss: 0.6060712337493896\n",
      "Checkpoint is saved\n",
      "step: 19884, loss: 0.823474645614624\n",
      "step: 19889, loss: 0.5288422703742981\n",
      "step: 19894, loss: 1.0381581783294678\n",
      "step: 19899, loss: 1.0995234251022339\n",
      "Checkpoint is saved\n",
      "step: 19904, loss: 1.0168771743774414\n",
      "step: 19909, loss: 0.9449599385261536\n",
      "step: 19914, loss: 0.8029624223709106\n",
      "step: 19919, loss: 0.8911596536636353\n",
      "Checkpoint is saved\n",
      "step: 19924, loss: 1.201860785484314\n",
      "step: 19929, loss: 0.7474033832550049\n",
      "step: 19934, loss: 1.153879165649414\n",
      "step: 19939, loss: 1.189680814743042\n",
      "Checkpoint is saved\n",
      "step: 19944, loss: 1.1259839534759521\n",
      "step: 19949, loss: 0.5437231063842773\n",
      "step: 19954, loss: 1.0097702741622925\n",
      "step: 19959, loss: 1.0967952013015747\n",
      "Checkpoint is saved\n",
      "step: 19964, loss: 0.7224721908569336\n",
      "step: 19969, loss: 0.768244743347168\n",
      "step: 19974, loss: 0.6175018548965454\n",
      "step: 19979, loss: 0.8295120596885681\n",
      "Checkpoint is saved\n",
      "step: 19984, loss: 0.5913614630699158\n",
      "step: 19989, loss: 0.9181795120239258\n",
      "step: 19994, loss: 1.1641429662704468\n",
      "step: 19999, loss: 0.5522943735122681\n",
      "Checkpoint is saved\n",
      "step: 20004, loss: 0.7166919708251953\n",
      "step: 20009, loss: 0.7844201326370239\n",
      "step: 20014, loss: 1.0216801166534424\n",
      "step: 20019, loss: 0.4147604703903198\n",
      "Checkpoint is saved\n",
      "step: 20024, loss: 0.9363579750061035\n",
      "step: 20029, loss: 0.7470062971115112\n",
      "step: 20034, loss: 1.1203370094299316\n",
      "step: 20039, loss: 0.9125627279281616\n",
      "Checkpoint is saved\n",
      "step: 20044, loss: 0.9397482872009277\n",
      "step: 20049, loss: 0.9973511099815369\n",
      "step: 20054, loss: 0.642163097858429\n",
      "step: 20059, loss: 0.6170520186424255\n",
      "Checkpoint is saved\n",
      "step: 20064, loss: 1.245114803314209\n",
      "step: 20069, loss: 0.6838659048080444\n",
      "step: 20074, loss: 1.1427197456359863\n",
      "step: 20079, loss: 1.0854589939117432\n",
      "Checkpoint is saved\n",
      "step: 20084, loss: 0.6519953012466431\n",
      "step: 20089, loss: 0.5660648345947266\n",
      "step: 20094, loss: 0.7953535914421082\n",
      "step: 20099, loss: 1.0419517755508423\n",
      "Checkpoint is saved\n",
      "step: 20104, loss: 0.726677417755127\n",
      "step: 20109, loss: 0.6872943043708801\n",
      "step: 20114, loss: 1.3603367805480957\n",
      "step: 20119, loss: 1.0343453884124756\n",
      "Checkpoint is saved\n",
      "step: 20124, loss: 1.0019607543945312\n",
      "step: 20129, loss: 0.953308641910553\n",
      "step: 20134, loss: 0.7586479187011719\n",
      "step: 20139, loss: 1.1218643188476562\n",
      "Checkpoint is saved\n",
      "step: 20144, loss: 0.9113302826881409\n",
      "step: 20149, loss: 0.5154958963394165\n",
      "step: 20154, loss: 1.036407709121704\n",
      "step: 20159, loss: 0.9534060955047607\n",
      "Checkpoint is saved\n",
      "step: 20164, loss: 1.144953966140747\n",
      "step: 20169, loss: 1.077946424484253\n",
      "step: 20174, loss: 0.6293684244155884\n",
      "step: 20179, loss: 0.638934850692749\n",
      "Checkpoint is saved\n",
      "step: 20184, loss: 1.0563437938690186\n",
      "step: 20189, loss: 1.1663765907287598\n",
      "step: 20194, loss: 0.5455289483070374\n",
      "step: 20199, loss: 0.8134822845458984\n",
      "Checkpoint is saved\n",
      "step: 20204, loss: 0.4977078437805176\n",
      "step: 20209, loss: 0.6483553647994995\n",
      "step: 20214, loss: 0.7882393598556519\n",
      "step: 20219, loss: 0.9983463287353516\n",
      "Checkpoint is saved\n",
      "step: 20224, loss: 0.7285085916519165\n",
      "step: 20229, loss: 0.7544656991958618\n",
      "step: 20234, loss: 0.8516541123390198\n",
      "step: 20239, loss: 0.774684488773346\n",
      "Checkpoint is saved\n",
      "step: 20244, loss: 0.9045758843421936\n",
      "step: 20249, loss: 0.6367338299751282\n",
      "step: 20254, loss: 0.6448222398757935\n",
      "step: 20259, loss: 0.2530938386917114\n",
      "Checkpoint is saved\n",
      "step: 20264, loss: 0.5702093839645386\n",
      "step: 20269, loss: 1.1603024005889893\n",
      "step: 20274, loss: 0.7353723049163818\n",
      "step: 20279, loss: 0.6717842817306519\n",
      "Checkpoint is saved\n",
      "step: 20284, loss: 0.991281270980835\n",
      "step: 20289, loss: 0.8108772039413452\n",
      "step: 20294, loss: 0.5879635810852051\n",
      "step: 20299, loss: 0.9513542056083679\n",
      "Checkpoint is saved\n",
      "step: 20304, loss: 0.9962888360023499\n",
      "step: 20309, loss: 0.8915092945098877\n",
      "step: 20314, loss: 0.7653992176055908\n",
      "step: 20319, loss: 1.0586826801300049\n",
      "Checkpoint is saved\n",
      "step: 20324, loss: 0.532394289970398\n",
      "step: 20329, loss: 1.2932387590408325\n",
      "step: 20334, loss: 0.8483583927154541\n",
      "step: 20339, loss: 0.5709261894226074\n",
      "Checkpoint is saved\n",
      "step: 20344, loss: 0.8118345141410828\n",
      "step: 20349, loss: 0.993945837020874\n",
      "step: 20354, loss: 0.8623677492141724\n",
      "step: 20359, loss: 0.5470883846282959\n",
      "Checkpoint is saved\n",
      "step: 20364, loss: 0.5387597680091858\n",
      "step: 20369, loss: 0.6166672110557556\n",
      "step: 20374, loss: 1.1352953910827637\n",
      "step: 20379, loss: 0.8471521735191345\n",
      "Checkpoint is saved\n",
      "step: 20384, loss: 1.3298649787902832\n",
      "step: 20389, loss: 0.7412445545196533\n",
      "step: 20394, loss: 0.8085897564888\n",
      "step: 20399, loss: 0.6489734053611755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint is saved\n",
      "step: 20404, loss: 0.8676848411560059\n",
      "step: 20409, loss: 0.848927915096283\n",
      "step: 20414, loss: 0.7429018020629883\n",
      "step: 20419, loss: 0.8629287481307983\n",
      "Checkpoint is saved\n",
      "step: 20424, loss: 0.5584609508514404\n",
      "step: 20429, loss: 1.0713486671447754\n",
      "step: 20434, loss: 0.9765496253967285\n",
      "step: 20439, loss: 1.1121225357055664\n",
      "Checkpoint is saved\n",
      "step: 20444, loss: 1.0037298202514648\n",
      "step: 20449, loss: 0.8340176939964294\n",
      "step: 20454, loss: 0.9072484970092773\n",
      "step: 20459, loss: 0.8698081374168396\n",
      "Checkpoint is saved\n",
      "step: 20464, loss: 0.9347933530807495\n",
      "step: 20469, loss: 0.8901329040527344\n",
      "step: 20474, loss: 0.9204308390617371\n",
      "step: 20479, loss: 0.7078996896743774\n",
      "Checkpoint is saved\n",
      "step: 20484, loss: 0.3458591103553772\n",
      "step: 20489, loss: 0.7629587650299072\n",
      "step: 20494, loss: 0.632800817489624\n",
      "step: 20499, loss: 0.8252822160720825\n",
      "Checkpoint is saved\n",
      "step: 20504, loss: 1.0496907234191895\n",
      "step: 20509, loss: 0.8066083192825317\n",
      "step: 20514, loss: 1.0940942764282227\n",
      "step: 20519, loss: 0.688537061214447\n",
      "Checkpoint is saved\n",
      "step: 20524, loss: 1.0978819131851196\n",
      "step: 20529, loss: 0.4471035301685333\n",
      "step: 20534, loss: 0.8097984790802002\n",
      "step: 20539, loss: 0.7974151372909546\n",
      "Checkpoint is saved\n",
      "step: 20544, loss: 0.8094260096549988\n",
      "step: 20549, loss: 0.36954495310783386\n",
      "step: 20554, loss: 1.1228642463684082\n",
      "step: 20559, loss: 0.5593502521514893\n",
      "Checkpoint is saved\n",
      "step: 20564, loss: 0.920600414276123\n",
      "step: 20569, loss: 0.8751810789108276\n",
      "step: 20574, loss: 0.9924111366271973\n",
      "step: 20579, loss: 0.6991360187530518\n",
      "Checkpoint is saved\n",
      "step: 20584, loss: 0.6344715356826782\n",
      "step: 20589, loss: 1.1581664085388184\n",
      "step: 20594, loss: 0.5328786373138428\n",
      "step: 20599, loss: 0.9607498645782471\n",
      "Checkpoint is saved\n",
      "step: 20604, loss: 0.7779710292816162\n",
      "step: 20609, loss: 0.838625431060791\n",
      "step: 20614, loss: 1.0487804412841797\n",
      "step: 20619, loss: 0.6835346221923828\n",
      "Checkpoint is saved\n",
      "step: 20624, loss: 1.055147647857666\n",
      "step: 20629, loss: 0.6801353693008423\n",
      "step: 20634, loss: 0.904854416847229\n",
      "step: 20639, loss: 0.7924482822418213\n",
      "Checkpoint is saved\n",
      "step: 20644, loss: 0.6974237561225891\n",
      "step: 20649, loss: 0.6827002763748169\n",
      "step: 20654, loss: 0.7783665060997009\n",
      "step: 20659, loss: 0.5653698444366455\n",
      "Checkpoint is saved\n",
      "step: 20664, loss: 0.929957389831543\n",
      "step: 20669, loss: 0.748063325881958\n",
      "step: 20674, loss: 0.7405210733413696\n",
      "step: 20679, loss: 0.8648792505264282\n",
      "Checkpoint is saved\n",
      "step: 20684, loss: 1.0450395345687866\n",
      "step: 20689, loss: 0.5744268894195557\n",
      "step: 20694, loss: 0.64332515001297\n",
      "step: 20699, loss: 0.8820712566375732\n",
      "Checkpoint is saved\n",
      "step: 20704, loss: 1.028145670890808\n",
      "step: 20709, loss: 0.8926281332969666\n",
      "step: 20714, loss: 0.9936563372612\n",
      "step: 20719, loss: 0.6917966604232788\n",
      "Checkpoint is saved\n",
      "step: 20724, loss: 1.0308029651641846\n",
      "step: 20729, loss: 0.7047610878944397\n",
      "step: 20734, loss: 0.9118064045906067\n",
      "step: 20739, loss: 1.1666650772094727\n",
      "Checkpoint is saved\n",
      "step: 20744, loss: 0.7478432059288025\n",
      "step: 20749, loss: 0.5433325171470642\n",
      "step: 20754, loss: 0.7641316056251526\n",
      "step: 20759, loss: 0.44963890314102173\n",
      "Checkpoint is saved\n",
      "step: 20764, loss: 1.0974332094192505\n",
      "step: 20769, loss: 0.9428192377090454\n",
      "step: 20774, loss: 0.5907397866249084\n",
      "step: 20779, loss: 1.060937523841858\n",
      "Checkpoint is saved\n",
      "step: 20784, loss: 0.6678229570388794\n",
      "step: 20789, loss: 0.5421164035797119\n",
      "step: 20794, loss: 0.7360334992408752\n",
      "step: 20799, loss: 0.616228461265564\n",
      "Checkpoint is saved\n",
      "step: 20804, loss: 0.8704140782356262\n",
      "step: 20809, loss: 1.2199007272720337\n",
      "step: 20814, loss: 1.0834355354309082\n",
      "step: 20819, loss: 0.8213979005813599\n",
      "Checkpoint is saved\n",
      "step: 20824, loss: 1.0348904132843018\n",
      "step: 20829, loss: 0.6850067377090454\n",
      "step: 20834, loss: 0.8510646820068359\n",
      "step: 20839, loss: 0.8877811431884766\n",
      "Checkpoint is saved\n",
      "step: 20844, loss: 1.2874207496643066\n",
      "step: 20849, loss: 0.7346304655075073\n",
      "step: 20854, loss: 0.7612491846084595\n",
      "step: 20859, loss: 0.8532639741897583\n",
      "Checkpoint is saved\n",
      "step: 20864, loss: 0.6166386008262634\n",
      "step: 20869, loss: 0.7649450302124023\n",
      "step: 20874, loss: 0.9610719084739685\n",
      "step: 20879, loss: 0.9999028444290161\n",
      "Checkpoint is saved\n",
      "step: 20884, loss: 1.0149354934692383\n",
      "step: 20889, loss: 1.4580938816070557\n",
      "step: 20894, loss: 0.7574211359024048\n",
      "step: 20899, loss: 0.724337100982666\n",
      "Checkpoint is saved\n",
      "step: 20904, loss: 0.7881466746330261\n",
      "step: 20909, loss: 0.6888514757156372\n",
      "step: 20914, loss: 1.4503250122070312\n",
      "step: 20919, loss: 0.469949334859848\n",
      "Checkpoint is saved\n",
      "step: 20924, loss: 0.9480481743812561\n",
      "step: 20929, loss: 0.9909566044807434\n",
      "step: 20934, loss: 0.8712027072906494\n",
      "step: 20939, loss: 0.9522477984428406\n",
      "Checkpoint is saved\n",
      "step: 20944, loss: 0.7450728416442871\n",
      "step: 20949, loss: 0.8787119388580322\n",
      "step: 20954, loss: 0.8473803997039795\n",
      "step: 20959, loss: 0.5511208772659302\n",
      "Checkpoint is saved\n",
      "step: 20964, loss: 0.46824055910110474\n",
      "step: 20969, loss: 0.5894191265106201\n",
      "step: 20974, loss: 1.0911117792129517\n",
      "step: 20979, loss: 0.9072481393814087\n",
      "Checkpoint is saved\n",
      "step: 20984, loss: 0.9469569325447083\n",
      "step: 20989, loss: 0.8641505241394043\n",
      "step: 20994, loss: 0.6431828141212463\n",
      "step: 20999, loss: 0.6572644710540771\n",
      "Checkpoint is saved\n",
      "step: 21004, loss: 0.5190390348434448\n",
      "step: 21009, loss: 0.38986366987228394\n",
      "step: 21014, loss: 0.7971094846725464\n",
      "step: 21019, loss: 0.9035242795944214\n",
      "Checkpoint is saved\n",
      "step: 21024, loss: 0.8001078367233276\n",
      "step: 21029, loss: 0.9498843550682068\n",
      "step: 21034, loss: 0.9493405818939209\n",
      "step: 21039, loss: 0.8413830995559692\n",
      "Checkpoint is saved\n",
      "step: 21044, loss: 0.8917595148086548\n",
      "step: 21049, loss: 0.7701429128646851\n",
      "step: 21054, loss: 0.950670599937439\n",
      "step: 21059, loss: 0.5434200763702393\n",
      "Checkpoint is saved\n",
      "step: 21064, loss: 0.5403579473495483\n",
      "step: 21069, loss: 0.7066474556922913\n",
      "step: 21074, loss: 0.33495527505874634\n",
      "step: 21079, loss: 1.0115985870361328\n",
      "Checkpoint is saved\n",
      "step: 21084, loss: 0.8308585286140442\n",
      "step: 21089, loss: 0.9528601765632629\n",
      "step: 21094, loss: 0.7478622198104858\n",
      "step: 21099, loss: 1.4765565395355225\n",
      "Checkpoint is saved\n",
      "step: 21104, loss: 0.4671139717102051\n",
      "step: 21109, loss: 0.9707987904548645\n",
      "step: 21114, loss: 0.7999294996261597\n",
      "step: 21119, loss: 0.8574086427688599\n",
      "Checkpoint is saved\n",
      "step: 21124, loss: 1.1718350648880005\n",
      "step: 21129, loss: 1.055501937866211\n",
      "step: 21134, loss: 0.5843163728713989\n",
      "step: 21139, loss: 0.2075011283159256\n",
      "Checkpoint is saved\n",
      "step: 21144, loss: 1.2929304838180542\n",
      "step: 21149, loss: 1.0861953496932983\n",
      "step: 21154, loss: 0.8323931694030762\n",
      "step: 21159, loss: 1.1744428873062134\n",
      "Checkpoint is saved\n",
      "step: 21164, loss: 0.5153765678405762\n",
      "step: 21169, loss: 0.6011548638343811\n",
      "step: 21174, loss: 0.28765153884887695\n",
      "step: 21179, loss: 0.7330026626586914\n",
      "Checkpoint is saved\n",
      "step: 21184, loss: 0.9428032040596008\n",
      "step: 21189, loss: 0.8191604614257812\n",
      "step: 21194, loss: 1.1097484827041626\n",
      "step: 21199, loss: 0.7894642949104309\n",
      "Checkpoint is saved\n",
      "step: 21204, loss: 0.7410258650779724\n",
      "step: 21209, loss: 0.6758861541748047\n",
      "step: 21214, loss: 0.5229611992835999\n",
      "step: 21219, loss: 0.6660711169242859\n",
      "Checkpoint is saved\n",
      "step: 21224, loss: 0.5086065530776978\n",
      "step: 21229, loss: 0.8804125785827637\n",
      "step: 21234, loss: 0.6784906983375549\n",
      "step: 21239, loss: 0.7484813928604126\n",
      "Checkpoint is saved\n",
      "step: 21244, loss: 0.8656187057495117\n",
      "step: 21249, loss: 0.6715859174728394\n",
      "step: 21254, loss: 0.4885704815387726\n",
      "step: 21259, loss: 0.7658457159996033\n",
      "Checkpoint is saved\n",
      "step: 21264, loss: 0.4953958988189697\n",
      "step: 21269, loss: 0.9057216048240662\n",
      "step: 21274, loss: 0.5573039650917053\n",
      "step: 21279, loss: 0.3983266353607178\n",
      "Checkpoint is saved\n",
      "step: 21284, loss: 1.1364855766296387\n",
      "step: 21289, loss: 0.8663680553436279\n",
      "step: 21294, loss: 0.6895948648452759\n",
      "step: 21299, loss: 0.8971419334411621\n",
      "Checkpoint is saved\n",
      "step: 21304, loss: 0.49743154644966125\n",
      "step: 21309, loss: 0.9147307276725769\n",
      "step: 21314, loss: 0.4170161485671997\n",
      "step: 21319, loss: 0.7836988568305969\n",
      "Checkpoint is saved\n",
      "step: 21324, loss: 0.39742720127105713\n",
      "step: 21329, loss: 0.9563577175140381\n",
      "step: 21334, loss: 0.6269852519035339\n",
      "step: 21339, loss: 0.8969159126281738\n",
      "Checkpoint is saved\n",
      "step: 21344, loss: 0.8367995619773865\n",
      "step: 21349, loss: 0.8584445118904114\n",
      "step: 21354, loss: 0.6606417894363403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21359, loss: 0.5043588280677795\n",
      "Checkpoint is saved\n",
      "step: 21364, loss: 0.6299815773963928\n",
      "step: 21369, loss: 0.8382488489151001\n",
      "step: 21374, loss: 0.513045072555542\n",
      "step: 21379, loss: 0.9873439073562622\n",
      "Checkpoint is saved\n",
      "step: 21384, loss: 0.9761548042297363\n",
      "step: 21389, loss: 0.8829024434089661\n",
      "step: 21394, loss: 0.6518779993057251\n",
      "step: 21399, loss: 0.48989155888557434\n",
      "Checkpoint is saved\n",
      "step: 21404, loss: 1.0351479053497314\n",
      "step: 21409, loss: 0.8391574621200562\n",
      "step: 21414, loss: 0.7944072484970093\n",
      "step: 21419, loss: 0.8988919258117676\n",
      "Checkpoint is saved\n",
      "step: 21424, loss: 0.6494423747062683\n",
      "step: 21429, loss: 0.897711992263794\n",
      "step: 21434, loss: 0.6886060237884521\n",
      "step: 21439, loss: 0.817542314529419\n",
      "Checkpoint is saved\n",
      "step: 21444, loss: 0.8549622893333435\n",
      "step: 21449, loss: 0.6679859161376953\n",
      "step: 21454, loss: 0.6417151689529419\n",
      "step: 21459, loss: 0.8486583828926086\n",
      "Checkpoint is saved\n",
      "step: 21464, loss: 1.1261606216430664\n",
      "step: 21469, loss: 0.4949234127998352\n",
      "step: 21474, loss: 1.2148921489715576\n",
      "step: 21479, loss: 0.7503706216812134\n",
      "Checkpoint is saved\n",
      "step: 21484, loss: 0.8836461305618286\n",
      "step: 21489, loss: 0.6656312942504883\n",
      "step: 21494, loss: 0.8359559178352356\n",
      "step: 21499, loss: 0.8975937366485596\n",
      "Checkpoint is saved\n",
      "step: 21504, loss: 0.7800188064575195\n",
      "step: 21509, loss: 0.7181934118270874\n",
      "step: 21514, loss: 0.7280130386352539\n",
      "step: 21519, loss: 0.8207733631134033\n",
      "Checkpoint is saved\n",
      "step: 21524, loss: 0.9142058491706848\n",
      "step: 21529, loss: 0.8701288104057312\n",
      "step: 21534, loss: 0.8913218975067139\n",
      "step: 21539, loss: 0.989411473274231\n",
      "Checkpoint is saved\n",
      "step: 21544, loss: 0.5035338401794434\n",
      "step: 21549, loss: 0.8595535159111023\n",
      "step: 21554, loss: 0.6412951350212097\n",
      "step: 21559, loss: 0.9475654363632202\n",
      "Checkpoint is saved\n",
      "step: 21564, loss: 0.8861286044120789\n",
      "step: 21569, loss: 0.7460280656814575\n",
      "step: 21574, loss: 0.7115732431411743\n",
      "step: 21579, loss: 0.7659483551979065\n",
      "Checkpoint is saved\n",
      "step: 21584, loss: 1.041933298110962\n",
      "step: 21589, loss: 0.653217077255249\n",
      "step: 21594, loss: 0.6308153867721558\n",
      "step: 21599, loss: 0.9519069194793701\n",
      "Checkpoint is saved\n",
      "step: 21604, loss: 0.961940348148346\n",
      "step: 21609, loss: 0.643220841884613\n",
      "step: 21614, loss: 0.5608927607536316\n",
      "step: 21619, loss: 0.8486623764038086\n",
      "Checkpoint is saved\n",
      "step: 21624, loss: 0.9604859352111816\n",
      "step: 21629, loss: 0.8155456185340881\n",
      "step: 21634, loss: 0.7568549513816833\n",
      "step: 21639, loss: 0.9300550818443298\n",
      "Checkpoint is saved\n",
      "step: 21644, loss: 0.70916748046875\n",
      "step: 21649, loss: 0.897571325302124\n",
      "step: 21654, loss: 0.42771223187446594\n",
      "step: 21659, loss: 0.7698051929473877\n",
      "Checkpoint is saved\n",
      "step: 21664, loss: 0.68939608335495\n",
      "step: 21669, loss: 1.0261354446411133\n",
      "step: 21674, loss: 1.2116947174072266\n",
      "step: 21679, loss: 0.7167970538139343\n",
      "Checkpoint is saved\n",
      "step: 21684, loss: 0.6872076988220215\n",
      "step: 21689, loss: 0.9842357635498047\n",
      "step: 21694, loss: 0.8681361675262451\n",
      "step: 21699, loss: 0.7147765159606934\n",
      "Checkpoint is saved\n",
      "step: 21704, loss: 0.8846088647842407\n",
      "step: 21709, loss: 0.6174749135971069\n",
      "step: 21714, loss: 0.7965568900108337\n",
      "step: 21719, loss: 0.5560140609741211\n",
      "Checkpoint is saved\n",
      "step: 21724, loss: 0.825736403465271\n",
      "step: 21729, loss: 0.9556952714920044\n",
      "step: 21734, loss: 0.8732035756111145\n",
      "step: 21739, loss: 0.9296975135803223\n",
      "Checkpoint is saved\n",
      "step: 21744, loss: 0.38268566131591797\n",
      "step: 21749, loss: 0.8032361268997192\n",
      "step: 21754, loss: 1.2340543270111084\n",
      "step: 21759, loss: 0.796873152256012\n",
      "Checkpoint is saved\n",
      "step: 21764, loss: 0.6791552901268005\n",
      "step: 21769, loss: 0.5975114107131958\n",
      "step: 21774, loss: 1.1479904651641846\n",
      "step: 21779, loss: 0.867682158946991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-94e4256bd458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint is saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtraining_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Training time for {0} steps: {1}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print ('------------------TRAINING------------------')\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "             \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print ('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, 'checkpoints/', global_step=step)\n",
    "            print ('Checkpoint is saved')\n",
    "    training_log = 'Training time for {0} steps: {1}s'.format(steps, time.time() - t)\n",
    "            \n",
    "    print (training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEkCAYAAAChew9BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4U2XDBvA7TXdLSVs6oFCqbaG0\nFoEqUyiCCAiIqMh0oIiivoqIMl4UXhytgggoHyrIUPYogiAgCiIbFBAEKoUyuwcp3SPJ90dGM9uk\nTZocev+ui0ubk5w8OU3PfZ7nPEMklUoVICIiEiAnexeAiIiorhhiREQkWAwxIiISLIYYEREJFkOM\niIgEiyFGRESCxRAjIiLBsmuIHT58GCNHjkS7du0gkUiwZs0azbbKykrMmjUL3bt3R4sWLdC2bVuM\nHz8eN2/etGOJiYjIkdg1xIqLixEdHY3ExER4eHjobCspKcHff/+NKVOm4MCBA1i7di3S0tLw9NNP\no6qqyk4lJiIiRyJylBk7QkJC8Nlnn2HMmDEmn5OcnIyuXbvi8OHDiImJacDSERGRIxLUPbHCwkIA\ngEQisXNJiIjIEQgmxCoqKjBz5kwMGDAAISEh9i4OERE5AGd7F8AcVVVVmDBhAgoKCrBu3Tp7F4eI\niByEw9fEqqqq8NJLL+H8+fPYtm0b/Pz8bPp+KSkpNt3/3YrHrW543OqOx65u7rbj5tA1scrKSrz4\n4ou4ePEiduzYgaCgIHsXiYiIHIhdQ6yoqAipqakAALlcjlu3buHs2bPw9fVF8+bN8fzzz+P06dNY\nt24dRCIRsrKyAAA+Pj4GXfKJiKjxsWtz4unTp9GrVy/06tULpaWlSEhIQK9evfDJJ58gLS0NP//8\nMzIyMtC7d2+0bdtW8y8pKcmexSYiIgdh15pYz549IZVKTW6vaRsREZHDd+wgIiIyhSFGRESCxRAj\nIiLBYogREZFgMcSIiEiwGGJERCRYDDEiIhIshhgREQkWQ4yIiASLIUZERILFECMiIsFiiBERkWAx\nxIiISLAYYkREJFgMMSIiEiyGGBERCRZDjIiIBIshRkREgsUQIyIiwWKIERGRYDHEiIhIsBhiREQk\nWAwxIiISLIYYEREJFkOMiIgEiyFGRESCxRAjIiLBYogREZFg2TXEDh8+jJEjR6Jdu3aQSCRYs2aN\nznaFQoGEhARERUUhODgYgwYNwsWLF+1UWiIicjR2DbHi4mJER0cjMTERHh4eBtsXLlyIxYsX49NP\nP8W+ffsQEBCAYcOGobCw0A6lJSIiR2PXEHv00UfxwQcfYOjQoXBy0i2KQqHAkiVLMGnSJAwdOhTR\n0dFYsmQJioqKsHnzZjuVmIiIHInD3hO7fv06srKy0KdPH81jHh4e6N69O44fP27HkhERkaNwtncB\nTMnKygIABAQE6DweEBCAjIwMk69LSUmp93tbYx+NEY9b3fC41R2PXd0I6bhFRkbWuN1hQ6yuavvA\ntUlJSan3PhojHre64XGrOx67urnbjpvDNicGBQUBAHJycnQez8nJQWBgoD2KREREDsZhQ6x169YI\nCgrC/v37NY+VlZXh6NGj6NKlix1LRkREjsKuzYlFRUVITU0FAMjlcty6dQtnz56Fr68vWrVqhYkT\nJ2L+/PmIjIxEREQE5s2bBy8vLzz99NP2LDYRETkIu4bY6dOnMWTIEM3PCQkJSEhIwKhRo7BkyRK8\n9dZbKC0txbvvvgupVIq4uDgkJSWhSZMmdiw1ERE5CruGWM+ePSGVSk1uF4lEmD59OqZPn96ApSIi\nIqFw2HtiREREtWGIERGRYDHEiIhIsBhiREQkWAwxIiISLIYYEREJFkNMS7K0EjuyxPYuBhERmYkh\npuWjv+7gfylu9i4GERGZiSGmJa9cbu8iEBGRBRhiWo5mVdi7CEREZAGGGBERCRZDjIiIBIshRkRE\ngsUQIyIiwWKIaXES2bsERERkCYaYlu0DmsFFpLB3MYiIyEwMMS2h3mL4uTLEiIiEgiGmxdVJBBkz\njIhIMBhiWpxEgFzBG2NERELBENPiJAJYESMiEg6GmBYnAHKmGBGRYDDEtDiJRKyJEREJCENMi0jE\nmhgRkZAwxLTwnhgRkbAwxLTwnhgRkbAwxLQ4iUTgsphERMLBENPiJAIUrIkREQkGQ0yLkwisiRER\nCYhDh5hMJsNHH32E9u3bIygoCO3bt8dHH32Eqqoqm7yfE1gTIyISEmd7F6AmCxYswLJly7BkyRJE\nR0fj/PnzmDhxIlxdXfHee+9Z/f1ErIkREQmKQ4fYiRMnMGDAAAwcOBAA0Lp1awwcOBB//fWXTd5P\nBEABERQKBUQizqFIROToHLo5sWvXrjh06BAuXboEAEhOTsbBgwfRr18/m7yfSCSCCAqOFSMiEgiR\nVCp12HO2QqHARx99hPnz50MsFqOqqgpTpkzBzJkzTb4mJSWlXu/Z9ZAHDvUohTMrYkREdhcZGVnj\ndoduTkxKSsL69euxbNkyREVF4dy5c5g2bRpCQ0Px3HPPGX1NbR+4NqLDtxAeHgFXMVPMEikpKfU+\n9o0Rj1vd8djVzd123Bw6xD744AO88cYbeOqppwAAMTExuHnzJr744guTIVZfnLWDiEg4HPqeWElJ\nCcRisc5jYrEYcrnt+hAqeygyxYiIhMCha2IDBgzAggUL0Lp1a0RFReHs2bNYvHgxRo4cabP3ZE2M\niEg4HDrEPvvsM3z88cd45513kJubi6CgIDz//PM2GSOm5sTlWIiIBMOhQ6xJkyZITExEYmJig72n\nCAwxIiKhcOh7YvbANcWIiISDIaZHWRNjjBERCQFDTA/viRERCQdDTA/viRERCQdDTI+TSMGZ7ImI\nBIIhpofjxIiIhIMhpkckYscOIiKhsFqIKRQKlJSUWGt3dsOaGBGRcFgcYjt27MCcOXN0Hvvyyy8R\nEhKCli1bYvTo0YIOM+XCmEREJAQWh9iCBQuQmZmp+fnMmTOYNWsW4uLi8MILL2Dv3r1YuHChVQvZ\nkNjFnohIOCyedurKlSt4+umnNT9v2rQJfn5+2Lx5M9zc3ODs7IykpCRMnz7dqgVtKLfKnHA8uwL3\n+jj0jFxERIQ61MTKysrg6emp+Xnfvn3o27cv3NzcAACxsbFIS0uzXgntYOLB2/YuAhERmcHiEAsJ\nCcHp06cBKGtlycnJ6NOnj2Z7fn4+3N3drVdCIiIiEyxuMxsxYgQSEhKQkZGB5ORk+Pr6YsCAAZrt\np06dQkREhFULSUREZIzFNbHJkydj8uTJSE9PR8uWLbF69Wo0bdoUAHD79m0cOXIEAwcOtHpBiYiI\n9FlcExOLxZg5cyZmzpxpsM3X1xcpKSlWKRgREVFt6jXY+cqVKzh27BgKCgqsVR4iIiKz1SnENm3a\nhPvuuw8PPvggHnvsMZw5cwYAkJeXh7i4OGzdutWqhSQiIjLG4hDbtm0bJkyYgDZt2mDOnDlQaM0z\n6O/vjzZt2mD9+vVWLSQREZExFofY559/jt69eyMpKQmjR4822P7AAw/gn3/+sUrhiIiIamJxiF26\ndAmDBw82uT0gIAC5ubn1KhQREZE5LA4xT09PFBcXm9x+9epV+Pv716tQRERE5rA4xHr16oW1a9ei\noqLCYFtGRgZWrVqlM4MHERGRrVg8Tuz9999H37590bt3bzzxxBMQiUTYu3cv9u/fj1WrVkEsFmPq\n1Km2KCsREZEOi2ti4eHh2LNnD4KCgpCYmAiFQoHFixdj4cKFiI2Nxe7du9GqVStblJWIiEhHndYb\nadu2LbZu3QqpVIrU1FTI5XKEhYWhWbNm1i4fERGRSfVaNEsikaBTp04AAIVCgZKSEp1lWoiIiGzJ\n4ubEHTt2YM6cOTqPffnllwgJCUHLli0xevRolJSUWK2AREREplgcYgsWLEBmZqbm5zNnzmDWrFmI\ni4vDCy+8gL1792LhwoVWK2BmZiZeffVVhIeHIygoCF26dMGhQ4estn8iIhIui5sTr1y5gqefflrz\n86ZNm+Dn54fNmzfDzc0Nzs7OSEpKwvTp0+tdOKlUiv79+6Nr167YuHEj/P39cf36dQQEBNR730RE\nJHwWh1hZWZnOfa99+/ahb9++cHNzAwDExsZi9erVVincokWLEBwcjG+++UbzWFhYmFX2TUREwmdx\nc2JISAhOnz4NQFkrS05O1hncnJ+fD3d3d6sUbufOnYiLi8O4ceMQERGBhx56CN9++63OpMNERNR4\nWVwTGzFiBBISEpCRkYHk5GT4+vpiwIABmu2nTp1CRESEVQp37do1fPfdd3jttdcwadIknDt3TjOQ\nesKECUZfU/9FOT2ttJ/Gh8esbnjc6o7Hrm6EdNwiIyNr3G5xiE2ePBnl5eX45Zdf0LJlS8yYMQNN\nmzYFANy+fRtHjhzBa6+9VrfS6pHL5ejYsSNmzZoFALj//vuRmpqKZcuWmQyx2j5wrQ6lWWc/jUxK\nSgqPWR3wuNUdj13d3G3HzeIQE4vFmDlzJmbOnGmwzdfX16oJHxQUhLZt2+o81qZNG9y6dctq70FE\nRMJVp5WdjTlx4gT27t1b4wz3luratSsuX76s89jly5c5rRUREQGoQ4jNnTtXp4s9AIwaNQoDBgzA\niBEj0LlzZ9y4ccMqhXvttddw8uRJzJs3D6mpqfjxxx/x7bffYvz48VbZPxERCZvFIbZ582adJr5d\nu3Zh9+7deOutt7Bs2TJUVFTgs88+s0rhOnXqhDVr1mDr1q3o1q0bPvzwQ8yYMYMhRkREAOpwTyw9\nPV3npuD27dsRHh6u6XyRkpJitXFiANC/f3/079/favsjIqK7h8U1MZFIBJlMpvn5wIED6Nu3r+bn\nFi1aICcnxzqlIyIiqoHFIRYREYGdO3cCAH799VdkZmaiX79+mu1paWmQSCTWK6GdpBfLan8SERHZ\nlcUh9p///Ae///47WrdujVGjRiEqKgq9e/fWbD9w4ADat29vzTLaRfTGTFwuqLR3MYiIqAYW3xMb\nNmwYfH198csvv8DHxwfjx4+Hs7NyN7dv34a/vz9GjBhh9YLaQykrY0REDq1Oi2L27t1bp/al5uvr\na9VOHURERDWp88rOUqkUv//+u2ZMWGhoKHr37i34+2FeYgWKZSIAgEj1mEKhwK1iGVp512shbCIi\nsrI6nZUXLlyIxMRElJeX68wo7+7ujunTp+PNN9+0WgEbmrPI8LFj2RUY+HMupONCGr5ARERkksUh\n9v3332P27NmIj4/HxIkTNQOf//33X3z99deYPXs2fH198eyzz1q9sPZSWMGlX4iIHJHFIfb1118j\nPj4eW7duhUhUXW0JCwvDo48+iieeeAJLliy5q0KMiIgck8Vd7FNTUzFo0CCdAFMTiUQYPHgwUlNT\nrVI4e9CucxlpWSQiIgdicYg1bdoU165dM7n92rVrmvXFhEh70eiF/xTaryBERFQri0NswIABWLp0\nKTZs2KDTqUOhUGDjxo1YtmwZBg4caNVCNiTtmtjGK6V2KwcREdXO4ntis2bNwsmTJzFx4kS8//77\nuPfeewEomxlzc3MRFRWlmQxYiNiFg4hIOCwOMT8/P+zfvx8rVqzA3r17cfPmTQBAbGws+vfvjyFD\nhiAvLw++vr5WL2xD0A+x2+Vyu5SDiIhqV6dxYm5ubnj11Vfx6quvGmybN28ePvnkE+Tn59e7cPag\n0EuxS9Lq+ROl5XJI3Ky2GDYREdUTz8h6Wrjrptjmq9X3xcLWZjR0cYiIqAYMMT3L2pfp/Lz0YjEq\n5LxTRkTkiBhierydgekdm+g89vlZdrUnInJEDDEjgj3EOj+fzuW6YkREjsisjh1//fWX2TtMT0+v\nc2GIiIgsYVaIPfLII0anmTJGoVCY/VxHJfDiExE1GmaF2OLFi21dDiIiIouZFWKjR4+2dTkcCjsj\nEhEJAzt2GCHTH/FMREQOiSFmBGtiRETCwBAzgiFGRCQMDDEjGGJERMLAEDOC89YTEQmDoEJs/vz5\nkEgkePfdd236Ps3cBXVYiIgaLcGcrU+ePImVK1ciJibG5u/1zL0eNn8PIiKqP0GEWEFBAV5++WV8\n9dVXkEgkNn8/oc84QkTUWAgixCZNmoShQ4eiV69eDfaeux5rZvTxI5nlkHK1ZyIih1CnlZ0b0qpV\nq5Camopvv/3WrOenpKTU+z1TUlKgjDBPg22P7crFiOaVmBLOme31WePYN0Y8bnXHY1c3QjpukZGR\nNW536BBLSUnBnDlzsHv3bri4uJj1mto+sDnvqdnHoTSjz/GRSBAZaftmTSHROW5kNh63uuOxq5u7\n7bg5dIidOHECeXl56Nq1q+YxmUyGI0eOYPny5UhPT4ebm1uDlyuzRNbg70lERIYcOsQGDRqEjh07\n6jz2+uuvIzw8HJMnT4arq6tdynU+n02JRESOwKFDTCKRGPRG9PT0hK+vL6Kjo23+/nHNXPCXkVWd\nUwtZEyMicgSC6J1oL0GeYpPbymUKFFfK0X9nDt48fLsBS0VERGoOXRMzZufOnfYuAgBg7G95OJ1X\nidwyOW4WVWkef2F/PgaGumNEuGHPRiIisi7WxOrowu0q5JYZjhf78Vop1l8usUOJiIgaH4ZYDXoG\nu8HL2fjsHQqYnuqe830QETUMhlgNJsZ44/wzwUa3pZdU18L044yzVhERNQyGWC3qEkjMMCKihsEQ\nq4U5gZRRIseB9DKbl4WIiHQxxGphbk1s/tmi6tfYqCxERKSLIVYLcwPpYGY5wtakK1/DFCMiahAM\nsVqYm0dyBSCtUFj0GiIiqh+GWC3qVKtiVYyIqEEwxGrhVId6VYq0EocyyyFZYXwpFyIisg6GWC3q\nUqlKLZRh8K5cAMDO66WokpseGE1ERHXHEKtFfRsGx+zLx+HMcquUhYiIdDHEamGd21u8R0ZEZAsM\nsVpYI37Yz4OIyDYYYrWwRv44McSIiGyCIVYLJxHQwd+lXvtwxAy7eqcKZ3IrAAAKhQIKBTufEJHw\nMMRqIRKJ8PvjgfXbh5XKYk1D9+Si9085AIB+O3Mwdl++nUtERGQ5hlgD0L4nViFTIPH0HfsVRkU7\nWP/MqcTBDPagJCLhYYg1AO3AuHKnColnCjH/bKHdymMMGxOJSIgYYg1g980yPPxTNoDqsJjzV+21\nsYwSWYPdq+ItMSISIoaYhaIkzphyfxOLXrPgXBFO51Za/F7tNmTi5xsNs04ZM4yIhIghZqHpHX0w\ns5NPnV4rkyvw+O5czc/mTEclrZDX6b1qoz92jSFGRELkbO8CCMXwez3w5D0eGNDKvc77aLYqXScs\nZIrafwEN1bORzYlEJEQMMTMtjfer9z70c8KcgBI10HQfctbFiEiA2Jzo4BxxjBkRkaNgiNVD0qP+\n9Xr9xtQS+K9MQ+/t2fjszB1Iyw3vf9mqIqa/WzYnEpEQMcTqQVzPgHnjkBQyBXAmrxKfnC5E2NoM\nTWcP9YKa6rf4V6rbu9HaXe+55BkRCRFDrB7U96uaulqvulRUqZsm6j132ZqNcply256bZfBdmW72\nPlMKKvHLzZq76jPDiEiIHDrE5s+fj4cffhitWrVCeHg4RowYgQsXLti7WBrqgBFbsc3vu+Ri3NHq\nVi8SAdcKqwBUN/mlFFg25uytw1I882uezmPq0IrekKHzMxGRkDh0iB06dAgvvfQS9uzZg+3bt8PZ\n2RlPPPEEbt++be+iAaheYuWtWG+r7fPDU3cQuiZD8/OB9HJ02Jyl8xxLA8fYUjDXCmUAgPQSZWBq\nt06WVTHSGoJy9QB7l4JI2Bw6xJKSkjB27FhER0cjJiYG33zzDXJzc3Hs2DF7Fw1A9T2xFp5iAMBH\nD9ZtEHRNfkgp0fx/cZWqhmbhic+ceqL2LoN/SMeF25bPMEKW8V2Zjm1ZYnsXg0jQBDVOrKioCHK5\nHBKJxORzUlJS6v0+5u3DE2m3bgFwR352JgA3VEhzALjV+/1NiVqfgQ/bViCjVATA1ezPWlrqBkCs\n93xPg+dVb/dE9x+zsSWuFKEe5iemNY594+KJKyVOPG71wGNXN0I6bpGRkTVuF1SITZs2DbGxsejc\nubPJ59T2gWuTkpJi3j4OpaF1aCtI41xRWClHrlsRAptIgBQpgj2ckFlq/emiKhUiTEt2w8Mt3ACU\nIzIyEq8fuo28MjnWP+KPl37Px2sx3ogLcNV5neeVXKCgXPdzHUoz2L9mu2qbf4tQRDZzNXieMWYf\nN6qmOs48bnXj6N85hUKBq4Uy3OvjWKdZRz9ulnLo5kRtM2bMwLFjx/DDDz9ALHaMJhh1c2ITFyf8\nV2s+xbGRXjZ93zJVL8XP/y7EmpQS7Fb1PNxytRTbrpUaPN+Sbic5pTLN//N2je1xMPvda196OTpt\nyar9iVQvggix6dOnY8uWLdi+fTvCwsLsXRwNUyegmXHWvzem7WhWBQBlJxB9+p04Bu/KwSUzezNm\nlMjw/P7qFZ45doyo7oor+QfUEBw+xKZOnaoJsDZt2ti7ODr0A8OePc3Us+MvOFeExNN3MGhXDgDg\nUGYFMkqqmzYVCgVuFFUZ3Ue7DZn4K7ei+rkWvP9//nHDzzdKIbNh8pXLFDifb/0OJ1Vyhd06svA0\nd/dqoGlPGz2HDrEpU6Zg7dq1WLp0KSQSCbKyspCVlYWioiJ7Fw2LH5Ig2tdF5zF1M589/JFRrvn/\nxDOFOJxZYfCcSrkC26+Xof0m000c5dWtiVAogPRiGeQ1pPOpnAokSytxTCrG6N/ycTJH+b7Nv09H\nXplM57lncg3LtCalGDNPFJjcv7bF54vQY1u2Wc+1xNrLJej+o/X3a0wlq7eNBjOsYTh0iC1btgyF\nhYUYOnQo2rZtq/n35Zdf2rtoGBPpBWe9qpj2bBvP3OvR0EUy0GylbueNiQdv6zQX1iajRIbojZnY\nlGp4n02tz44cDNVaI019SEplCqQVV4fYltQS9P4px6DG8/ohKb46b95FyY1C4zXIuriu2tfkI1J8\nbKRZ1lYCVqVj5/Xq49nYT3Q1XSAJXWP/3TYUhw4xqVRq9N/06dPtXTSj7vNzQRMX5VfX183+h1Z/\nzPJFC5vMnlMF3it/3MYbh24jq6Q6lCrlCs38jVVaHTFFWn+6314s1vz/SweUA9S7/5iN/DIZSqsU\nKKjQbeaszcpLJbU+xxwnsstxv2oA+fJ/i5Gl1ZP0u+Qi9NqWrQk5W7hWxM4zgHKRWD8Lpk+zlQqZ\nAqU2GODP5sSGYf8z7V2kX0t33BzbAsDd9wVenVKCthsyASgnJw5YlY5PThcCAPK0Zt+/UVSFby8U\naV4jWZGGPXrzNt67LhOv/JGPdqr9AcqBv+nFMvTclo2zedXNjmN/yzNoltSnUCgsmmVEf35KbXP+\nuoOz+ZX45HR17WzCH/lIllbiVlGVZmLm+qjtq9Fpc6bZTaxCZps1yy337P58dNqSWfsTLXSXnQIc\nFkPMxjb3q99yLdZkjQU2r2nVUFanFBtsn3xUiveO656AR+jN26jcjwwlesFTIVfgXH6lzv28HTfK\ncDpXtwb53jEpNl2prpUtuVCM4B/SMfDnHHx86g5yjYTexduVeHZfHiQr0vDkL4blUTM2RdfGK6X4\n6Vopblfolvf7S8VG7/OplcsUdVptILVQhsNZ5bU+r6RKjhPZtT/PEhklMhRVNky8qA+NtVdksNTZ\nPN3OT9Zizp/btcIqu39+oWOI2Yj6+9s3xHYzeFjqHyv07NOex9HYH35BhXl/kFVG/nDV66mJRMox\ncAfSlTW4SwVVOifWby8W45uL1ffR1BMiH82qwNy/CxGxLlMz47/aqN/y8NP1mmfyB6qbQzdc0b0P\nWKWoHhd4q0h54nnzsBQf6d1PO55Vju8vFaOoUo6g79PxXbJh0JujQqYwWutLKajEOdXv8esLxXh0\nZ67Bc2pzq6gKGSXGa7ftNmTitYOWzU0qWZFWp16pCq3/llZVN+ktOlcIyYo0g+WHbMXcos88UYBv\nVK0M+t8vY0Rm1MU6bM7CQSOdsMyhUChwSe8YZZTIDC6s8spkuFxQWa8m09wyGWackGruYUpWpNmk\nCbYuGGI2MuX+JtjwiL9Vaj93owu3De859f4pR/P/H566g6F7lDWmGScK8P5J3drdnzmVUCgU+Pzv\nQlw0sq+g79Php9WxxZzfQvz2bOQbWZgUUHbDV9fS7tuUpekNuj+9XPV5KnHfxkzMPFmANw9LEblO\n2Tx15U4VpOVynZUH9DszZGsNMF9/WVnDNNXTte+OHPRU9dCs63CGuKQsPLxduY+cUplBWOaUyXE6\nt6LGk9R/Dt3GNxeKNOvfmVt5O5ldoblY+eGSMuAVCqDfzhw8ulP5+z+gOrZdthr2GA36Pg1fnis0\n7830XC6oxN9aTdVzz9xBVolME2Lax0G7UxKgrPV+db4Ii84VoaBCeYFSG3P/8ovrWPP9La0cnfWO\n0at/3Nb5OwKAJ3/JwwNJ2XhkR9174C45X4T/O1+MQ1qBW1rlGA3CDDEb8XcXo38rdwDAj/1NNynG\n+rmY3NZYGfvjP2TkavWlA7fx4ak7OJZt/EpWrgA2XSkxOS5O3995ule1W69WN1lWyXUXQS1WneDV\nWfNXTgVuFcs0tYtS1QaRCAhbm4EHk7I1S+zc0jpByhVAm/WZyC+TIbdMhldVtSAn1cWPuqlJrlAo\na6ZaueJk5AJJoVDg5QP5ePeY1GDb6pRivHNUinIZNGFdaOL+4MM/5aD5D+lYcLYQCoUCKQWVOksE\n/ZBSgqnHC9BslfJkLtfronI6twLjVB2Drt6pwm9pylpwv505mPWn8oLkG1XHHzmUrQTqGqax3//e\nW2W4XliFchnw/p916036QFI24rdXn+A/Pl2In66Xwtj1QszGTJ2WixY/KFeWEImUtWRzaP96Uu+Y\n/g6mFhrWimVyBU7X0FQNQNMcP2BnDiQr0lBQIa+eJFyL+qLhvNbF3uHMcqw1cjvAFPVuHbE3KUOs\nAfRu4W708XuaiPH9w35Gt7k7xsxadrHonGGX+5QCw5NA0lXTXf/VXv7jdo3j4mqi7lEJAAv/KdKp\nbSy9qHsCUG/7M0c3CLWblPrv1L1CBoBTBcpf9ODduZraG1D9edX7PZ1bqamZAspxgb+mGTaPvn/y\nDjallmK1kZ6cS84XaZo31ZUO3iegAAAea0lEQVQsde0yv6z6Xph2WM3+6w7+yKjAg0nZmHJUGYza\nvUrVruudiLddK8XWa6UYsTcXA3/OwVNa9yHVtUz1+fC2Xu3X2H3J4XvzND1Ktf2RUY4d1w2/B1kl\nMrPuNTmJRJBpPU+72Trx9B1c1QsfkcjwXpex99lxvRRf/lP9Pe60JQs3i6pwz1rDGtx/jXTi2XGj\nDA//lIN3j0lxrJb7o+qLuA6bMzXfv+1a088ZO57vHpXitUPVFzrLk4tr7LSUrmp+1v6oxv4m7YEh\n1oDUX6bUUcEAgKXxfrjHxxmh3oaJ9cuggIYsmkNJM3G/xtpqu6KWK5Truakt0mrG2p+ue2KZccKw\n5gMAO29Un0wuSpV/9BduV/dyVFeELtyuMtrdXn9w9B3VCx7fnauZfkwmV2DbtVKczK7AedUwilKZ\nwuDzaZ+A1LtV3+PMKZPj7SPKz3Ber3lWHToFFXKs/LcYc88YNud1+zEbn/+tfDx+ezZuqoYR7LlV\nbjAZtrpYl1UBod9sqX3O3Xil5mEVL/6ej7H7DMc+tt2QiQ9P3UEvVdOrdtAd1woFJ5HucQlbk4HF\nqnGLO26UoeOWLJ1muJtFMoz5rfr9FAoFntmbh7A16TohMOevO5om5+f2KQN8dUoJbpfXHqxzz9zR\nNNMuvViMNSmGx2BNSrFmCIya9r6f258PmVyB3DIZrupdYFTJgQvSKq3XyTH5qPHvr5qxsaL9f679\nfqy0XK75LLbCEGtAIV7KsPJTVbPcVO1TGx4xbG4M9DBeFXsggM2PdaH/hwwAgWbc19iYWn0CWX/F\neM1PrlDA1CiAa0beV3t2FWNXydoq5UD3H7OMzpOp9u6xAjy/Px/9dubglFYT1I+qq3HJijRIVqTp\nnLgAYINWQHx+ttDkoHZ171KZAph0xPTgdHUZ/86r1Jm+zPAzKZCu1aR6sYYOHBP+MN7JRLIiDbuy\nxTpNvPrmny3C2fxK5JbJdE748/6uDmGFQrdjR5UC+EVvSIh+7fq4quYjVyjguzIde9PKIVV1aCqp\nkuPPnApc0qqlbFd1KPpUFf7n8yshVxheZHyp6tDy8elCnXui6SUyvPpHPnZcL0Wy6ljpD1sx5u2j\nUkSs0x068OW5Qvyco3tuqa3GOvlIdcDVdIu/oEKONuurF/TtvT0bYWszMPfvut3DNJdjrRFwF+sX\n4oYQL7HOgF31l6ed3vRVPYNdEejhhC+6SfC23hWSlzOvOxqSsatgffUZsHu1pObfZ4VcgQu3q4x2\nhFFb/m9106a5vUMB5SB2tY0mAlqbJZ3RjIW3uqay7VoZtl2rPrmO/LU6YJacL0Kp3ktXmOjh+cEl\nN7RQLYt3Nq8C7f2NLxukfyLfm1Z9EaH/9wUAmWa2BBjrj6G+d1aTHtuy0cRFpHM/Mq1YpnOv73Wt\npr7fVOVdf6UUPYNd8dPAALO67xvrjax8j+oe06Fr0tHEyDnl8d252PKoP178Pb/WXr1XCqoQ3tQZ\nmSUyZGvVus+o7jGvv1yC6R1tNyk6Q6yBbHq0GUqrFHgxqvZlWmY90BROIhHGRXkZ/JG5O7O3Y2PS\nZn3dB+E6iYDIdbWfVM31e7p1x6QZM93I/SFjQaOm7tzSa3sOJrf3Rl6ZHB5m/I2MP2B6+rV/zbzX\nU59mMv0ONcaWUDJGDmWob7tWe03MnDC+U6HAnYrq5435LQ87byj3XVghNwiww5kViG9eHYLTj0ux\n5EIxpONCdO7DabteZNvbAwyxBuThLNJcLT7e2t3kYnkPBNSwEKUD9g4ix7Tucglyyhq2G/SiOnZ/\nryvtnp7zz5o/MfjmGuYDNZc1x4Srh1bU5kpBVY2hri29DgO41QEGAA8ZmWx77t+FeFprXtglF1Q9\nTBUKzWtzSmWae6INgW1TdvJ9H394uxge/vtMdLl/Pcbb5L4uPBNc43sZeRtqBH5Ls33NSd8Hdez+\nLkTGBuzX1VkzJyKwxYrxppgKwV9uGdYCF/9TpGk+7Lg5C312GPbEtRWe3hzIk/d4IOlR3U4ej4Uq\nu+d/3LkpAGhqb9M6NMGJYYEAgBZeYnzeranJ/brU1nuAiCymf6+tsXj/pOGFivb9vKIGnsmDzYkO\n4tbY5vAQiyDWC5zVffw042hujW0Od7EISy4UQywC2khcIB0XAgAGa5tps0ZN7Pk2nlhlpVnkiYis\nhTUxB+Ht4mQQYIDyxrW/qku+t4uTZg0z/dkaamrZcDVRE4uSGF7D+JtYQqZfS+MDtuviowdt11OJ\niBoXhphA6eeSdoYFeTghoXN186K6B+3C7hIAwBNhyhuzYhGQ9Kg/zjwdpHnu/f66NboDjysHXd/T\nxHSlXd3kaS7OJ0lE1sIQE6Dh93oYBIeXVrfi4fd64oW21V35naCuvSl/9nev/rX3CXFHmFZA6eeL\nekC2q97Y68QuTTE+ygtLe/miZ7D5M/UfHhqI+3wbrhU72khtk4juHgwxAVoa74e2Et0aU4dmrjj/\nTDCk40LwUeem8HAWYWiYMujUwaRde3v3/iaYcn8Tg32bqiNF+Dhj58BmAICWXmK80s4L87pJMDzc\ns8ZZE/Q193RCfAt3vNzOcLycn15T5v4hylpgz+AahhzUwjHm2dZlqskWAGbHmdfUOr2j4e/OlBfb\nenGBRrprMcTuIupprdRWPazb07G5Z/X2/3bywbB7PA32YepkJxKJ0ENV47rXx1mnSdDY7bjfh1TP\n/fhylBeOqXpSeql6mWgH3weqE7d6ItZREZ74d0QwOjZThldEU2e08DT8qk4wEoQA8Mfj1e+tPz2i\n+n2PPBFo9LVqrqq3GxbmgWkdzA8MoPrzqL3QRvl5ACDA3QkLekhMvtbFzCsC7d9lbV6P8UY7VY10\nXFvD33lj0T2o7hdD5LgYYo2AuqmxT0jt966cRMChoYF4RRUQod5iDK7lnpexEFPP/Tgm0hP/7eSD\nKFVPSnXzpDoQ34jxxuT2ypBQKJTNjXO7NkWQ6iR9a2xzfNZVgrPDq8fCzeuqvN9n6nSvvbyNt4vu\ns1JGBuPSyGCD3pz3NNENhYU9fLGtfzOseNgP0zr6IPf5FpptySOC8b8HdIPqZ1UtFQD0J4xY0MMX\nYtVfWsqo5hjS2gOmuBj5UCt76650sOVRf0SYGChvTHhTZ+wdHIC0sc3xRXdf/DM8yOjzUkYajjd8\n6h7dsu56rJnBc2rioRXKz4Tr7mtahybwMfaBbeSJMA/810gNNv3Z5ljTx/hqEsbsHGj6GMx5oPaa\ndNrY5ma/F9WOIdYIBHg44fqY6j+cmk4b3YLccJ+fi6YXpKezE1b3Nb0eGgD0DHZDbBMZWnqJMbm9\nN16K8kILLzF6t3DDO+2bQGKk+WxIaw/M6NgEz7WprhnIFUCMn4vOIHBvFye4OIng7CRCoIfy8b6q\nML7Xx1kzIXL/Vu4I9xEjSlJdS/zzyUBs6uePRaqaz9XRzeHnLtYErHpg+Rsx3vi2l/Ik5q1KIB8X\nEeJbVN/rc3YS4bMuyvAM9hTjrdjqk+G10c3RPdgN3z/sBzcx0EtrWh71yt7OZnZm8TJyUtcPYj83\nJ3QPdtNplozRus9orDLn5eKkqQW39DYegAEeYjTXqvHuHRSASe2b6DTzdgtyw9nhQfhpQPWJPEcr\n4AHlRRAAPBvpia97+Srf31mkOcZqE2O8Db6M87o21Qwb0fZOe9OD/fWZChknEfBuB8OQcReLMKi1\nB6TjQpD+bHNkPFv9eUaEG15wdA00XqNb1ENick5D9ffq2UhPze9Bn7XDraaap4cl9wAArH/E/JDX\n963qO2ArDLFGoImLE5qq2sfeiPHG2EjDJqWro5sj/dnmmKSqFb0W443FDxlv9vLSq2rE+Llg+f3l\n+OeZYHwQ1xSfd1O+7sf+zUxOrQUA73XwQRute3u1rTV4aWRzSMeF4B4fZwwOdcegUHf8Olh5whzQ\n0h1/PRWMY8OUtYyUkcGIaOqCZu5ijI7wxKGhgfDVC9OPH1SGUktvMR4MdIV0XAhuPat7QtbWysiS\nOYND3TUh/XiYB7KeC0GHZq5Y0lP5h7u5n/ICQOLmpNPEaswz4R4YEe6JroGuOsMfglW10ggfZ9wc\n21zTzHpldPVJz9lJhM6q6cpmm1EbMHUOuziiuaam+2CgK2L9XPCeqjlV3Ys11NsZPVVBfXNsc81g\n+pfbeeH1GG80U3Uc+vIhXwwN88CVUcFIUS0/9EMfP3ytOjZNXZ0MLqjGtzMeVv1auuOPxwOwd1CA\nQa3pkZDqi4ZfBwdoavkAEOxR/TtXD0vZoRdy2mXwdHaCh7NIMwxkYnR1edThaGruADexyOhCpQBw\ncWQwjj4RiC9UPYT1a9cvt/OCey3Bog5C9b3i8VFeBrVbbYEeYp3w/7aXr2Z2n12PNUPHZtV/e0t7\n+eKSXk08/4UWeEY1xVR/C4fYTGlffZHnaeP5Xtl16y7315NBOr0RP+psfGYP/RN8iJcYYyIN7znp\n78+a9FcHrol27fDCM8EI1rtnFqC1lI2zk8jodF7xLdzQpqmz0StrYyUZ0Modfz1Z3RR3+wXTgff0\nvR5YeK5Q595hh2bV7xPo4aSZ8XvnwGbwdBYhVlUDVjfZ3SiSQa4A7vFxxlP3eOB+fxc00buK/7qn\nL9zEypUQorQuCP5zX5MaFzmsae5a/U2tVeEdpjfMQr/G9N+OPpC4OaGsSqF5DQDNOEdAWQMvq1Lg\nk9PKGR48xCIUQAFnke4s+dnPtcDgXbk4kVOBrOdaaJqhAegsVDkm0hMfP9gUm1JLEOHjbDDvqFgk\ngnRcCCQr0jTh85Beb1pjQz7U76f9O+sR7Kb5zH8+GQhnJxFS71ShT4g7DmWWo3OAK5aqZtx/JtwD\nG6+UIq6ZC/7KrYSrk0hntYqHVcH7dqw3HghwxSBVE/Pavn5wF4tQWqVAE1cn/N/5IuxWLbvS3EuM\nlIIqzRIu87pJdJZJUdvyqD+e+iUPcoUCMzv54GBGBS7fqcIz4dUXryIRsH9IIDZdKcHLf9zG8HDd\nC1sfV2Ugdwtyw8bUUohEIrwc5aX5fGr3+bloZss/OzwIPi5OkLg5IeF0w00/xhC7y4U3te6v2Nr7\nU/Nzc6rzFVsLr7ovg33iSeP3h4wRiUQ6n7+m8W4uTiJNrdCY888EwwlA0qlUnZqD9n5ba4XGd72N\nN+eMjKhbRw1XsTLImnuKcaOWyVoHhnroNLGZos5Xd2cR/h5uej5Pd2eR5h7nnkEBqJArENlU9yLD\nVSzCF90luFFUpRNggDLUM55tgbE/38DLUV6QuDnhZb0aXN7zLbDqUgl6Nq8OIe0a1NP3etQ4CXBt\nU7VFqMqrDnZ1MD7Wyh3rL5egtarJ9rchgZCsSDNZ8531gO5F5WOhujWr7kGu+COjHE/+koe1ff0g\ngnLM5rq+yu9DO61m5CujghG+LlNzv1emUH6Xfhms2wIQ11SmGfdp6m+ulepvalyUF8apVt6Y202C\n4ioF1mpNVvxAs+oQC9Vqpn49xhvdg1wxdE+ewe/P2hhi5BCODQusdYHIhmTr2d/UJ8kOTW03CMDV\nCagwsfujTwRBBGjuM2rnsf7wDQC1Lm9yY0xzk/d6atK6hkH0MX4uiDExIbaHswiJ7SoQ2cz4fR+x\nk8hg2SPt79eyeD8sizddrnAf3XAw1Uyo7x4fZxwcGog7FXLNvVFj9/jM/ao7O4k0HbKauYs1LSYD\nVWE3PsoL5TIFrhbK4O8uxs2xzTW1dVPv8XVsOXxUtxfEer+yAHcniEXAgu7G72N90V2Cggq5Zsb6\ndr4uRlskfFyVQ2l+HxKA9v62XciXIUYOwdRK1vawpo+fVafZspeJ0d4oMTEZa033Kns1dzN64q2J\n+qToqKIkzpp7hubo2dwN+aqTs3ZzqLl8XJ009w2NsXTSmq2P+hs0+Sv3I8Ib91Xff1IH2P4hAWhp\nRguFWK8g/44MhgimWxncxCIsjffF47tz8WdOJZq5O9XYItHBxEWGNTHEiPQMqqELvJD870HTKxs0\nNjU17Zpibu2rLrydRdj4SM29frU9bMbwGG0dzQyPNk2ddZoUzfnMns5OaOEpBlCJJ++x/9+KY18+\nqSxbtgzt27dHUFAQ4uPjceTIEXsXiYiozkQiER5tZf/afusmzkg3436nvnFtvZQzwTjAPKgOH2JJ\nSUmYNm0a3nnnHfzxxx/o3Lkzhg8fjps3b9q7aEREjdLDIe6Y3930zDMNyeFDbPHixRg9ejSef/55\ntG3bFnPnzkVQUBCWL19u76IREZGdOXSIVVRU4MyZM+jTp4/O43369MHx48dt8p6RkZE22e/djset\nbnjc6o7Hrm7utuPm0CGWl5cHmUyGgADdcQ4BAQHIzs62U6mIiMhROHSIERER1cShQ8zf3x9isRg5\nOTk6j+fk5CAwsOalNIiI6O7n0CHm6uqKDh06YP/+/TqP79+/H126dLFTqYiIyFE4/GDn119/Ha+8\n8gri4uLQpUsXLF++HJmZmRg3bpy9i0ZERHbm0DUxAHjyySeRkJCAuXPnomfPnjh27Bg2btyI0NBQ\nq75PYx9QffjwYYwcORLt2rWDRCLBmjVrdLYrFAokJCQgKioKwcHBGDRoEC5evKjzHKlUigkTJiA0\nNBShoaGYMGECpFLdWbbPnz+Pxx57DMHBwWjXrh0+/fRTKBS2nqnQdubPn4+HH34YrVq1Qnh4OEaM\nGIELFy7oPIfHztDSpUvRvXt3tGrVCq1atUK/fv2wZ88ezXYeM/PMnz8fEokE7777ruaxxnbsHD7E\nAGD8+PE4d+4csrOzceDAAfTo0cOq++eAaqC4uBjR0dFITEyEh4fhVDILFy7E4sWL8emnn2Lfvn0I\nCAjAsGHDUFhYqHnO+PHjcfbsWWzevBmbN2/G2bNn8corr2i237lzB8OGDUNgYCD27duHxMREfPnl\nl/jqq68a5DPawqFDh/DSSy9hz5492L59O5ydnfHEE0/g9u3bmufw2Blq0aIF/ve//+HAgQPYv38/\nevXqhTFjxuCff/4BwGNmjpMnT2LlypWIiYnRebyxHTuRVCp1vGhtYH379kVMTAwWLVqkeaxTp04Y\nOnQoZs2aZceS2UdISAg+++wzjBkzBoDyyi4qKgovv/wypkyZAgAoLS1FZGQkPvzwQ4wbNw7//vsv\nunTpgt27d6Nr164AgKNHj2LgwIE4efIkIiMj8d1332H27Nm4dOmSJijnzp2L5cuX48KFCw4xhU19\nFRUVITQ0FGvWrMHAgQN57CwQFhaGWbNm4YUXXuAxq0VBQQHi4+OxaNEifPrpp4iOjsbcuXMb5fdN\nEDUxW7LHgGqhuX79OrKysnSOkYeHB7p37645RidOnIC3t7dOh5uuXbvCy8tL5zndunXTqen17dsX\nGRkZuH79egN9GtsqKiqCXC6HRKKckofHrnYymQxbtmxBcXExOnfuzGNmhkmTJmHo0KHo1auXzuON\n8dg1+hDjgOraZWVlAUCNxyg7Oxv+/v46V2gikQjNmjXTeY6xfai33Q2mTZuG2NhYdO7cGQCPXU3O\nnz+PkJAQBAYG4u2338bq1asRExPDY1aLVatWITU1FTNnzjTY1hiPncP3TiQSihkzZuDYsWPYvXs3\nxGLHWR/NUUVGRuLgwYO4c+cOtm3bhokTJ2LHjh32LpZDS0lJwZw5c7B79264uNh2sUmhaPQ1MQ6o\nrl1QkHItppqOUWBgIPLy8nR6LykUCuTm5uo8x9g+1NuEbPr06diyZQu2b9+OsLAwzeM8dqa5urri\n3nvvRYcOHTBr1izExsbi//7v/3jManDixAnk5eWha9eu8Pf3h7+/Pw4fPoxly5bB398ffn5+ABrX\nsWv0IcYB1bVr3bo1goKCdI5RWVkZjh49qjlGnTt3RlFREU6cOKF5zokTJ1BcXKzznKNHj6KsrEzz\nnP3796N58+Zo3bp1A30a65s6daomwNq0aaOzjcfOfHK5HBUVFTxmNRg0aBCOHDmCgwcPav517NgR\nTz31FA4ePIiIiIhGd+zE06ZNm23vQthbkyZNkJCQgODgYLi7u2Pu3Lk4cuQIvvrqKzRt2jhWxy0q\nKkJycjKysrLwww8/IDo6Gj4+PqioqEDTpk0hk8mwYMEChIeHQyaT4b///S+ysrKwYMECuLm5oVmz\nZvjzzz+xefNmxMbGIi0tDW+//TY6deqk6bobHh6OFStW4Ny5c4iMjMTRo0fxwQcfYNKkSYK9YJgy\nZQrWr1+PlStXomXLliguLkZxcTEA5QWSSCTisTNi9uzZcHV1hVwuR1paGpYsWYKNGzdi9uzZmuPE\nY2bI3d0dAQEBOv82bdqE0NBQjBkzplF+39jFXmXZsmVYuHAhsrKy0K5dO3zyySdWH4/myA4ePIgh\nQ4YYPD5q1CgsWbIECoUCiYmJWLlyJaRSKeLi4jBv3jxER0drniuVSvHee+9h165dAICBAwfis88+\n0/TUA5Q386dMmYJTp05BIpFg3LhxmDp1qkN12bWE9mfTNnXqVEyfPh0AeOyMmDhxIg4ePIjs7Gz4\n+PggJiYGb775Jvr27QuAx8wSgwYN0nSxBxrfsWOIERGRYDX6e2JERCRcDDEiIhIshhgREQkWQ4yI\niASLIUZERILFECMiIsFiiBERkWAxxIhsLDk5GS+++KJm5fCoqCg89thjSEhI0Dxn2bJlBqtpE1Ht\nONiZyIZOnDiBIUOGIDg4GKNGjUKLFi2QkZGBM2fOYN++fZqlM7p16wY/Pz/s3LnTziUmEhYuxUJk\nQ/PmzYOnpyf279+vmWFczdHWZSISIjYnEtnQ1atXERUVZRBgQPWSFrGxsbh48SIOHz4MiUQCiUSC\n2NhYzfPKy8uRmJiITp06ITAwEO3atcP06dNRUlKisz+JRIK3334bSUlJ6NKlC4KCgtCjRw/8+uuv\nOs+rqqrC3LlzERcXh+DgYISFhaFv377Yvn27DY4AkW2xJkZkQ6GhoTh27BjOnTunE0zaEhISMHXq\nVHh5eeGdd94BAHh5eQFQTuY6duxYHD58GM899xyioqLw77//4rvvvkNycjKSkpJ0JmQ9fvw4tm7d\nildeeQXe3t5YtWoVRo4ciZ9++gndunUDACQmJuLzzz/Hs88+i7i4OBQXF+Ps2bM4deoUHn/8cRsf\nESLr4j0xIhs6cOAAhg0bBgDo2LEjunXrhp49eyI+Ph7u7u6a55m6J7Zp0yZMmDABP/30Ex566CHN\n4xs3bsSECROQlJSEPn36AKieUf+XX35B586dAQD5+fno1KkToqKisHv3bgBAz5490aJFC2zYsMF2\nH5yogbA5kciG4uPjsWvXLvTv3x8XL17EV199hREjRqBNmzZYvXp1ra/funUrIiIi0K5dO+Tl5Wn+\n9ejRAyKRCAcPHtR5fseOHTUBBgB+fn4YPnw4jh07BqlUCgDw8fHBxYsXcfnyZet+WCI7YHMikY11\n6dIF69atQ2VlJZKTk7Fnzx4sWrQIb7zxBlq1aoX4+HiTr71y5QpSUlIQHh5udLv+EvLGnqd+7MaN\nG5BIJJgxYwbGjBmDBx54AFFRUejTpw+GDx+Ojh071uNTEtkHQ4yogbi4uCA2NhaxsbF48MEHMXTo\nUGzcuLHGEJPL5YiKikJiYqLR7cHBwRaXo0ePHjhz5gx27dqF/fv3Y/369ViyZAlmz56Nt956y+L9\nEdkTQ4zIDuLi4gAAmZmZAGBytdx77rkHZ86cQXx8vFkr6l65csXkY6GhoZrHJBIJRo0ahVGjRqG0\ntBTDhw9HQkIC3njjDYjFYos/D5G98J4YkQ0dOHAAcrnc4PG9e/cCACIjIwEAnp6emntW2oYNG4bs\n7Gx89913BtvKy8tRWFio89jp06dx4sQJzc/5+fnYtGkTunTpoun4kZ+fr/MaDw8PtGnTBmVlZSgt\nLbXwExLZF3snEtlQt27dUFRUhMGDB6Nt27aQy+X4+++/sWHDBs0g6NatW+Pdd9/FsmXLMHXqVERE\nRMDLywsDBw6EXC7H6NGjsXv3bgwbNgxdu3aFQqHA5cuXsXXrVqxcuRI9e/YEoKxdRUdHIyMjAxMm\nTNB0sb927Rq2bduGHj16AAAiIiLQvXt3dOrUCX5+fvjnn3+wfPly9O3blz0WSXAYYkQ29Ouvv2L7\n9u04fvw40tPTUV5ejuDgYMTHx+Odd95BWFgYAGUHjTfffBOHDx/GnTt30KpVK5w7dw6AcnDykiVL\nsG7dOly5cgXu7u4ICwtD//79MXHiRPj6+gJQhti4cePQs2dPJCYm4tq1a4iIiMCsWbPQv39/TZk+\n//xz7Nq1C5cvX0ZZWRlCQkIwbNgwTJo0Cd7e3g1+jIjqgyFGdJdQh9gXX3xh76IQNRjeEyMiIsFi\niBERkWAxxIiISLA4TozoLmGsiz7R3Y41MSIiEiyGGBERCRZDjIiIBIshRkREgsUQIyIiwWKIERGR\nYP0/1PIOAQq54RcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63da8e5940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/-21759\n",
      "1.\n",
      "--------------------------------\n",
      "Cameron is expected to give a speech setting out the government’s future strategy\n",
      "Se ha sido un discurso que ha debate de un futuro \n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "What we ought to be doing is to get behind the Muslim communities\n",
      "¿Qué es que escuchar que era que era las personas \n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "Cameron is due to use a speech to warn young Britons tempted to join IS fighters\n",
      "Se ha sido utilizar un discurso para utilizar a la <ukn> para <ukn> <ukn> <ukn> <ukn> <ukn> \n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      "The prime minister warned of the dangers posed by those who quietly condone IS militants extremist ideology\n",
      "El Ministro El Ministro del peligros del peligros de que <ukn> en un poco <ukn> \n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "There are two kinds of damages\n",
      "Hay dos de buena de <ukn> \n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "The first is compensatory meaning money to pay for the actual cost of an injury or loss\n",
      "El primero es el que el dinero para una vez para el coste \n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "The second is punitive or exemplary meaning an amount of money that is more than the actual damages\n",
      "La segundo es que no es que una que una es que es que es más \n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "A deportation has certain consequences regarding the number of years within which a deportee may not legally immigrate\n",
      "Un motivo ha determinados consecuencias consecuencias que ha planteado una situación que puede constituir \n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "The UK could decide to back out at any point in the negotiations\n",
      "El Reino podría decidir a cualquier punto a cualquier <ukn> \n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "The terms of the Brexit are still unclear\n",
      "El términos de los términos son los <ukn> \n",
      "--------------------------------\n",
      "11.\n",
      "--------------------------------\n",
      "Brussels is likely to make an example of them to deter other member states from following suit\n",
      "La Bruselas es una cuestión de ellas de todos que ha de <ukn> \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = es_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    en_sentences = ['Cameron is expected to give a speech setting out the government’s future strategy',\n",
    "                    \n",
    "                   'What we ought to be doing is to get behind the Muslim communities',\n",
    "                    \n",
    "                   'Cameron is due to use a speech to warn young Britons tempted to join IS fighters',\n",
    "                    \n",
    "                   'The prime minister warned of the dangers posed by those who quietly condone IS militants extremist ideology',\n",
    "                    \n",
    "                   'There are two kinds of damages', \n",
    "                   'The first is compensatory meaning money to pay for the actual cost of an injury or loss', \n",
    "                    'The second is punitive or exemplary meaning an amount of money that is more than the actual damages',\n",
    "                    \n",
    "                   'A deportation has certain consequences regarding the number of years within which a deportee may not legally immigrate',\n",
    "                    'The UK could decide to back out at any point in the negotiations',\n",
    "                    'The terms of the Brexit are still unclear',\n",
    "                    'Brussels is likely to make an example of them to deter other member states from following suit']\n",
    "    \n",
    "    en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([es_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print ('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print (en_sentences[i], end = '\\n', )\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print (words[i], end = ' ')\n",
    "            \n",
    "            print ('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### BLEU index calculatios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3144006510201675\n",
      "0.5607900239988007\n",
      "0.14914147968282956\n",
      "0.46888166570791445\n",
      "0.668740304976422\n",
      "0.31991221505676887\n",
      "0.27705194057255156\n",
      "0.5093330917854971\n",
      "0.38297956737438044\n",
      "0.3148988880384631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis1 = 'La gestión pública tiene que dar un discurso para la estrategia futura'.split()\n",
    "hypothesis2 = 'Lo que tenemos que hacer es que sea el problema de las comunidades comunitarias'.split()\n",
    "hypothesis3 = 'El europeo está emplear utilizar un discurso para el <ukn> de <ukn> <ukn> <ukn>'.split()\n",
    "hypothesis4 = 'El Ministro del Sr Evans ha dicho que por los que se ha hecho'.split()\n",
    "hypothesis5 = 'Hay dos tipos de crédito'.split()\n",
    "hypothesis6 = 'El primero es el dinero del dinero para pagar el coste de un beneficio'.split()\n",
    "hypothesis7 = 'El segundo es que es un cierto papel de dinero que es un cierto dinero'.split()\n",
    "hypothesis8 = 'Una condición que se ha hecho algunas consecuencias sobre años dentro de un posible plazo'.split()\n",
    "hypothesis__ = 'La decisión podría decidir decidir a la cuestión en las negociaciones'.split()\n",
    "hypothesis9  = 'Los términos del <ukn> están todavía claro'.split()\n",
    "hypothesis10 = 'Bruselas se puede realizar por ejemplo en su corazón otros Estados miembro de otros Estados'.split()\n",
    "\n",
    "reference1 = 'Se espera que Cameron pronuncie un discurso exponiendo la estrategia futura del gobierno'.split()\n",
    "reference2 = 'Lo que deberíamos hacer es apoyar a las comunidades musulmanas'.split()\n",
    "reference3 = \"Cameron tiene programado un discurso para advertir a los jóvenes británicos tentados a unirse con los combatientes del Estado Islámico\".split()\n",
    "reference4 = 'El primer ministro advirtió sobre los peligros que plantean aquellos que calladamente toleran la ideología extremista de los militantes del EI'.split()                \n",
    "reference5 = 'Hay dos tipos de daños'.split()\n",
    "reference6 = 'El primero es el dinero compensatorio que significa dinero para pagar el costo real de una lesión o pérdida'.split()\n",
    "reference7 = \"El segundo es punitivo o ejemplar que significa cantidad de dinero mayor que los daños reales\".split()\n",
    "reference8 = 'Una deportación tiene ciertas consecuencias con respecto a la cantidad de años durante los cuales un deportado no puede inmigrar legalmente'.split()\n",
    "reference9 = 'Los términos del Brexit aún no están claros'.split()\n",
    "reference10 = 'Es probable que Bruselas haga un ejemplo de ellos para disuadir a otros Estados miembros de seguir su ejemplo'.split()\n",
    "\n",
    "bleuScores = []\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference1], hypothesis1)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference2], hypothesis2)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference3], hypothesis3)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference4], hypothesis4)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference5], hypothesis5)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference6], hypothesis6)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference7], hypothesis7)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference8], hypothesis8)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference9], hypothesis9)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference10], hypothesis10)\n",
    "bleuScores.append(BLEUscore)\n",
    "print (BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3144006510201675, 0.5607900239988007, 0.14914147968282956, 0.46888166570791445, 0.668740304976422, 0.31991221505676887, 0.27705194057255156, 0.5093330917854971, 0.38297956737438044, 0.3148988880384631]\n"
     ]
    }
   ],
   "source": [
    "print (bleuScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average BLEU score is:  0.396612982821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print (\"average BLEU score is: \", np.mean(bleuScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The average BLEU score is 0.396612982821\n",
    "We used on 2 millions sentences from parlamet parallel corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
