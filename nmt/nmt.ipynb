{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read en0\n",
      "read es0\n",
      "read en1\n",
      "read es1\n",
      "read en2\n",
      "read es2\n"
     ]
    }
   ],
   "source": [
    "# BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "#dataRead\n",
    "\n",
    "\n",
    "en_sentences = []\n",
    "es_sentences = []\n",
    "\n",
    "for i in range(3):\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".en\", 'r') as en_file:\n",
    "        for en_line in en_file:\n",
    "            en_sentences.append(en_line)\n",
    "    print (\"read en\" + str(i))\n",
    "    with open(\"dataset/europarl-v7.es-en-\" + str(i) +\".es\", 'r') as es_file:\n",
    "        for es_line in es_file:\n",
    "            es_sentences.append(es_line)\n",
    "    print (\"read es\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I regret this, but the vote has already been taken and the decision is made so let us leave the matter there.\n",
      "\n",
      "Lo lamento, pero la votación se ha realizado, se ha adoptado la decisión y, por consiguiente, dejemos así las cosas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 123\n",
    "print (en_sentences[index])\n",
    "print (es_sentences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DataPreparation\n",
    "def create_dataset(source_sentences,target_sentences):\n",
    "    source_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in source_sentences for word in sentence.split())\n",
    "    target_vocab_dict = Counter(word.strip(',.\" ;:)(][?!') for sentence in target_sentences for word in sentence.split())\n",
    "\n",
    "    source_vocab = list(map(lambda x: x[0], sorted(source_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    target_vocab = list(map(lambda x: x[0], sorted(target_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    \n",
    "    source_vocab = source_vocab[:20000]\n",
    "    target_vocab = target_vocab[:30000]\n",
    "    \n",
    "    start_idx = 2\n",
    "    source_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(source_vocab)])\n",
    "    source_word2idx['<ukn>'] = 0\n",
    "    source_word2idx['<pad>'] = 1\n",
    "    source_idx2word = dict([(idx, word) for word, idx in source_word2idx.items()])\n",
    "    \n",
    "    start_idx = 4\n",
    "    target_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(target_vocab)])\n",
    "    target_word2idx['<ukn>'] = 0\n",
    "    target_word2idx['<go>']  = 1\n",
    "    target_word2idx['<eos>'] = 2\n",
    "    target_word2idx['<pad>'] = 3\n",
    "    \n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    x = [[source_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in source_sentences]\n",
    "    y = [[target_word2idx.get(word.strip(',.\" ;:)(][?!'), 0) for word in sentence.split()] for sentence in target_sentences]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2 \n",
    "        if abs(n1 - n2) <= 0.3 * n:\n",
    "            if n1 <= 15 and n2 <= 15:\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "    return X, Y, source_word2idx, source_idx2word, source_vocab, target_word2idx, target_idx2word, target_vocab\n",
    "\n",
    "def save_dataset(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#en_sentences = ['hello my friend', 'we need to reboot the server']\n",
    "#es_sentences = ['hola mi amigo', 'necesitamos reiniciar el servidor']\n",
    "save_dataset('./data.pkl', create_dataset(en_sentences, es_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, es_word2idx, es_idx2word, es_vocab = read_dataset('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [9207, 3, 2, 1713]\n",
      "Sentence in Spanish - encoded: [11270, 13, 594, 4, 1490]\n",
      "Decoded:\n",
      "------------------------\n",
      "Resumption of the session \n",
      "\n",
      "Reanudación del período de sesiones "
     ]
    }
   ],
   "source": [
    "#CHECK THAT WORKs\n",
    "print ('Sentence in English - encoded:', X[0])\n",
    "print ('Sentence in Spanish - encoded:', Y[0])\n",
    "print ('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[0])):\n",
    "    print (en_idx2word[X[0][i]], end = ' ')\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[0])):\n",
    "    print (es_idx2word[Y[0][i]], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [es_word2idx['<go>']] + y[i] + [es_word2idx['<eos>']] + (length-len(y[i])) * [es_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "#print X\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 15\n",
    "output_seq_len = 17\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "es_vocab_size = len(es_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [es_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [es_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = es_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
